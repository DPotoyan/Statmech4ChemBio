{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Entropy_flip_2_coins.jpg\" alt=\"Information\" style=\"width:30%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large deviation theory, entropy and free energy.\n",
    "\n",
    "Consider a symmetric 1D random walk again. \n",
    "\n",
    "$$P_N (n) = \\frac{N!}{(N-n)! \\cdot n!} \\cdot p^n q^{N-n}$$\n",
    "\n",
    "Taking log and using Stirlings approximation $log N! = NlogN-N$  we obtain\n",
    "\n",
    "$$ log P_N (n) = N \\Big[ -\\frac{n}{N} log \\frac{n}{N} - \\Big(1-\\frac{n}{N}\\Big) log \\Big(1-\\frac{n}{N}\\Big)  + \\frac{n}{N} log(p) + \\Big(1-\\frac{n}{N}\\Big) log (q)\\Big]$$\n",
    "\n",
    "- Probability can be cast in exponential form. That's great but we already know that in the limit of large N the distribution becomes gaussian.\n",
    "- True but before N becomes large is a very interesting regime where fluctuations play important role. We are going to discover a another feature common to most random variables. \n",
    "\n",
    "$$P_N \\sim e^{-N I(n)}$$\n",
    "\n",
    "- We find that probability to deviate from mean values decays exponentially with a decay funciton specific to a system and independent of N. \n",
    "- Later on we will be calling $I(n)$ **a free energy per particle**. Mathematicians call it large deviation function. For now let us carry on and not think too much about this.\n",
    " \n",
    "- Let us now examine one particular expression that going to always pop in in any problem. We are going to this expression **Entropy**. \n",
    "\n",
    "$$s(n) = - \\Big[ \\frac{n}{N} log \\frac{n}{N} + \\Big(1-\\frac{n}{N}\\Big) log \\Big(1-\\frac{n}{N}\\Big)\\Big] = -p_1 log p_1 - p_2 log p_2 $$\n",
    "\n",
    "- In general for i number of options (random walk in D dimensions, multiple conformations, states etc) we have:\n",
    "\n",
    "$$s= -\\sum_i p_i log p_i$$\n",
    "\n",
    "- Looks like to get the most probably value we could maximize $s(n)$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(p=(0.01,0.99))\n",
    "def large_dev(p):\n",
    "\n",
    "  f = np.linspace(0.01, 0.999, 100)\n",
    "  q = 1-p\n",
    "\n",
    "  I = -f * np.log(p) - (1-f)*np.log(q) + f * np.log(f) + (1-f)*np.log(1-f)\n",
    "\n",
    "  plt.plot(f, I, linewidth=2.5)\n",
    "  plt.ylim(0, 1.0)\n",
    "  plt.xlim(0, 1.0)\n",
    "  plt.xlabel(r'$f$')\n",
    "  plt.ylabel(r'$I(f)$')\n",
    "  plt.title(r'Biased coin large deviation function', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=25\n",
    "f = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "p = 0.5\n",
    "q = 1-p\n",
    "\n",
    "I = ( - f * np.log(f) - (1-f)*np.log(1-f)+  f * np.log(p) + (1-f)*np.log(q) )\n",
    "\n",
    "plt.plot(f, np.exp(N * I), '-o')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which of these two statements conveys the most information?**\n",
    "\n",
    "- I will eat some food tomorrow.\n",
    "- I will see a giraffe walking by my apartment. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A measure of information (whatever it may be) is closely related to the element of... surprise!**\n",
    "\n",
    "- has very high probability and so conveys little information,\n",
    "- has very low probability and so conveys much information. \n",
    "\n",
    "> If we quanitfy suprise we will quantify information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge leads to gaining information**\n",
    "\n",
    "Which is more surprising (contains more information)?\n",
    "\n",
    "- E1: The card is heart?\n",
    "\n",
    "- E2:The card is Queen?\n",
    "\n",
    "- E3: The card is Queen of hearts?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $P(E_1) = \\frac{1}{4}$\n",
    "\n",
    "\n",
    "- $P(E_2)  =  \\frac{4}{52} = \\frac{1}{13}$\n",
    "\n",
    "\n",
    "- $P(E_1 \\, and\\,  E_2) = \\frac{1}{52}$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We learn the card is heart $I(E_1)$\n",
    "\n",
    "2. We learn the card is Queen $I(E_2)$\n",
    "\n",
    "3. $I(E_1 and E_2) = I(E_1) + I(E_2)$\n",
    "\n",
    "4. Knowledge of event can add to information: $I(E) \\geq 0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A logarithm of probability is a good candidate function for information!\n",
    "\n",
    "Because information should be additive!\n",
    "\n",
    "$$log P(E_1) P(E_2) = log P(E_1) + log(E_2)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why bit (base two)\n",
    "\n",
    "Consider symmetric Bernouli process, e.g 1D random walk with equal jump probabilities:\n",
    "\n",
    "> **Random walk = string of Yes/No questions**\n",
    "\n",
    "- to decode fa single step we need one bit\n",
    "\n",
    "$$I(X=0) = I(X=1) = -log_2 \\frac{1}{2} = 1$$\n",
    "\n",
    "- to decode N step trajectory we need N bits. \n",
    "\n",
    "$$(x_0,x_1,...x_N) = 10111101001010100100$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "N = 100 #trials\n",
    "p = 0.01 # probability of sucess\n",
    "\n",
    "W_bin = [binom.pmf(n, N, p=0.1) for n in range(N)] \n",
    "\n",
    "plt.plot(np.arange(N),np.log(W_bin), '-o')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many yes or no on average to reconstruct random walk?\n",
    "\n",
    "**Answer**\n",
    "<br>\n",
    "$$H(X) = \\langle -log p(X) \\rangle$$\n",
    "\n",
    "**Shannon  Measure of Information (SMI)**\n",
    "$$H = -\\sum_i p_i log p_i$$\n",
    "\n",
    "**Surprise**\n",
    "\n",
    "$$I_i = -log p_i$$\n",
    "\n",
    "> Information is an average of surprise.!!! How surprised are you on average? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information per letter $I(m)$ to decode the message\n",
    "\n",
    "- $m:$ Letters in the alphabet (Russian: 33,Enlgish: 26,  Korean: 24) \n",
    "\n",
    "\n",
    "- $I(Russian) > I(English) > I(Korean)$\n",
    "\n",
    "\n",
    "- $I(m_1, m_2) = I(m_1) + I(m_2)$ reagrdless of the order letters are sent!\n",
    "\n",
    "\n",
    "> One bit is an amount of information one can obtain from the answer to a single yes–no question. The number of bits to decode a message grows witht the lengt of an alphabet and length of the word. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphabets are not random! hghjxcjxcc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some letters happen more often than the others! Probability of each letter in an independent sequence is $p_m = \\frac{1}{m}$\n",
    "\n",
    "$$\\boxed{H(p) = - \\sum_m p_m log_m p_m}$$\n",
    "\n",
    "> We must send a message explaining how to combine the transferred symbols as a part of the message, but the length of the needed message is finite and independent of the length of the actual message we wish to send, so in the long message limit we may ignore this overhead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shanon Measure of Information\n",
    "\n",
    "$$\\boxed{H = -\\sum_i p_i log p_i}$$\n",
    "\n",
    "> **[To Shanon], You should call it Entropy, for two reasons. \n",
    "In the first place you uncertainty function has been used in statistical mechanics under that name. \n",
    "In the second place, and more importantly, no one knows what entropy really is, so in a debate you will always have the advantage.” J von Neumann**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How much knowledge we need to find out outcome of fair dice?\n",
    "\n",
    "- We are told die shows a digit higher than 2 (3, 4, 5 or 6). How much knowledge does this information carry? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $H(E_1) = log_2 6$\n",
    " \n",
    " \n",
    " - $H(E_1) - H(E_2) = log_2 6 - log_2 4$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information per Letter of English\n",
    "\n",
    "If the symbols of English alphabet (+ blank) appear equally probably, what is the information carried by a single symbol? This must be $log_2(26 + 1) = 4.755$ bits, but for actual English sentences, it is known to be about **$1.3$ bits. Why?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two kittens\n",
    "\n",
    "There are two kittens. We are told that at least one of them is a male. What is the information we get from this message?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_1 = \\{mm,mf,fm, ff \\} $$\n",
    "\n",
    "$$E_2 = \\{mm,mf,fm\\}$$\n",
    "\n",
    "$$H(E_1) -H(E_2) = log_2 4 -log_2 3 = 0.41$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game of message decoding:\n",
    "\n",
    "Given some 70 letters decode a 250 letter paragraph!\n",
    "\n",
    "$$\\frac{70}{250}log_2 27 = 1.3$$\n",
    "\n",
    "> Cover T. M. and King, R. C. (1978). \"A convergent gambling estimate of the entropy of English\" IEEE Trans. Info. Theory, 24, 413–421"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monty Hall problem\n",
    "\n",
    "There are five boxes, of which one contains a prize. A game participant is asked to choose one box. After they choose one of the five boxes, the “coordinator” of the game identifies as empty three of the four unchosen boxes. What is the information of this message? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $H(E_1) = log_2 5 = 2.322$\n",
    "\n",
    "\n",
    "- $H(E_2) = -\\frac{1}{5} log_2 5 - \\frac{4}{5} log_2 \\frac{4}{5} = 0.722$\n",
    "\n",
    "- $H(E_1)-H(E_2) = 1.6$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non integer number of YES/NO questions??\n",
    "\n",
    "We have encountered a fraction of bit of information several times now. What does it imply in terms of number of YES/NO questions. That is becasue in some cases single YES/NO question can rule out more than one elementary event.\n",
    "\n",
    "> 999 blue balls and 1 red ball. how many questions we need to ask to determin the colors of all balls? $S = 9.97$ bit or 0.01 bit per ball. Divide the container by 500 and 500 and ask where the red ball is? 1 questions rules out 500 balls at once. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is information physical?\n",
    "\n",
    "**Wheeler's It from bit.**\n",
    "\n",
    "> Every it — every particle, every field of force, even the space-time continuum itself — derives its function, its meaning, its very existence entirely — even if in some contexts indirectly — from the apparatus-elicited answers to yes-or-no questions, binary choices, bits. It from bit symbolizes the idea that every item of the physical world has at bottom — a very deep bottom, in most instances — an immaterial source and explanation; that which we call reality arises in the last analysis from the posing of yes-no questions and the registering of equipment-evoked responses; in short, that **all things physical are information-theoretic in origin**\" John Weeler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jayne's MaxEnt (Maximum Entropy principle)\n",
    "\n",
    "Probability is an expression of incomplete information. Given that we have some\n",
    "information, how should we construct a probability distribution that reflects that\n",
    "knowledge, but is otherwise unbiased? The best general procedure, known as Jaynes\n",
    "Maximum Entropy Principle , is to choose the probabilities $p_k$ to maximize the Shanon Measure of Information of the distribution, subject to constraints that express what we do know"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fair die example**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J[p_1, p_2, ..] = - \\sum p_i log p_i - \\lambda \\sum_i p_i$$\n",
    "\n",
    "$$p_1  = p_2 = ... =  p_N $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**biased die**\n",
    "\n",
    "We are given infromation that on average rolling a die yields $\\langle x \\rangle = 5.5$\n",
    "\n",
    "\n",
    "$$J[p_1, p_2, ..] = - \\sum p_i log p_i - \\lambda \\sum_i p_i - B \\sum_i p_i x_i $$\n",
    "\n",
    "$$p_i \\sim e^{-B x_i}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
