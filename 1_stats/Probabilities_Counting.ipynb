{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "@deathbeds/jupyterlab-fonts": {
     "styles": {
      "": {
       "body[data-jp-deck-mode='presenting'] &": {
        "zoom": "132%"
       }
      }
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    ":::{admonition} What you need to know\n",
    "\n",
    "- **Sample space** $\\Omega$ is a set of elementary outcomes or events.\n",
    "- **Events** can contain more than one elementary event and can be constructed by forming subsets ($A$, $B$, $C$ etc) of $\\Omega$ \n",
    "- **Probability function** P(A) assigns a numeric value to each event, A quantifying certainty of an event happening on a 0-1 scale. \n",
    "- **Venn diagrams** visualize P(A) as a \"volume fraction\" of our confidence in the event expressed on 0-1 scale.\n",
    "- **Probability axioms** define a set of logical rules for creating composite events from trivial ones.\n",
    "- **Bayesian approach:** In physical sciences and modeling one often deals with situations where counting is impossible. Hence, probability is interpreted as a degree of belief.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic world of complex many particle systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"./figs/2d_gas_simulation.gif\" alt=\"compton\" class=\"bg-primary mb-1\" width=\"300px\">\n",
    "\n",
    "Characterizing many particle complex systems is best done via probabilistic approach. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the probability to find a gas in upper right corner cell?\n",
    "- What is the probability that all gas atoms will be in left side of the box?\n",
    "- What is the probability distribution of velocities of the gas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"./figs/1d_random_walk.gif\" alt=\"compton\" class=\"bg-primary mb-1\" width=\"300px\">\n",
    "\n",
    "Simulations of a Random Walk in 1D\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the probability of finding molecule away from center by n steps out of N?\n",
    "- How do we obtain probability distribution after N steps given probability for 1 step?\n",
    "- Why is there tendency for probability distributions to evolve towards Gaussian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Sample space\n",
    "\n",
    "- The **sample space**, often signified by an $\\Omega$ is a set of **all possible elementary events**. \n",
    "- Elementary means the events can not be further broken down into simpler events. For instance, rolling a die can result in any one of six elementary events.\n",
    "- States of $\\Omega$ are sampled during a system trial, which could be done via an experiment or simulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "-  If our trial is a single roll of a six-sided die, then the sample has size $n(\\Omega) = 6$\n",
    "\n",
    "$$\\Omega =\\{1, 2, 3, 4, 5, 6 \\}$$\n",
    "\n",
    "- A fair coin is tossed once has a sample size $n(\\Omega) = 2$\n",
    "\n",
    "$$\\Omega = \\{H, T \\}$$\n",
    "\n",
    "- If a fair coin is tossed three times in a row, we will have sample space of size $n(\\Omega) = 2^3$ \n",
    "\n",
    "$$\\Omega = \\{H,T\\}^{3}= {HTT, THT, TTH, THH, HTH, HHT, HHH, TTT}$$\n",
    "\n",
    "- Position of an atom in a container of size $L_x$ along x. $n(\\Omega)$ is a huge number. We will need some special tools to quantify. \n",
    "\n",
    "$$\\Omega = \\{0... L_x\\}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Events, micro and macro states\n",
    "\n",
    "- An event in probability theory refers to an outcome of an experiment. Event can contain one or more elementary events from $\\Omega$. \n",
    "\n",
    "- Event can be getting getting 5 on a die or getting any number less than 5. \n",
    "\n",
    "- In the context of statistical mechanics, we are going to call elementary events in $\\Omega$ **microstates** and events containing multiple microstates as **macrostates** \n",
    "\n",
    "    - If we roll a single die there are six micostates. We can define a macrostate as an event $A$ of getting any number less than 4\n",
    "\n",
    "    $$A= \\{1, 2, 3 \\}$$\n",
    "\n",
    "    - Or we can create a macrostate $B$ containing only even numbers\n",
    "\n",
    "    $$B = \\{2, 4, 6 \\}$$\n",
    "\n",
    "    - IF we roll toss two coins microstates are HT, TH, HH, TT. We can define a macrostate $D$ of having 50% H and 50% T \n",
    "\n",
    "    $$D = \\{TH, HT\\}$$\n",
    "\n",
    "    - A microstate of a gas atom in 1D container could be its position x. A macrostate could be finding atom anywehere in the second half of the container\n",
    "\n",
    "    $$C  = \\{L_x/2, ..., L_x \\} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Compute probabilities through counting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    ":::{admonition} **probabilities of events as fractions in the sample space**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "{ P(A) = \\frac{n(A)}{n(\\Omega)}}\n",
    "$$\n",
    "\n",
    "- $n(A)$ probability of event, e.g rolling an even number. The size of the event space is 3\n",
    "- $n(\\Omega)$ size of sample space. In the context of single die roll is equal 6\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Visualizing events as Venn diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib_venn._common.VennDiagram at 0x17c58c3d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAErCAYAAABDzICRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx9klEQVR4nO3deXCb530n8O/74sVJACTB+5JIkZIsiTotxbKjxEfjuIpV1/Y48bZN2nonO01222ynSXeTGSfdcWZnm0mT2SaNndmddLQbK8402bh12lFkJ0ok35IpWzctiYckUjzAGyBu4N0/3ki2LFIiQADP+7zv9zODoXXyJwN48X2f4/couq7rICIiIttSRRdAREREYjEMEBER2RzDABERkc0xDBAREdkcwwAREZHNMQwQERHZHMMAERGRzTEMEBER2RzDABERkc0xDBAREdkcwwAREZHNMQwQERHZHMMAERGRzTEMEBER2RzDABERkc0xDBAREdkcwwAREZHNMQwQERHZHMMAERGRzTEMEBER2RzDABERkc0xDBAREdkcwwAREZHNMQwQERHZHMMAERGRzTEMEBER2RzDABERkc0xDBAREdkcwwAREZHNMQwQERHZHMMAERGRzTEMEBER2RzDABERkc0xDBAREdkcwwAREZHNaaILIAnE40AsBiSTQCJx49d0GsjlAF03vr6fqgJOJ+ByLfzw+YBAAPB4xPzbLC6ZSSKSiiCWjiGVTS36yOnXP28KFKiKClVR4XQ44Xa44dE8cGu//epww6254XP64HP6BP3riKhYGAbIEI0Cs7PA3JzxeP9/ZzKl//6aZoSCQADw+42vVVVATY3xY1pULB3DZGwS04lpRFNRRJIRRFIRRFNRpLKpkn9/TdUQcAUQdAcRdAdR6ak0vrorEXAHSv79iWj5FF3XddFFUJnFYkA4fP0jkRBd1eI8HiMU1NQAtbXGo7ISUBTRlZXdXHIOE7EJTMQmMBmbxERsAvFMXHRZi3I5XKjz1aGuou7aV7+L4Y7IbBgG7GBmBhgaAq5cAcbHjTAgO5cLaGwEWlqMRygkuqKSmEnMYHhuGFciVzASHUEiY+LQtkRezYu6ijo0+ZvQVtmGkNeazx2RTBgGrCiZBIaHjQAwNGRMAVidxwM0NxuP1lYgGBRdUUHmU/MYmhvCcMQIALG0BYLbLficPrQEWtBW2YaWQAu8Tq/okohsh2HAKqJRoK8PGBgwhv3t/rTW1ACrVhmPykrR1dxUNBVF/3Q/+qf7MT4/Lroc4Wp9tWivakdXqAtBt5yhjkg2DAMyi8eB/n4jBIyOiq7GvEKh94JBVZXoagAAkWTkWgAIx8KiyzGtWl8tukJd6KzuRIWrQnQ5RJbFMCCbTMb48L9wwVgDwKcvPzU1wLp1QFeXse6gjDK5DPqm+nB24ixHAArQ6G9EV6gLXaEuuBzlfe6IrI5hQBZzc8Dp08C77wKp0m8XszxNMwLBunVAXV1Jv9V0fBpnJ87i3OS5smz1szpN1bA6tBob6jdw8SFRkTAMmJmuA5cvA6dOGQsBqTRqa4H1641woBWn9UZOz10bBRiNcgqnVBr9jeiu70Z7VTtUhQ1ViQrFMGBGmQxw5owxEhCJiK7GPjweoLvbeBQ4hZDJZXA2fBbHx47bYieAWficPqyvW4/u+m5OIRAVgGHATDIZIwCcOGEsDiQxXC5jpGDTpiW3SU5lUzg9fhonx09aoheArFwOF7rru7GpYRNDAVEeGAbM4GoIOH7c3J0A7UbTjDUFmzYBFQuvZE9kEjg5dhKnw6e5HsBEGAqI8sMwIBJDgBwcDmDDBmDrVsDtBmBMBxwfPY7jY8eRyZXh7AYqCEMB0dIwDIhy/jxw5AgwPy+6Eloqtxv61q041+TC0ZG3uCZAIh7Ng+3N27Gudh0UG55pQXQrDAPlNj4OvPaa8ZWkkqhyY6pGR8yZwwWfhksKR3NkU+2pxp1td6I12Cq6FCJTYRgol0QCePNNo08ASSXj0TDdpCGmXv/hP+/x4JQnixmkBVVGheqo6sCdbXfyBEWi32IYKIezZ40pgWRSdCWUp0ijFzMVSeSQW/DXdUXBsN+Ls47YIr+DzEpTNWxr2obNDZs5dUC2xzBQStEocOiQcYIgSSXt1TDV5EBCWVqAS7pcOOkFJhXuKJBNfUU97mm/B1WeKtGlEAnDMFAqvb3AG2+wdbBkdACRZh9mvHHoyPOtoQAjfh9OcZRAOg7FgR0tO7CxfiNHCciWGAaKbX4eOHzYaCNMUkn7nJhsVJBc5t190uXCaS8Q5iiBdBr9jbin/R4enUy2wzBQTBcuAK+8wtEACUXrvZgKJPIfDViMAlzy+3DWwe2HstFUDXe03IEN9RtEl0JUNgwDxZDNGtsFz54VXQnlKacqmG7zIKqVpv3znNeDHncKKU4cSGdV9SrcvfJuOB1O0aUQlRzDwHJFo8BLLwHhsOhKKE9pnxPhJgVplHYkJ6NpOFGhctpAQlWeKny88+NcXEiWxzCwHENDwMGDbCUsofk6L6aCi28ZLDoFuBjwolflAVSycapOfHTlR9EZ6hRdClHJMAwUqqcHOHYM4P8+qegApld4EXGK+VCe8XnxljOBrMLXjWy667uxs3UnVEUVXQpR0TEM5CuTMUYDBgdFV0J5yjkUTKxwI66KHcmJu9044k0jwXUE0mnyN+HjnR+HW3OLLoWoqBgG8pFIAL/4Bc8VkFDW7cB4qwOpEq8PWKq0U0OPT8GswlbGsqn2VGP36t1sZUyWwjCwVHNzwP79wOys6EooTym/E+MNOrIw11HDWYcDpwIaRsE21bLxOX3Y3bUbNb4a0aUQFQXDwFKEw8aIQJyLv2QTD3kwUZ0q30LBPOmKggtBD/oVvrZk41SduL/zfp6ASJbAMHArly4Bv/ylsVaApDJf58VkMF6sNkIldTHoQ6/KBkWyURUVH135UaypWSO6FKJlYRi4mf5+Y7Fgzpx3lbS4aL0XkwG57rYvBdixUFa7VuzC+rr1ossgKhj3yCxmYIBBQFIyBgEAWBGJYUPWJ7oMKsArl17BmfAZ0WUQFYxhYCEDA8CvfsUgIKFog5xB4KpWBgJpvXLpFZwNsyU5yYlh4IMGBxkEJBVt8GLSL28QuKo1EsNGBgIpvXzpZfRO9IougyhvDAPvNzhoLBZkEJCOVYLAVc0cIZDW4YuHGQhIOgwDVw0PMwhIKh7yWCoIXNUaiaFL94ougwpw+OJh9E31iS6DaMkYBgBgaso4eZBBQDrJoAvhaus27emci6NF94gugwrwm8HfYDQ6KroMoiVhGIjFjM6CKXO0qaWlS3s1hOuy0KXoJFAgHdgQSaEWLtGVUJ6yehYHLhzATGJGdClEt2TvMJBOG0Fgfl50JZSnrMuB8WYgi6zoUkpOyeWwJZJFQNdEl0J5SmaT2H9+P+Jp601jkbXYNwzkcsYagclJ0ZVQnnKqgvFWBzImO2uglBzZLHbEFHhs/JaVVSQVwYG+A8jk7PN6JfnY98ry6qvA5cuiq6ACTK3wIKXYb1rHmU5jR8IJxcKzIlY1Pj+OgwMHRZdBtCh7hoHeXuAsm4PIaK7Jh3mHfYdcfYkkNnPLoZQGZwZxbOSY6DKIFmS/MDA5aYwKkHQSlW7M+Ni7vyEaw0ruMJBSz5UeDM8Niy6D6Ab2CgOplLGFMGv9RWdWk3U5MFGbsfK+gbysiaQQBBcUykaHjoMDBxFLM9SSudgrDPzmN8DcnOgqKE86gIlWzRY7B5ZKzeWwLaZCgyK6FMpTPBPHL/t/iZzOviZkHvYJAydPGu2GSTozbT4kFOs2FiqUO5XC7SlOF8hoNDqKo8NHRZdBdI09wsD4OPDmm6KroAIkqtyYc3FIdTFVsTg6cmxZLKPjY8dxafaS6DKIANghDGSzxvQAWw1LJ+dQMFnDqYFb6ZxPwQeH6DKoAIcvHkYyw1EvEs/6YeDoUWBmRnQVVIDpVo+tGgsVypHNYkvSKboMKkAsHcNrl18TXQaRxcPA+LixVoCkEw95ENXs208gX4F4giccSur81HlcnLkougyyOeuGgavTAzo3o8kmp6mYrOaIQL46IklU6JwukNHLl17mdAEJZd0w8NZbnB6Q1FSLG1lOD+RNzeWwldMFUoqlY3h96HXRZZCNWTMMjI8DJ06IroIKEK92Y57TAwWrSCTQyd0FUjo3eY67C0gY64UBXQdeeYXTAxLSFWA6xF0fy9Uxn4LLgm9tO3j10qvI5riDhsrPeleMd98FJiZEV0EFmGvyIo206DKk58hm0Z1hMyIZRVIRHB87LroMsiFrhYFUCjhyRHQVVICs24E5LxdQFUvdfAzVOtcPyOid0XcQTUVFl0E2Y60w0NMDJBKiq6ACTDe6kAOnCIpGB9YnubNARplcBkeGeVND5WWdMDA3B5w+LboKKkAq4OKiwRLwJxJo5VHHUrowdQHj8+OiyyAbsU4YePNNthyW1HQdT94rldWxLBSupZXSG0NviC6BbMQaYSAcBgYGRFdBBUhUuXkiYQm50mm0szOhlEajo9xqSGVjjTDQ0yO6AirQbEh0BdbXHs9wdEBSx0aOiS6BbEITXcCyTUwAl5ieZcRRgfK4OjowoHBdhmzG58cxNDeE1mCr6FLKLpEA5ueBWAxIJo1Z4KszwaoKaBrg87330OT/NBNK/v99x5icZTVbzdvVcmmPZzDoMxo7kVx6rvRYOgzEYsZMbzhs3NtNTRk/l+8SMJcLCAaB2lqgrs74GgoBDm6qWRK5w8DUFDA4KLoKKkCiyo2EylGBcnGl01ipezCocOutbMbmxzA8N4yWYIvoUooikwGGh4GLF4HLl427/2JIpYwwMTEB9PYaP6eqQH09sHIl0N4OVFYW53tZkdxhgKMC0uJagfLriGdxkaMDUjo2ckzqMJDNGmu8+/qMIJAp0zlkuRwwOmo83nzTCAPt7cDatUBVVXlqkIW8YWBmhjsIJJUMurhWQABXOo02eHAJHB2QzUh0BKPRUTT6G0WXkpdIBDhzxugSb4Z+cLOzwPHjxqO5GVi/3ggHqjWW0i+LvGHg9GkeRiSpSDUn8URZkQQusQ+RlE6Nn5ImDIyPGwO3ly+b9zJ95Yrx8PmA7m7jYedFiHL+09Np4Px50VVQAbJOFTHNBLcINlWRSKDS7cSswgOhZDM4M4hYOgaf0ye6lEXNzABHj8o1aBuLGUfanDoFbNsG3HabPUcK5PwnX7hgrBYh6UTrPNBh0lsFm+jM8gAjGeX0HHonekWXsaBYDDh8GPjJT+QKAu8XiwGvvAL80z8B/f2iqyk/OcPAmTOiK6AC6ACiXoY40WpiCWjgKkIZnQ2fRU43V9v1CxeMENDba94pgXzMzQG//CVw4IAREOxCvjAwNgZMToquggoQr/EggzItI6ZFqbkcOnJcOCCj+fS8aVoUx2LAiy8CBw8aTYGs5uJFI+TYZUZavjDAUQFpRYKiK6CrWhJZ0SVQgU6Piz+ddWDA+KC0epuXZBL49a+N0GP1mWm5wkAyac/JHAvIuB1IqFw4aBbuVAo1OtcOyGg4Moy55Jyw7//WW8BLL1lzNGAxg4PA888bCyStSq4wMDBgdK8g6cRCbtEl0Ae0ZuXcTERA31Rf2b9nJmOEALv2epudBf75n43tklYkVxjgqIC0Yl6uFTCb2gS3F8qqb7q8YSAaNT4IZd0pUCypFPCLXwAnToiupPjkCQOJhNEhgqST8WpIKhafcJOQlsmgHi7RZVABpuJTmEnMlOV7RSLAz39uHAVDxo6JN94wpkusRJ4wMDCQ/zFWZArz1fzAMauWNLtByqp/uvQjpXNzwAsvGIGArnfsmNGsyCrkCQN95Z8jo+LgFIF51SRSUCywN9yOSr1uIBoF/vVfi3eqoBW98w7Q0yO6iuKQIwzE48DIiOgqqABpnxMpcIrArBzZLBo5VSCl6cQ0puPTJfm7Uyng3/7NCAR0cz091tjxLkcYGBy0RmsrG4pXcfua2TVxV4G0BmaKv6JP140OfLOzRf+rLeu11+Rf0iZHGBgaEl0BFSju4ToPswsmuatAVkNzxb82vvEGL7n5yuWMACXz2grzhwFdB4aHRVdBBcipCpKKjTqTSMqdTsMHLiSU0fj8ONLZ4oW5c+eAkyeL9tfZSiJhnGeQljRbmz8MhMPW7wNpUclKN08olESzznUDMsrpOVyJFGd8emYGePnlovxVtjU1Bbz6qugqCmP+MMBRAWkl/OZ/eZGhRtK7GTLaEy+XrgOHDrHBazGcOwdcMsdZUnkx/9WaYUBacRe3FMoikOTom6yG55Z/jTx50jgQlorj8GH5BrTNHQYyGb5CJZV1OZDmlkJpOLJZhMCdHzKaTkwjlo4V/OdnZoCjR4tXDxnHO7/2mugq8mPuMDA2xnErSSWCnIOWTX2OYUBWy1k38OqrvMyWwrlzwOio6CqWztxhIBwWXQEVKOVVRJdAeQpmudhTVuPz4wX9uaEhzsSW0ptviq5g6RgGqCRSTt5qyKYixTUespqITRT056zUW9+MxsaMnnkyMHcYmCjsBU7ipRQuT5eNK52GBo7oyGgiNgE9zy6tfX28xJbD0aNyNNA1bxhIJuVu52Rjaa+GHNh5UEa1PKdASplcBrPJ/PoHW+0IXrOangYuXBBdxa2ZNwxwikBaKT8XosmqKsdOhLIKzy/9mnn5Ms8eKKdTp0RXcGvmDQMcv5JWysOhZllVZiQYz6QF5bNu4PTpEhZCNwiHzX9/yzBARZdycopAVt4MF37KaqlhIBo1RgaovMx+zLF5wwDHsKSV4eJBabkyfO5ktdQ1A2fOyLGgzWr6+oylcGZl3jDAxYNS0hUgA95dykrJ6TzBUFKxdAyZ3K23h8qwmM2KMhlgYEB0FYszZxhIJORr7EwAgIxHE10CLVMQfA5lFUne/CZqYsKYJiAxLl4UXcHizBkGOCogLYYB+VXkuABUVpHUza+dZv4wsoPhYWOEwIwYBqioMi5+kMiuQjfnZYFubS45d9NfZxgQK5Mxb/tnc77r527+gibzYhiQnyfL3SCyutk0QSzGTVpmYNZAZs4wwJEBaWU0fpDIzsMDi6R1s5EBmU7Qs7KxMdEVLMycYWB+XnQFVKCswg8S2Wk5BjpZxdKxRX+NowLmMDNjznUD5gwD3EkgrZzCDxLZqQwD0kplF792MgyYg66b87kwZxgwc2cGuikeUCQ/R459ImSVzC5+7TR7O1w7YRhYKoYBaTEMWIAOOHUuBJXRYiMD0Sgvq2bCMLBUnCaQkq4q0ME1A1bgVtiFUEY5PYd09saW0mw0ZC5mfD7MFwZyOXOurqBbyjnN93KiwrjZa0BaC00VxBZfV0gCmPH5MF+7OAuOZT1z6BCeOXQIg5OTAIANTU342p492N3dLbiy4so5VMCi5xJ870f7ceCVt9F3eRQetwvb1q/Cl//Do+hsaxRdWkm4Yb1pgkM/PYRDPz2EyRHjfdi0qgl7PrsH3R+21vtwoakC2TdonTt3GC+++E1cutSD2dkRfP7zz2PLlodFl1UwhoGlSFvv1LTWqir87SOPYHV9PXQA/+f11/H7Tz+Nt598Ehuam0WXVzQ5zXofIFe9eeIcPvP792Dz2nZksll88wf/jD/+r3+Pl37w3+DzukWXV3SaBcNAVX0VHvnzR1C/oh7Qgdf/9XU8/cWn8eS+J9HcaZ334UJhwIwfPvlIpebR2roZH/7wv8f3v/+o6HKWLZUyBsA1E30Cm6iU37Lgtqbf27z5uh//94cfxjOHDuGN/n5LhQEo1vsAuer//u1/vu7Hf/df/hS3P/YlnDx/EXdsWiOoqtKx4jO5+aPXvw8f/k8P49D/O4T+k/2WCgP6AucTx+MCCimi7u7d6O7eLbqMoorHgUBAdBXvMV8YsPhB29lcDj/p6cF8KoU7V60SXU5RWfuZu15k3ri6VgUqBFdSGool48B7ctkcen7Zg1Q8hVWbrPY+vPGdmLXm7J3UzPacMAyUycnhYdz5jW8gkU7D73bj+c99DuutNCoAWPN2cgG5XA5PPf1P2L6hE2s7WkSXUxq6bsnnc/jCML7xxDeQTqXh9rrxuW9+Ds2rrPU+zOk3jq5a9LIqNbMNgpsvDFjU2oYGvPPkk5iNx/HTY8fwJ3v34tAXv2i9QGADX/3Oc3h38Ap++j//WnQpJWPVkYGGlQ148kdPIh6N49ivjmHvf9uLL/6vL1ouEBDly3z7hyw67+zSNHTV1+P2lSvxPx55BJtbW/H3Bw+KLqu4bHD38bXvPoeDb57Ej//ur9BUVy26nJKxas8hzamhvq0eK9etxCN//gha17Ti4HPWeh8uFOQselmVmmqyT1/zjQzY5FWb03UkLdZPwcrPnK7r+Jt/+DEOvPIOfvytv0JbU63okkrKLs2j9JyOTNpi78MFrqEO9pAyHYaBW7FgGPjK889j94YNWBEKIZJM4kdHjuA3587hwBe+ILq04rLwxORXv/Mc/uXgEfzvp/4jKnwejE/NAgCCFV543C7B1dFSPP8Pz2PDXRsQagwhGUviyC+O4FzPOXzhu9Z6H6rKjZ8yHo+AQoookYgiHL5w7ccTEwO4fPkdVFSEEAqtEFhZ4bxe0RVcz3xhwOkUXUHRjUci+OO9ezEyO4tKrxebWlpw4AtfwP3r14suraiUrHXDwLM/PwQA+Hdf/NZ1P//Nv/4TfPKBu0SUVFJpC44MRKYi2Ps3ezE7MQuv34uW1S34wne/gPU7rfU+dKo3XkMrJN/0cvHiW/j2t++99uOf/OSvAAB33vkn+NM/3SuoqsI5neb7qFP0hTalipTNAj/4gegqqABZtwNDrSbbL0MF6alyYQI8I0RGf7jxD+F3+a/7ub4+4Fe/ElQQ3aCyEnj8cdFVXM9ksxYwJrc4wSUlNW2yvTJUsOQC29NIDi7HjdNWPp+AQmhRZhypMV8YAAC39dq72oGS0y27Jc1ukgrDgIwUKAuGAb9/gd9MwjAMLBXDgLRUk76kKD8pMAzIyK0tfO0MBAAX17maRk2N6ApuZM4rN8OAtBgG5JflNJ20FhoVuKrW2rthpVJXJ7qCG5nzys0IKy1VN+dLipYup3KqR1Zux+I3UgwD5sGRgaXiahdpOazaus5GMipHBmTldS6+ed2Md6N2VFlpzvtdc4YBM53rSHnRMuZ8SdHSJTgyIK2gO7jorzU0lLEQWpRZnwdzXrkZBqSlpc3VtoLyl9DMeVmgWwu4Fr92+v1AKFTGYmhBK0zaMNGc7/rg4umWzE1LMQzILqbyOZTVzUYGAGDlyjIVQgtyOIC2NtFVLIxhgIpKS7ADoeyi4HMoq4D75qOqDANiNTWZrw3xVeYMAx6Pef+P0U1p8bToEmiZIgwD0rrVyEB9Pddni9TeLrqCxZkzDABcNyApRQc0E55/RUujKwrmFYYBGXk1LzT11u+9rq4yFEM3UFWgo0N0FYszbxiorBRdARVI07k1TVZpjUFOVpWepV0z160rcSG0oFWrzHds8fuZNwywQ4a0XGmGAVnFnAwDsqr1Le2aWVkJtLaWuBi6gdlPrGcYoKJzJbgaXVYRjT0GZLXUMACY/4PJampqgMZG0VXcnHnDANtlScs1nxFdAhVomusFpJVPGFi5ksuyymnDBtEV3Jp5w4DHY85zHumWnLE0DyyS1ARSokugAmiqhmpP9ZJ/v6IAt99ewoLommAQWLNGdBW3Zu4rNkcHpOXSuTVUNimnE2mFUzwyCnlDUJT8pnhWrwaql54fqEA7dhg7CczO3CVy3YC0uIhQPlw8KK86X/43TopifFBR6dTWAp2doqtYGnOHAY4MSIuLCOUz5+DiQVnVVRR2rWxvN+/BOVbwoQ+JrmDpzB0GGhvlGF+hG7jnOPcsm7CDCz9l1RxoLvjP7trFy2wprFol1xZOc78EnE6jfyZJR0tm4QTXDcgi63Bw8aCkKt2V8Lv8Bf/5mhpg69YiFkTweIyQJRNzhwEAaGkRXQEVyJNmGJBF1O0SXQIVqCW4/Gvk1q1GKKDi2LXLCAQyMX8YkGmcha7jjeZEl0BLNMm1g9JqDS7/GqmqwD33cLqgGFatMh6yMf9TX1cHuHjXIiP3bBJckiaHEZWnTcpIgbKs9QLvV1MD7NxZlL/KtoJB4CMfEV1FYcwfBlTVOASapKNmdbh1t+gy6BZSTiei4OJBGdVV1MHlKN7NUnc3cNttRfvrbMXpBB54AHBLeskzfxgAOFUgMU+S/QbMbs7FOQJZFWOK4IN27TJ/H32zURTgd35H7iZOcoSB9nbRFVCBvLMcfja7UY09IWTVUdVR9L9TVYH77wf8hW9QsJ0dO4AVK0RXsTxyhIGKCkZVSbmiaW4xNLGsw4ErSIgugwpQ6a5Eja80WwC8XuATnzC+0s11dwNbtoiuYvnkCAOAPD0d6Qa+BIehzWrG44LOVZ5SWlVd2iXrVVXAgw/Kt0WunNavB+66S3QVxSFPGOjoMCZmSDoV01ycZlbDGrd/yqozVPobpFAI+L3fA3y+kn8r6XR3y9dY6GbkCQM+H3cVSMoZS8MFbg81m4xDwyiSosugAlR5qhDyhsryvaqrjUAQDJbl20lh2zbrjAhcJU8YAOTs5EAAAF+cUwVmM+3lFIGsSj1F8EGVlcAjj7AhrKYZuwa2bxddSfHJFwbYIktKFdPse282Qxqnb2TVFeoq+/d0u4Hdu43hcTuqqAAeesi6y9fk+mT1eLjNUFJaPAMPGxCZRsrpxDgPJpJSo78RVZ4qId9bVY3h8bvvNu6S7aKpyRgZqa0VXUnpyBUGAGP5JknJPyffy82qRrw2upJbzPo68dfAtWuBxx6z/jIuTTPCjx0WUcp3dW5uNva8kHR8k3E4wI6Eoumqin6VCwdl5NW8ZV8vsJhg0PiQvOsua44SNDUZgccu0yLyhQGAowOSUnTAn+CuAtGmvG6kwC2FMlpbuxaqYq7Ldne38aHZUfxmiEL4fMZhQ3bbQSFnnluzBjhyBMhwAZRsAhMpzLUCbIArTh8XDkpJgWKKKYKFBINGC+PxcePSfOWK6Iry53IZnQS7u6050nErcv6TXS6gqwvo7RVdCeXJkczCl/Vi3hEXXYotxdxuTCucIpBRW2Ub/C5zHxhQXw/s2QMMDQE9PcDYmOiKbs3lAtatM4KArCcOFoOcYQAANmxgGJCUfzqHeQuvyjWzyx5zDTHT0nXXyzN53dpqPCYngdOngQsXzDeQW1NjfIx0ddlzJOCDFF3X5R2xPXAAuHhRdBVUgNFVLiQVbm0rp7RTw699GTYaklB9RT0evu1h0WUULJUCzp0D+vuN0QJRnzpeL7BypbEboqFBTA1mJXce2raNYUBSldMKxsvTTZV+65LPCV0x2e0ZLcm2pm2iS1gWl8uYi+/uBhIJ4NIl49I9NASkS3zKeShkHC/c3m5MY9DC5A4DdXVAWxtw+bLoSihP3ukk3NUcHSiXtFNDH2IAOCwgm1pfLVZUrhBdRtF4PMYa8DVrjBGC6WlgYsJ4hMPA1FRhAUFRAL/faAxUV2d8ra3lqYtLJXcYAIzRAYYBKVXOqBivFl2FPXBUQF6yjwrcjKIYd+6hkBEOrkqngVgMmJ83viaTQC5nPBTFeGiasQ2wosL46vXyYNvlkD8MNDQYp2cMD4uuhPLknUrAXcXRgVJLOzX0K9y9IaOQN4T2qnbRZZSd02kcjlRZKboS+7DG0uJt1k3OVlc5Y42XoJld9jrZYkhSVh4VIHOxxpW4qclYO0DS8U4l4NbZlbBU0k4NfSpHBWRU66s1Tethsj5rhAEA2LmTE0aSqprk81Yq/T6NowKSurP1TtElkI1YJwxUVwO33Sa6CiqAZzYJX9YrugzLibndGFQSosugArRXtaMpYPEjAclUrBMGAGDHDmNDK0mneiwNhdveiuqsV95+YnamKiruaLlDdBlkM9YKAx4PcPvtoqugAmjxDIJJjg4Uy2SFFxPgLg0ZbWrYhEoPl9FTeVkrDABGs+lqbl6XUfBKHJoFdruKllVVnHLyMCIZVTgruIOAhLBeGFBVYNcu0VVQAdScjuoZhoHluuR3I8Flg1K6s+1OaCrfA1R+1gsDgLHVcL05z/2mm/NNJuDLsX9ooeJuN86zwZCUOqo6uJWQhLFmGACAO+4AAgHRVVABQkMpOOAQXYZ0dFXBO94sTyWUkEfzYNcKjmh+0N69e6EoyrWHx+NBc3MzHnjgAXznO99BJBIRXaJlWDcMOJ3A3XeLroIK4EjnEJp1ii5DOpf8HsyB5w/I6K62u+B1cgHtYp566in88Ic/xDPPPIO/+Iu/AAD85V/+JTZu3IgTJ04Irs4arD051dxsTBecOSO6EsqTbyKBCr8X8w4OeS9FzONGLzsNSqm9qh1doS7RZZja7t27sX379ms//spXvoKDBw9iz549eOihh3D27Fl4vQxTy2HdkYGr7rjDONeSpFM9nOR0wRLoqoq3PVnRZVAB3A43PrLiI6LLkNJ9992Hr371q7h48SKeffZZ0eVIz/phwOkE7rmHrYol5EjnUDPN6YJbGfC7EeX0gJR2rdjF6YFl+MxnPgMAePHFFwVXIj/rhwHAmC7gyYZS8k4lEEjzYrmYOa8H5zk9IKX1devRGeoUXYbUWltbUVlZib6+PtGlSM8eYQAwwkBrq+gqqADVQwm4eLLhDdJOJ3rc7DIoo1pfLQ8iKhK/389dBUVgnzCgKMB99wEVFaIroTwpOR11V3JcP/A+uqrgnQogxeZC0nE5XLh/1f1wqHw9F0M0GkWA28iXzT5hADDOLvjYx4wuhSQVLZFBzRTXD1x1IeDBFNKiy6AC3Nt+LwJufngVw9DQEGZnZ9HVxd0Yy2W/T8WGBmOHAUnHO51AVcInugzhJiq86GeXQSltbtiMlVUrRZdhGT/84Q8BAA888IDgSuRnvzAAABs3Ap1cuCOjyuEYvDZuV5xwu/C2k0FARi2BFuxo2SG6DMs4ePAgvv71r6OjowN/9Ed/JLoc6Vm76dDN3HMPMD8PjI6KroTyVHs5hdGVLqRtdkRvRtNw1JvlKgEJhbwh3N95P1TFnvdfy7V//3709vYik8lgbGwMBw8exEsvvYSVK1fihRdegMdj3xuEYlF0XddFFyFMMgn8y78AMzOiK6E8ZdwOjLYqyNpkf31OVfFWwIFphesEZONz+vDwbQ/D72Lzs3zt3bsXTzzxxLUfu1wuhEIhbNy4EXv27METTzzBxYNFYu8wAABzc0YgiHPoVTYpvxNjDVnkLH6vrCsKTlS6MIqk6FIoT07ViYfWPoQaX43oUohuimNWwSDwwAOAZt8ZE1m5omnUTTph9d6S54IeBgEJqYqKj636GIMASYFhAADq640eBGxZLB3PTBI1c9btUHgx6MMgdw5I6cNtH0ZbZZvoMoiWhGHgqvZ2nmEgqYpwHNVx6205HPP70KvGRJdBBdjZuhPr6taJLoNoyRgG3m/1auDuuxkIJBS8ErNUIBj3+/COxiAgo52tO7GpYZPoMojywjDwQWvWMBBIyiqBYMzvw9sMAlK6o+UOBgGSEsPAQtasAT76UdFVUAGCV2KojskbCMY4IiCtD7V8CJsbN4sug6ggDAOLWbuWgUBSwZEYQvPyLSocCTAIyGpH8w5sadwiugyigrHPwK1cuAD85jdAztp72a0o0ujFVIUcK/FHAj6ccDAIyEaBgjvb7kR3fbfoUoiWhWFgKa5cAV58EUjZq/2tFczXejFZmYAO877MB4NevKvKEVroPQ7Fgfs67kNHdYfoUoiWjWFgqaamgP37jfMMSCrJSjfCtRlkkRVdynV0VcXZgAuXlYToUihPbocbD3Q9gEZ/o+hSiIqCYSAf8/NGIJiaEl0J5Snt1RBuVpCGOXr7Zx0OvO13YFLhaJNsAq4Adq/ejSpPlehSiIqGYSBfqZQxZXDliuhKKE9Zp4pwmxNJRWxr36TLhaPeLOYVc41U0K3V+mrxu12/C59T3h0rRAthGChELge8+SZw8qToSihPuqpgcoUH8w4xc/RRjwdH3EmkFb7tZLM6tBofWfkRaCrPMSHrYRhYjv5+4NAhIG2OoWdaukijF9MV8bIuK7wS8OGUGoPOflZSURUVd7XdhfV160WXQlQyDAPLNTNjTBvMzIiuhPKUCrgQrs8hg0xJv0/W4cAZv4YrgqcnKH9+lx/3r7ofdRV1okshKimGgWJIp4HDh4G+PtGVUJ5ymorJNhdiamlW9Mc8bvR4MoiZbCcD3VprsBX3ddwHj+YRXQpRyTEMFNPp08Zagkxp7zSp+OaafJjxxYo6bTAS8OEkpwWkoyoqtjVtw9bGrVB4RgnZBMNAsc3OGh0Lx8ZEV0J5SgZdmKzTl739MKNpOFvh4LSAhELeEO5pvwe1vlrRpRCVFcNAKei6sdPg6FEgy+FhmegKMNvsxZynsMWF4QovTjgTyJi44yHdSIGCLY1bcHvz7VAVHtlC9sMwUEozM8Cvfw2Ew6IroTyl/C5MNgApLK0pUNrpxJkKFaPgaIBsqjxVuLf9Xi4SJFtjGCg1XQeOHweOHeNaAsnoCjDX7MOsJ37Tsw3G/D6cdMSRZe8AqaiKio31G7G9eTscqkN0OURCMQyUSzQKvPYaMDgouhLKU9rnxFSjisQH1gAk3C6c8QBhthSWTnOgGbtW7GJLYaLfYhgot8uXgddfZ18CCcVqPJiuyiDp0DHod+GCwpMGZeN3+bGzdSdWVa8SXQqRqTAMiJDLAWfOAD09QJJzzNLQNOibNuJMqwtHR99GKssRAVk4VSe2NG7BpoZNnBIgWgDDgEjJpLGe4PRptjQ2M0UB1qwBtm8HKioAAIlMAsdGjuFM+Axyek5wgbQYh+LAbbW3YWvTVh4uRHQTDANmkEgAJ04wFJiNqgJdXcCWLUBV1YK/JZqK4p3Rd/DuxLvI6txGahYOxYF1deuwpXELQwDREjAMmAlDgTk4HMDatcDmzUAgsKQ/EkvHcGLsBM6EzyCT464RURgCiArDMGBGiQRw6pSxriBRmp75tABNA9avBzZtAnyFfZAkMgmcGj+FU+OnuKagjFwOF9bWrMXmxs0MAUQFYBgws2zWOCb59GlgfFx0NdYVDALr1hmjAZ7iHEqTyqZwbvIczobPYjoxXZS/k24U8oawoW4DVteshqZqosshkhbDgCzCYSMU9PWxxXExqCqwcqURAlpbS/qtRqOjOBs+i/7pfq4rKAJVUdFe1Y4NdRvQFGgSXQ6RJTAMyCaRAM6fBy5cYJvjQvj9740CFDgVUKhkJol3J99F70QvZhIzZf3eVhDyhtBZ3Ym1tWs5FUBUZAwDMpubM0JBXx8wzaHoRfn9QEcHsGoV0NAguhoAwERsAv3T/eif7sdcck50OaYVdAfRWd2JzlAnQt6Q6HKILIthwCqmpoxQMDDA7oaAsQvgagCorxddzU1NxibRP92PgZkBjhgACLgC6KjuQGd1Jw8PIioThgErikSAoSHjMTwMpGywql3TjLv+lhZjDUCtnOfRT8WnMDQ3hCuRKxiJjCCds/4WU6fqRHOgGa3BVrQGW1HpqRRdEpHtMAxYna4bawuuBoOJCWv0MFBV446/udkIAA0Nxs9ZSE7PITwfxnBkGFciVzAWHbPEAkRN1VDjrbkWABr8DVAVaz13RLJhGLAbXTemEcLh9x6Tk+beoaCqQChk3O3X1Lz3VbPXVrJsLoup+BQmYhOYjE9iIjaBqfiUqZscqYqKkDeEOl8d6irqUOerQ7W3mh/+RCbDMEDGwUnT08DsrLEo8f2PaLR8dfh8xlx/IGAs+quqMj74q6osd9dfLLquYyYxg8n4JGYSM4gkI4ikIogkI4ilY9BRnre3z+lD0B1E0B1EpbvS+OqpRLWnmgcDEUmAYYBuLps11iDEYsbBSsmksb3x/V/TaSNQXH1cpSjGw+kEXK6FH1cDgN9vtAGmosnpOURTUURTUcyn5pHKphZ96NDx/kuBqqjXHpqqwaN54NbcxleH+9p/ezQPgu4gG/4QSY5hgIiIyOY49kpERGRzDANEREQ2xzBARERkcwwDRERENscwQEREZHMMA0RERDbHMEBERGRzDANEREQ2xzBARERkcwwDRERENscwQEX19NNPQ1EU3HHHHaJLoSXYu3cvFEW57lFfX497770X+/fvF10eEZUJTxehotq3bx/a29tx5MgRXLhwAV1dXaJLoiV46qmn0NHRAV3XMTY2hr179+ITn/gEfv7zn2PPnj2iyyOiEuPIABXNwMAAXnvtNXz7299GXV0d9u3bJ7okWqLdu3fj05/+ND7zmc/gS1/6El5++WU4nU4899xzoksjojJgGKCi2bdvH6qrq/Hggw/iscceYxiQWFVVFbxeLzSNg4dEdsAwQEWzb98+PProo3C5XPiDP/gDnD9/HkePHhVdFi3B7OwsJiYmEA6Hcfr0aXz+859HNBrFpz/9adGlEVEZMPZTUfT09KC3txff/e53AQC7du1Ca2sr9u3bhx07dgiujm7lYx/72HU/drvd+Md//Efcf//9gioionJiGKCi2LdvHxoaGnDvvfcCABRFweOPP45nn30W3/rWt+BwOARXSDfzve99D2vWrAEAjI2N4dlnn8VnP/tZBAIBPProo4KrI6JSU3Rd10UXQXLLZrNoa2vD3Xffja9//evXfv7tt9/Gpz71KRw4cAAf//jHBVZIi9m7dy+eeOIJHD16FNu3b7/287lcDlu3bkU4HMbg4CBcLpfAKomo1LhmgJbt4MGDGBkZwY9//GOsXr362uNTn/oUAHAhoYRUVcW9996LkZERnD9/XnQ5RFRinCagZdu3bx/q6+vxve9974Zf+9nPfobnn38e3//+9+H1egVUR4XKZDIAgGg0KrgSIio1hgFalng8jp/97Gf45Cc/iccee+yGX29ubsZzzz2HF154AY8//riACqkQ6XQaL774IlwuF9atWye6HCIqMYYBWpYXXngBkUgEDz300IK/vnPnzmsNiBgGzGv//v3o7e0FAIyPj+NHP/oRzp8/jy9/+csIBoOCqyOiUmMYoGXZt28fPB7PolvQVFXFgw8+iH379mFychI1NTVlrpCW4mtf+9q1//Z4PLjtttvwzDPP4M/+7M8EVkVE5cLdBERERDbH3QREREQ2xzBARERkcwwDRERENscwQEREZHMMA0RERDbHMEBERGRzDANEREQ2xzBARERkcwwDRERENscwQEREZHMMA0RERDbHMEBERGRz/x+6Q/25CeEq4AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COllab has this but in local notebook you may want to install it\n",
    "#!pip install matplotlib-venn #install if running locally\n",
    "import matplotlib_venn as venn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Omega = {1,2,3,4,5,6}\n",
    "\n",
    "A = {1, 2, 3, 4, 5}\n",
    "\n",
    "B = {4, 5, 6}\n",
    "\n",
    "venn.venn2([A, B], set_labels=('A','B'))\n",
    "\n",
    "print(len(A)/len(Omega))\n",
    "print(len(B)/len(Omega))\n",
    "print(len(A & B)/len(Omega))\n",
    "print(len(A | B)/len(Omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Probability Axioms\n",
    "\n",
    ":::{admonition} Positivity and Normalization\n",
    ":class: important \n",
    " \n",
    "$$P(A) \\ge 0$$\n",
    " \n",
    "$$P(\\Omega)=1$$\n",
    "\n",
    "> Probability of rolling each number is 1/6 and rolling any number is 1.\n",
    ":::\n",
    "\n",
    ":::{admonition} Addition rule\n",
    ":class: important \n",
    " \n",
    "For any sequence of **mutually exclusive** events, $A_i \\cap A_j = \\emptyset $, the probability of their union is the sum of their probabilities,\n",
    " \n",
    "$$P\\left( A_1 \\cup A_2 \\cup \\ldots \\right) = P\\left(A_1\\right) + P\\left(A_2\\right) + \\ldots$$\n",
    "\n",
    "> Probability of die rolling even number is: $1/6+1/6+1/6$\n",
    "::: \n",
    "\n",
    "\n",
    ":::{admonition} Product rule\n",
    ":class: important  \n",
    " \n",
    "When independent events  $A_i \\cap A_j = \\emptyset$, the probability of their intersection is a product of their probabilities\n",
    " \n",
    "$$P\\left( A_1 \\cap A_2 \\cap \\ldots \\right) = P\\left(A_1\\right) \\cdot P\\left(A_2\\right) \\cdot \\ldots$$\n",
    "\n",
    "> Probability of rolling twice getting 3 and 5 is: $\\frac{1}{6}\\cdot \\frac{1}{6}$\n",
    "\n",
    "::: \n",
    "\n",
    ":::{admonition} Complement\n",
    ":class: important \n",
    " \n",
    "Given that $A \\cap \\bar A=\\emptyset$ and $A \\cup \\bar A=\\Omega$.\n",
    " \n",
    " $$P(\\bar A)=1-P(A)$$\n",
    "\n",
    "> the probability of not rolling a number: $1-\\frac{1}{6}$\n",
    ":::\n",
    " \n",
    ":::{admonition} Conditional probability and Bayes Theorem\n",
    ":class: important \n",
    " \n",
    "Knowledge of past events may change the probability of future events\n",
    " \n",
    "$$P(A,B)=P(A|B)P(B)=P(B|A)P(A)$$\n",
    "\n",
    "> the probability of getting 4 given that we have rolled an even number: $p(4|even) = \\frac{p(even|4)p(4)}{p(even)}=\\frac{1\\cdot 1/6}{1/2}=1/3$\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"./figs/bayes.png\" alt=\"compton\" class=\"bg-primary mb-1\" width=\"500px\">\n",
    "\n",
    "**Joint Probability $P(A,B)$:** Quantifies the probability of two or more events happening simultaneously. **Marginal Probability $P(A)$:** Quantifies the probability of an event irrespective of the outcomes of other random variables. Is obtained by marginalization, summing over all possibilities of B. **Conditional Probability $P(A|B)$:** Quantifies probability of event A given the information that event B happened.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{admonition} **Example of Using Bayes Formula to Test Hypothesis**\n",
    "\n",
    "- A test for cancer is known to be 90% accurate either in detecting cancer if present or in giving an all-clear if cancer is absent.\n",
    "- The prevalence of cancer in the population is 1%. How worried should you be if you test positive? Try answering this question using Bayes’ theorem.\n",
    "\n",
    ":::{dropdown} **Solution**\n",
    "\n",
    "- Accuracy of a test (how often positives show up when cancer is certain)\n",
    "\n",
    "$$P(+|X) = 0.9 \\\\ P(-|X^c)=0.9$$\n",
    "\n",
    "- Only 1% of the population has cancer; hence, we get the probability of an individual having (not having) cancer as:\n",
    "\n",
    "$$P(X)=0.01\\,\\,\\,\\,hence\\,\\,\\,\\,\\,\\,\\, P(X^c)=1-P(X)=0.99$$\n",
    "\n",
    "- Now we have all terms to compute $p(X|+)$ probability of disease given the positive test. \n",
    "\n",
    "$$P(X|+) = \\frac{P(+|X)p(X)}{p(+)} = \\frac{P(+|X)p(X)}{p(+|X)p(X)+p(+| X^c)p(X^c)} =  \\frac{0.9\\cdot 0.01}{0.9\\cdot 0.01+0.1\\cdot 0.99} = 0.083$$\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "@deathbeds/jupyterlab-fonts": {
     "styles": {
      "": {
       "body[data-jp-deck-mode='presenting'] &": {
        "zoom": "145%"
       }
      }
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    ":::{admonition} **Prior, Posterior, and Likelihood**  \n",
    ":class: tip, dropdown\n",
    "\n",
    "Bayes' theorem provides a powerful framework for testing hypotheses or learning model parameters from data. While the mathematical formulation remains the same, the terminology used in Bayesian inference differs from the standard probability notation:\n",
    "\n",
    "$$\n",
    "P(\\theta | D) = \\frac{P(D | \\theta) \\cdot P(\\theta)}{P(D)}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "\n",
    "- **Prior**: $P(\\theta)$ represents our initial belief about the hypothesis or parameter before observing the data. For example, if we are tossing a coin, a reasonable prior might be a gaussian centered at $1/2$ or take unifrom distribution in absence of information.\n",
    "\n",
    "- **Evidence**: $P(D)$ is the probability of the observed data, also known as the **marginal likelihood**. It accounts for all possible parameter values and normalizes the posterior. For example, it is the probability of obtaining a specific sequence, such as $HTHH$, given all possible biases of the coin.  \n",
    "\n",
    "- **Likelihood**: $P(D | \\theta)$ describes how probable the observed data is for a given parameter $\\theta$. E.g for sequence of $HTHH$ it will be $L(\\theta)=\\theta^3(1-\\theta)$ giving probability of landing three H and 1 T. \n",
    "\n",
    "- **Posterior**: $P(\\theta | D)$ is the updated probability of the hypothesis after incorporating the observed data. This is the key quantity in Bayesian inference, as it represents our revised belief about $\\theta$ given the data.  We can take value of $\\theta$ corresponding to maximum of posterior to be most likely value of our parameter. For our case of uniform prior and likelhood the maxima will be $\\theta = 3/4$ as we may expect. \n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Computing number of microstates via combinatorics\n",
    "\n",
    "**Binomial Distribution (Two-State Systems)**  \n",
    "\n",
    "When molecules can be in **two states** (e.g., adsorbed vs. free, spin-up vs. spin-down), the number of ways to arrange $N$ molecules into **state A** $k$ and **state B** $N-k$ follows:  \n",
    "\n",
    "$$\n",
    "W(N, n) = \\binom{N}{n} = \\frac{N!}{N!(N-n)!}\n",
    "$$  \n",
    "\n",
    "- For example, if $N$ gas molecules distribute between two parts of the box or spins occupying two energy levels, this formula gives the number of microstates for a given occupation $k$.  \n",
    "\n",
    "**Multinomial Distribution (Multiple States)**  \n",
    "\n",
    "For systems with **more than two states**, such as molecules distributed among **$m$ energy levels**, the number of ways to assign $N$ molecules into states $n_1, n_2, ..., n_m$ with  $\\sum n_i = N$ is:  \n",
    "\n",
    "$$\n",
    "W(n_1, n_2, ..., n_m) = \\frac{N!}{n_1! n_2! \\cdots n_m!}.\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "::::{admonition} **Example: partitioning gas particles**\n",
    "\n",
    "Consider a container filled with 1000 atoms of Ar.\n",
    "\n",
    ":::{dropdown} What is the probability that the left half has 400 atoms?\n",
    "\n",
    "$$n(A) = \\frac{1000!}{400! \\cdot 600 !}$$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} What is the probability that the left half has 500 atoms?\n",
    "\n",
    "$$n(B) = \\frac{1000!}{500! \\cdot 500 !}$$\n",
    ":\n",
    "::\n",
    "\n",
    ":::{dropdown} What is a probability that 1/3 has 100 next 1/3 has 200 and next 1/3 has 700?\n",
    "\n",
    " $$n(C) = \\frac{1000!}{100!\\cdot 200! \\cdot 700!}$$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} What is the total number of all possible partitionings or states of gas atoms in a container?\n",
    "\n",
    "$$n(S) = \\sum^{n=N}_{n=0}\\frac{N!}{n!\\cdot (N-n)!} = 2^N$$ \n",
    "\n",
    "Each N lattice site in the container can be vacant or filled with $2^N$ states. \n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "@deathbeds/jupyterlab-fonts": {
     "styles": {
      "": {
       "body[data-jp-deck-mode='presenting'] &": {
        "opacity": "100%",
        "z-index": "2",
        "zoom": "222%"
       }
      }
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "::::{admonition} **Example: spins**\n",
    "\n",
    "Solid metal has 100 atoms. Magnetic measurements show that there are 10 atoms with spin down. If ten atoms are chosen at random, what is the probability that they all have spin up? \n",
    "\n",
    ":::{dropdown} **Solution**\n",
    "\n",
    "- The total number of ways to choose any 10 atoms out of 100, regardless of spin is:\n",
    "\n",
    "$$n(S) = \\frac{100!}{10!(90)!}$$\n",
    "\n",
    "- The number of ways to choose 10 atoms out of 90 with spin up is:\n",
    "$$n(up) = \\frac{90!}{10!(80)!}$$\n",
    "\n",
    "- Probability of picking 10 up spins is:\n",
    "\n",
    "$$p(up) =\\frac{n(up)}{n(S)}$$\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0089134454556417e+29\n",
      "1.977470353093058e+33\n"
     ]
    }
   ],
   "source": [
    "def gas_partition(k1=30, k2=30, k3=30):\n",
    "    '''partitioning N gas molecules into regions k1, k2 and k3'''\n",
    "\n",
    "    from scipy.special import factorial\n",
    "\n",
    "    N = k1+k2+k3 \n",
    "\n",
    "    return factorial(N) / (factorial(k1) * factorial(k2)* factorial(k3))\n",
    "\n",
    "print( gas_partition(k1=50, k2=50, k3=0) )\n",
    "\n",
    "print( gas_partition(k1=50, k2=49, k3=1) )\n",
    "\n",
    "print( gas_partition(k1=50, k2=25, k3=25) )\n",
    "\n",
    "print( gas_partition(k1=34, k2=33, k3=33) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strinling approximation of factorial and binomials\n",
    "\n",
    ":::{admonition} **Stitrling approximation of N!**\n",
    ":class: important\n",
    "\n",
    "- This is the crude version of Stirling approximation that works out for $N\\gg 1$\n",
    "\n",
    "$$logN! \\approx \\sum log N_i = \\int log N dN \\approx NlogN-N $$\n",
    "\n",
    "$${N! \\approx N^N e^{-N}}$$\n",
    "\n",
    "- A more accurate version is:\n",
    "\n",
    "$${N! \\approx N^N e^{-N} \\sqrt{2\\pi N}}$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as sp\n",
    "\n",
    "# Define range for N\n",
    "N_values = np.arange(1, 100, 1)\n",
    "\n",
    "# Exact factorial using log(N!)\n",
    "log_fact_exact = np.log(sp.factorial(N_values))\n",
    "\n",
    "# Crude Stirling approximation\n",
    "log_fact_crude = N_values * np.log(N_values) - N_values\n",
    "\n",
    "# More accurate Stirling approximation\n",
    "log_fact_accurate = N_values * np.log(N_values) - N_values + 0.5 * np.log(2 * np.pi * N_values)\n",
    "\n",
    "# Plot comparisons\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(N_values, log_fact_exact, label=\"Exact $\\log N!$\", color=\"black\", linewidth=2)\n",
    "plt.plot(N_values, log_fact_crude, label=\"Crude Stirling Approximation\", linestyle=\"--\", color=\"red\", linewidth=2)\n",
    "plt.plot(N_values, log_fact_accurate, label=\"Accurate Stirling Approximation\", linestyle=\":\", color=\"blue\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"$\\log N!$\")\n",
    "plt.title(\"Comparison of Stirling Approximations for $\\log N!$\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk\n",
    "\n",
    "- Consider a problem with a **binary outcome,** with fixed probabilities $p_{+} + p_{-} = 1$.  \n",
    "- A clssic example is **Random walk** of N steps where molecules jumps right ($+1$) or left ($-1$) with fixed probabilities. \n",
    "    - Other examples are tossing $N$ coins or counting $N$ non-interacting molecules in the left vs right hand side of a container.  \n",
    "\n",
    "- Each experiment generates a sequence—e.g., $+1, -1, -1, -1, +1$ for a random walk or $HTHTTT$ for coin flips. \n",
    "- Such a sequence represents a **single microstate** in the sample space of all possible sequences which is $\\Omega=2^N$. \n",
    "    - For **unbiased random walk** $p_{+} = p_{-} = 1/2$, **all microstates are equally probable** and equal to $\\frac{1}{2^N}$. \n",
    "    - For **biased random walk** $p_{+} \\neq p_{-}$ the probability of microstates (sequence) is determined by the **product of step probabilities** (becasue steps are independent)\n",
    "\n",
    ":::{admonition} **Probability of a sequence (microstate)**\n",
    ":class: important\n",
    "\n",
    "$$p(sequence) = p_{+}^{N_{+}} \\cdot p_{-}^{N_{-}}$$\n",
    ":::\n",
    "\n",
    "- A more interesting question is the probability of taking $N_{+}$ steps to the right, or having a net displacement of $\\Delta N $, regardless of the sequence of events.\n",
    "\n",
    "$$\n",
    "N_{+} + N_{-} = N\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{+} - N_{-} = \\Delta N\n",
    "$$\n",
    "\n",
    "\n",
    ":::{admonition} **Probability of net number of steps or displacements (macrostate)**\n",
    ":class: important\n",
    "\n",
    "- **Probability of $N_{+}$ Steps to the Right** \n",
    "\n",
    "$$\n",
    "P(N_{+} | N, p_{+}) = \\frac{N!}{N_{+}! N_{-}!} \\cdot p_{+}^{N_{+}} \\cdot p_{-}^{N_{-}}\n",
    "$$\n",
    "\n",
    "- **Probability of $\\Delta N$ net displacement from the origin** \n",
    "\n",
    "$$\n",
    "P(\\Delta N | N, p_{+}) = \\frac{ N!}{ \\Big(\\frac{N+\\Delta N}{2}\\Big)! \\Big(\\frac{N-\\Delta N}{2}\\Big)!} \\cdot p_{+}^{(N+\\Delta N)/2} \\cdot p_{-}^{(N-\\Delta N)/2}\n",
    "$$\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import comb\n",
    "\n",
    "def P_x(N, x, p_plus):\n",
    "    \"\"\"Computes the binomial probability P(x|N, p_+)\"\"\"\n",
    "    if (N + x) % 2 != 0:  # Ensure x is valid (must be even relative to N)\n",
    "        return 0\n",
    "    k = (N + x) // 2\n",
    "    return comb(N, k) * (p_plus ** k) * ((1 - p_plus) ** (N - k))\n",
    "\n",
    "# Define parameters\n",
    "p_plus_unbiased = 0.5  # Unbiased case\n",
    "p_plus_biased = 0.7  # Biased case\n",
    "N_values = [10, 50, 100, 200]  # Different N values\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "# Colors for different N values\n",
    "colors = [\"blue\", \"green\", \"red\", \"purple\"]\n",
    "\n",
    "# Loop through different N values\n",
    "for i, N in enumerate(N_values):\n",
    "    x_vals = np.arange(-N, N + 1, 2)  # x values must be -N to N, in steps of 2\n",
    "    x_norm = x_vals / N  # Normalize x by N\n",
    "\n",
    "    # Compute probabilities using vectorized function\n",
    "    P_unbiased = np.array([P_x(N, x, p_plus_unbiased) for x in x_vals])\n",
    "    P_biased = np.array([P_x(N, x, p_plus_biased) for x in x_vals])\n",
    "\n",
    "    # Plot lines\n",
    "    axes[0].plot(x_norm, P_unbiased, marker=\"o\", linestyle=\"-\", label=f\"N={N}\", color=colors[i])\n",
    "    axes[1].plot(x_norm, P_biased, marker=\"o\", linestyle=\"-\", label=f\"N={N}\", color=colors[i])\n",
    "\n",
    "# Titles and labels\n",
    "axes[0].set_title(f\"Unbiased Coin ($p_+$ = {p_plus_unbiased})\")\n",
    "axes[0].set_xlabel(\"$x/N$\")\n",
    "axes[0].set_ylabel(\"$P(x)$\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title(f\"Biased Coin ($p_+$ = {p_plus_biased})\")\n",
    "axes[1].set_xlabel(\"$x/N$\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log of probability leads us to Entropy and Energy \n",
    "\n",
    "$$log\\, p(N_{+}|N, p_{+}) =  {\\bigg[ log  \\frac{N!}{N_{+}! N_{-}!}\\bigg ]} + {log \\bigg[p_{+}^{N_{+}} \\cdot p_{-}^{N_{-}}\\bigg]} = {entropy} + {energy}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy as log of number of microstates in a macrostate\n",
    "\n",
    "$$\n",
    "\\log\\frac{N!}{N_{+}! \\cdot N_{-}!} \\approx N \\log N -N_{+} \\log N_{+} - N_{-} \\log N_{-}  = N \\Big[ -\\frac{N_{+}}{N} \\log\\frac {N_{+}}{N} - \\frac{N_{-}}{N} \\log \\frac{N_{-}}{N} \\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= N\\Big[-f_{+} \\log f_{+} - f_{-} \\log f_{-}\\Big] = N\\Big[-flog f + (1-f)log(1-f)\\Big] = N s(f)\n",
    "$$\n",
    "\n",
    "\n",
    "- Where we introduced fractions of steps going to the right/left, $f_{\\pm} = \\frac{N_{\\pm}}{N}$. Since these fractions are dependent we only have one variable to keep track of $f_{+}=f$ and $f_{-} = 1-f$\n",
    "\n",
    "- Note that **$p_{+}$ is fixed** while **$f_{+}$ is variable** becasue as we observe more steps fractions will fluctuate. For large  $N$, however, we exepct to get $f_{\\pm} \\rightarrow p_{\\pm}$. \n",
    "- A general expression will later be identified with **entropy $s =  -\\sum_i p_i \\log p_i$**where $p_i$ would be identified with microstate probabilities. \n",
    "\n",
    "#### Energy as the degree of bias or tilt\n",
    "\n",
    "- Taking the logarithm of the probability factor we get a term resembling an **energy-like function** $\\epsilon$ that tilts distribution to left or right depending on fraction of steps $f_{\\pm} = \\frac{N_{\\pm}}{N}$\n",
    "\n",
    "$$\n",
    "\\log \\Big[ p_{+}^{N_{+}} \\cdot p_{-}^{N_{-}} \\Big] = - N [ f_{+}log p_{+}+f_{-}log p_{-}] = -N \\epsilon(f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Entropy expressed via **fractional displacement $x=\\frac{\\Delta N}{N}$**\n",
    ":class: tip, dropdown\n",
    "\n",
    "$$log \\frac{ N!}{ \\Big(\\frac{N+\\Delta N}{2}\\Big)! \\Big(\\frac{N-\\Delta N}{2}\\Big)!} = N \\Bigg [\\frac{1+x}{2} \\log \\frac{1+x}{2} - \\frac{1-x}{2} \\log \\frac{1-x}{2}\\Bigg ] = N s(x)$$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{admonition} Energy term expressed via **fractional displacement $x=\\frac{\\Delta N}{N}$**\n",
    ":class: tip, dropdown\n",
    "\n",
    "$$\n",
    "\\log \\Big[ p_{+}^{(N+\\Delta N)/2} \\cdot p_{-}^{(N-\\Delta N)/2} \\Big] = - N\\Big [ \\frac{1}{2}\\log ( p_{+} p_{-}) + \\frac{x}{2}  \\log \\frac{p_{+}}{p_{-}} \\Big] = -N \\epsilon(x)\n",
    "$$\n",
    "\n",
    "- The term $\\frac{1}{2} (\\log p_{+} + \\log p_{-})$ is a **baseline** energy-like term.\n",
    "- The term $\\frac{x}{2} (\\log p_{+} - \\log p_{-})$ represents the **bias** introduced by the displacement $l$, tilting the distribution towards $p_+$ or $p_-$.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Deviation Theory  \n",
    "\n",
    "- Putting everything together we find that probability of observing fraction of steps $f_{\\pm} = \\frac{N_{\\pm}}{N}$ is governed by a rate function $I(f)$, with $N$ appearing as a linear factor:  \n",
    "\n",
    "  $$\n",
    "  \\log P_{N}(f) \\approx -N \\big(\\epsilon(f) - s(f) \\big) = -N I(f).\n",
    "  $$\n",
    "\n",
    "- When there are $N$ steps, molecules, or components, the probability distribution over the fraction of steps $f$ tends to concentrate near the minima of $I(f)$. This follows from a general mathematical property of probability distributions known as the **large deviation theorem**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Large Deviation Theorem (LDT)**\n",
    ":class: important\n",
    "\n",
    "- **Large deviation function $I(f)$:** dictates both the shape and decay of probability distributions in the large-$N$ limit.  \n",
    "\n",
    "$$\n",
    "P_N (f) \\sim e^{-N I(f)}.\n",
    "$$\n",
    "\n",
    "- **Example of $I(f)$ for Random Walk:** Determines how deviations of $f_+$ from $ p_+$ are exponentially suppressed as $N$ increases.\n",
    "\n",
    "$$\n",
    "I(f) = f_+ \\log \\frac{f_+}{p_+} + f_- \\log \\frac{f_-}{p_-}\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Re-import required libraries since execution state was reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "p_plus = 0.7  # Biased probability\n",
    "p_minus = 1 - p_plus  # Complementary probability\n",
    "\n",
    "# Define range for f_+\n",
    "f_plus_values = np.linspace(0.01, 0.99, 200)  # Avoid log(0) issues\n",
    "f_minus_values = 1 - f_plus_values  # f_- = 1 - f_+\n",
    "\n",
    "# Compute entropy component s(f_+)\n",
    "s_values = -(f_plus_values * np.log(f_plus_values) + f_minus_values * np.log(f_minus_values))\n",
    "\n",
    "# Compute energy component ε(f_+)\n",
    "epsilon_values = - (f_plus_values * np.log(p_plus) + f_minus_values * np.log(p_minus))\n",
    "\n",
    "# Compute large deviation rate function I(f_+)\n",
    "I_values = f_plus_values * np.log(f_plus_values / p_plus) + f_minus_values * np.log(f_minus_values / p_minus)\n",
    "\n",
    "# Compute probability P_N(f_+) using large deviation approximation\n",
    "N = 50  # Arbitrary large N\n",
    "P_x_values = np.exp(-N * I_values)  # Exponential suppression\n",
    "P_x_values /= np.trapz(P_x_values, f_plus_values)  # Normalize for probability density\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# First subplot: Entropy and Energy Components\n",
    "axes[0].plot(f_plus_values, s_values, label=r\"$s(f)$ (Entropy)\", color=\"blue\")\n",
    "axes[0].plot(f_plus_values, epsilon_values, label=r\"$\\epsilon(f_+)$ (Energy)\", color=\"green\")\n",
    "axes[0].plot(f_plus_values, I_values, label=r\"$I(f)$ (Rate Function)\", color=\"red\")\n",
    "axes[0].axvline(p_plus, linestyle=\"--\", color=\"black\", label=r\"$f_+ = p_+$\")\n",
    "axes[0].set_xlabel(r\"$f_+$\")\n",
    "axes[0].set_ylabel(\"Value\")\n",
    "axes[0].set_title(\"Entropy, Energy, and Rate Function\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "# Second subplot: Probability Distribution P_N(f_+)\n",
    "axes[1].plot(f_plus_values, P_x_values, label=r\"$P_N(f_+)$\", color=\"purple\")\n",
    "axes[1].axvline(p_plus, linestyle=\"--\", color=\"black\", label=r\"$f_+ = p_+$\")\n",
    "axes[1].set_xlabel(r\"$f_+$\")\n",
    "axes[1].set_ylabel(r\"$P_N(f_+)$\")\n",
    "axes[1].set_title(\"Probability Distribution\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import comb\n",
    "\n",
    "def plot_large_deviation(N, theta, color):\n",
    "    \"\"\"Plots the large deviation approximation for given N and bias theta.\"\"\"\n",
    "    f = np.linspace(0.01, 0.99, 200)  # Avoid log(0) issues\n",
    "\n",
    "    # Compute the rate function I(f)\n",
    "    I = f * np.log(f / theta) + (1 - f) * np.log((1 - f) / (1 - theta))\n",
    "\n",
    "    # Compute normalized probability P_LDT(f) ∼ exp(-N I(f))\n",
    "    p_ldt = np.exp(-N * I)\n",
    "    p_ldt /= np.trapz(p_ldt, f)  # Normalize using trapezoidal rule for integration\n",
    "\n",
    "    plt.plot(f, p_ldt, color=color, linestyle=\"-\", linewidth=2, label=f\"LDT Approx. (N={N})\")\n",
    "\n",
    "def plot_binomial(N, theta, color):\n",
    "    \"\"\"Plots the exact binomial distribution for given N and bias theta.\"\"\"\n",
    "    n = np.arange(N + 1)\n",
    "    f = n / N  # Convert discrete counts to fractions\n",
    "\n",
    "    # Compute binomial probability mass function\n",
    "    prob = comb(N, n) * theta**n * (1 - theta)**(N - n)\n",
    "\n",
    "    # Normalize probability for direct comparison with LDT curve\n",
    "    prob /= np.trapz(prob, f)\n",
    "\n",
    "    plt.plot(f, prob, 'o', color=color, markersize=5, label=f\"Binomial (N={N})\")\n",
    "\n",
    "# Parameters\n",
    "theta = 0.5  # Fair coin\n",
    "Ns = [5, 10, 20, 50, 100]  # Different values of N\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(Ns)))  # Use colormap for better distinction\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, N in enumerate(Ns):\n",
    "    plot_large_deviation(N, theta, colors[i])\n",
    "    plot_binomial(N, theta, colors[i])\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel(r\"$f_+$\", fontsize=14)\n",
    "plt.ylabel(r\"Probability Density\", fontsize=14)\n",
    "plt.title(\"Comparison of Binomial Distribution (Points) and Large Deviation Approximation (Lines)\", fontsize=12)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian nature of fluctuations\n",
    "\n",
    "- The Taylor expansion of the large deviation function around its minimum leads to a Gaussian distribution in the limit of small fluctuations. This is a key result in large deviation theory, often used to justify the Gaussian nature of fluctuations in equilibrium statistical mechanics. \n",
    "\n",
    "- Since $I(f)$ has a minimum at $f^*$, we expand it in a Taylor series around $f^*$:\n",
    "\n",
    "$$\n",
    "I(f) = I(f^*) + \\frac{1}{2} I''(f^*) (f - f^*)^2 + \\mathcal{O}((f - f^*)^3).\n",
    "$$\n",
    "\n",
    "- Since $I(f^*) = 0$ (by definition, the probability is maximal at $X^*$, we obtain:\n",
    "\n",
    "$$\n",
    "I(f) \\approx \\frac{1}{2} I''(f^*) (f - f^*)^2.\n",
    "$$\n",
    "\n",
    "- Using this expansion in the large deviation form,\n",
    "\n",
    "$$\n",
    "P(f) \\approx e^{-N I(f)} \\approx e^{-N \\frac{1}{2} I''(f^*) (f - f^*)^2} = e^{-\\frac{(f - f^*)^2}{2 \\sigma^2}}\n",
    "$$\n",
    "\n",
    "- Where we define the variance as $\\sigma^2 = \\frac{1}{N I''(f^*)}$. This is precisely a **Gaussian distribution** centered at $f^*$, with variance $\\sigma^2$. Note that f refers to fraction or normalized particle quantity $f = n/N$. You may also encounter distribution as a function of absolte particle numbers $P(n)$ \n",
    "\n",
    "- Thus, the Taylor expansion of the large deviation function around its minimum shows that, in the limit of small fluctuations large $N$, the probability distribution approximates a Gaussian. This result underpins why **equilibrium fluctuations in statistical physics and thermodynamics are often Gaussian.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Explicit derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Appendix A. **Gaussian or large $N$ limit of Binomial Distribution**\n",
    ":class: tip, dropdown  \n",
    "\n",
    "The binomial distribution for large values of $N$ has a sharply peaked distribution around its maximum (most likely) value $\\tilde{n}$. This motivates us to seek a continuous approximation by Taylor expanding the probability distribution around its maximum value $\\Delta n = n - \\tilde{n}$ and keeping terms up to quadratic order.\n",
    "\n",
    "$$\n",
    "P_N(n) = \\frac{N!}{n! (N - n)!} p^{n} (1-p)^{N-n}\n",
    "$$\n",
    "\n",
    "Thus, from the onset, we aim for a Gaussian distribution. The task is to find the coefficients and justify that the third term in the Taylor expansion is negligible compared to the second.\n",
    "\n",
    "$$\n",
    "\\log P(n) = \\log P(\\tilde{n}) + \\frac{1}{2} B_2 \\Delta n^2 + O(\\Delta n^3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log P(n) = \\log N! - \\log n! - \\log( N - n)! + n \\log(p) + N_{-} \\log(1-p)\n",
    "$$\n",
    "\n",
    "We evaluate the derivative of $\\log n!$ in the limit of $n \\gg 1$ as:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dn} \\log n! = \\frac{\\log(n+1)! - \\log n!}{n+1 - n} \\approx \\log(n+1) \\approx \\log(n)\n",
    "$$\n",
    "\n",
    "- We could also arrive at the same result by using Stirling's approximation $\\log N! \\approx N \\log N - N$.\n",
    "- Taking the first derivative of the Taylor expansion for the binomial distribution, we find the peak of the distribution around which we expand:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dn} \\log P(n) \\Big|_{n = \\tilde{n}} = - \\log n + \\log(N - n) + \\log(p) - \\log(1-p) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{N - n}{n} \\cdot \\frac{p}{1 - p} \\right) = 0 \\quad \\Rightarrow \\quad \\tilde{n} = N p\n",
    "$$\n",
    "\n",
    "- We recall that $\\tilde{n} = N p$ is also the mean of the binomial distribution!\n",
    "- Having found the peak of the distribution and knowing the first derivative, we now proceed to compute the second derivative:\n",
    "\n",
    "$$\n",
    "B_2 = \\frac{d^2}{d n^2} \\log P(n) \\Big|_{n = \\tilde{n}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{d}{dn} \\log \\left( \\frac{N - n}{n} \\cdot \\frac{p}{1 - p} \\right) = \\left( -\\frac{1}{N - n} - \\frac{1}{n} \\right) \\Bigg|_{n = \\tilde{n}} = -\\frac{1}{N p(1 - p)}\n",
    "$$\n",
    "\n",
    "- While the first derivative gave us the mean of the binomial distribution, we notice that the second derivative produces the variance $\\sigma^2 = N p (1 - p)$.\n",
    "- Now, all that remains is to plug the coefficients into our approximated probability distribution and normalize it. Why normalize? The binomial was already properly normalized, but since we made an approximation by neglecting higher-order terms, we must re-normalize.\n",
    "\n",
    "$$\n",
    "P(n) \\approx P(\\tilde{n}) e^{-(n - \\tilde{n})^2 / 2N p (1 - p)}\n",
    "$$\n",
    "\n",
    "- Normalizing the Gaussian distribution is done via the following integral:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} e^{-a x^2} dx = \\left(\\frac{\\pi}{a}\\right)^{1/2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int P(\\tilde{n}) e^{-(n - \\tilde{n})^2 / 2N p (1 - p)} dn = P(\\tilde{n}) (2\\pi N p (1 - p))^{1/2} = 1\n",
    "$$\n",
    "\n",
    "- Finally, we obtain the normalized Gaussian approximation to the binomial distribution:\n",
    "\n",
    "$$\n",
    "P(n) \\approx \\frac{1}{(2\\pi N p (1 - p))^{1/2}} e^{-(n - \\tilde{n})^2 / 2N p (1 - p)} = \\frac{1}{(2\\pi \\sigma^2)^{1/2}} e^{-(n - \\mu)^2 / 2\\sigma^2}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Appendix B. **Poisson limit or the limit of large $N$ and small $p$ such that $Np=const$**\n",
    ":class: tip, dropdown \n",
    "\n",
    "- This is a situation of rare events like rains in forest or radioactive decay of uranium where each individual event has small chance of happening $p \\rightarrow 0$  yet there are large number of samples $N\\rightarrow \\infty$ such that one has a constant average rate of events $\\lambda = pN = const$\n",
    "- In this limit distirbution is no longer well described by the gaussian as the shape of distribution is heavily skewed due to tiny values of p.\n",
    "\n",
    "$$P_N(n) = \\frac{N!}{n! (N-n)!} p^n (1-p)^{(N-n)}$$\n",
    "\n",
    "- Writing factorial $N!/(N-n)!$ explicitely we realize that it is dominated $N^n$ and also $N-n \\approx N$\n",
    "\n",
    "$$P_N(n) = \\frac{N(N-1)...(N-1+1))}{n!} p^n (1-p)^{(N-n)} \\approx \\frac{N^n}{n!} p^n (1-p)^{N}$$\n",
    "\n",
    "- Next let us plug in $\\lambda = pN = const$ and recall the definition of exponential $lim_{x\\rightarrow \\infty }(1-1/x)^x = e^{-x}$\n",
    "\n",
    "$$P(n) = \\frac{N^n}{n!} \\Big( \\frac{\\lambda}{N} \\Big)^n \\Big( 1-\\frac{\\lambda}{N} \\Big)^{N} = \\frac{\\lambda^n}{n!} \\Big( 1-\\frac{\\lambda}{N} \\Big)^{N} \\approx \\frac{\\lambda^n}{n!} e^{-\\lambda}$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Example of Gaussian limit of Large Deviation function for random walk**\n",
    ":class: note, dropdown\n",
    "\n",
    "\n",
    "The large deviation rate function for a simple random walk is given by:\n",
    "\n",
    "$$\n",
    "I(f) = f_+ \\log \\frac{f_+}{p_+} + f_- \\log \\frac{f_-}{p_-}\n",
    "$$\n",
    "\n",
    "where $f_+$ and $f_-$ are empirical step probabilities, and $p_+$, $p_-$ are their expected values with $p_+ + p_- = 1$.\n",
    "\n",
    "**Expansion Around the Minimum**\n",
    "\n",
    "The function $I(f)$ is minimized at $f_+ = p_+$, $f_- = p_-$. Introducing small deviations $\\delta f$:\n",
    "\n",
    "$$\n",
    "f_+ = p_+ + \\delta f, \\quad f_- = p_- - \\delta f.\n",
    "$$\n",
    "\n",
    "Expanding the logarithms:\n",
    "\n",
    "$$\n",
    "\\log \\frac{p_+ + \\delta f}{p_+} \\approx \\frac{\\delta f}{p_+} - \\frac{(\\delta f)^2}{2 p_+^2}, \\quad\n",
    "\\log \\frac{p_- - \\delta f}{p_-} \\approx -\\frac{\\delta f}{p_-} - \\frac{(\\delta f)^2}{2 p_-^2}.\n",
    "$$\n",
    "\n",
    "Substituting into $I(f)$, the linear terms cancel, and we obtain:\n",
    "\n",
    "$$\n",
    "I(f) \\approx \\frac{(\\delta f)^2}{2} \\left( \\frac{1}{p_+} + \\frac{1}{p_-} \\right).\n",
    "$$\n",
    "\n",
    "**Gaussian Limit**\n",
    "\n",
    "By the large deviation principle:\n",
    "\n",
    "$$\n",
    "P(f) \\approx e^{-N I(f)} = e^{-N \\frac{(\\delta f)^2}{2} \\left( \\frac{1}{p_+} + \\frac{1}{p_-} \\right)} = e^{- \\frac{(\\delta f)^2}{2 p_{+}p_{-}/N} }\n",
    "$$\n",
    "\n",
    "This is a Gaussian with variance:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{p_+ p_-}{N}.\n",
    "$$\n",
    "\n",
    "Thus, the empirical frequency $f_+$ follows a Gaussian distribution in the large $N$ limit. \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1: Counting Dies and coins\n",
    " \n",
    " You flip a coin 10 times and record the data in the form\n",
    "of head/tails or 1s and 0s\n",
    "-  What would be the probability of ladning 4 H's?\n",
    "-  What would be the probability of landing HHHTTTHHHT sequence?\n",
    "-  In how many ways can we have 2 head and 8 tails in this experiments?\n",
    "-  Okay, now you got tired of flipping coins and decide to play some dice. You throw die\n",
    "10 times what is the probability of never landing number 6?\n",
    "- You throw a die 3 times what is the probability of obtaining a combined sum of 7?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Problem 2: Counting gas molecules\n",
    "\n",
    "$$C^{N}_{k}$$\n",
    "\n",
    "A container of volume $V$ contains $N$ molecules of a gas. We assume that the gas is dilute so that the position of any one molecule is independent of all other molecules. Although the density will be uniform on the average, there are fluctuations in the density. Divide the volume $V$ into two parts $V_1$ and $V_2$, where $V = V_1 + V_2$. \n",
    "- What is the probability p that a particular molecule is in each part? \n",
    "- What is the probability that $N_1$ molecules are in $V_1$ and $N_2$ molecules are in $V_2$? \n",
    "- What is the average number of molecules in each part? \n",
    "- What are the relative fluctuations of the number of particles in each part?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Project [Porosity of materials](https://en.wikipedia.org/wiki/Porosity)\n",
    "\n",
    "A simple model of a porous rock can be imagined by placing a series of overlap- ping spheres at random into a container of fixed volume $V$ . The spheres represent the rock and the space between the spheres represents the pores. If we write the volume of the sphere as v, it can be shown the fraction of the space between the spheres or the porosity $\\phi$ is $\\phi =e^{-Nv/V}$, where $N$ is the number of spheres. \n",
    "\n",
    "For simplicity, consider a 2D system, (e.g $v=\\frac{1}{4}\\pi d^2$, see [wiki](https://en.wikipedia.org/wiki/Area_of_a_circle) if you forgot the formula). \n",
    "Write a python function which place disks of $d=1$ into a square box. The disks can overlap. Divide the box into square cells each of which has an edge length equal to the diameter of the disks. Find the probability of having 0, 1, 2, or 3 disks in a cell for $\\phi$ = 0.03, 0.1, and 0.5.\n",
    "\n",
    "> You will need [np.random.uniform()](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.uniform.html) to randomly place N disks of volume v into volume V. \n",
    "> Check out this cool python lib for porosity evaluation of materials [R Shkarin, et al Plos Comp Bio 2019](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0215137&type=printable)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "livereveal": {
   "theme": "sky"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
