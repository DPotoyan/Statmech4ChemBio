

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Entropy and Information &#8212; Statistical Mechanics for Chemistry and Biology</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1_stats/Entropy';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Thermodynamics" href="../2_thermo/intro.html" />
    <link rel="prev" title="Diffusion" href="Diffusion.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Statistical Mechanics for Chemistry and Biology - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Statistical Mechanics for Chemistry and Biology - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    About
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../schedule.html">Schedule (Spring 2025)</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Stats</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Probabilities_Counting.html">Probability theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="Random_Variables.html">Random variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="Diffusion.html">Diffusion</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Entropy and Information</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../2_thermo/intro.html">Thermodynamics</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../2_thermo/01_Thermo.html">Review of thermodynamics principles</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ensembles/intro.html">Ensembles</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ensembles/phase_space.html">Phase Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ensembles/nve.html">NVE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ensembles/nvt.html">NVT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ensembles/npt.html">muPT</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../4_ideal_systems/intro.html">Gases</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../4_ideal_systems/quantum_systems.html">Quantum non-interacting systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_ideal_systems/photons_phonons.html">Photons and Phonons</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5_ising/intro.html">Phase transitions</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../5_ising/00_MC.html">Monte Carlo and “the power of randomness”</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_ising/01_MCMC.html">Ising models and Metropolis algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_ising/02_phase_tranistions.html">Phase transitions through the lense of Ising models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_ising/03_MeanField.html">Mean Field Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_ising/03_vdw.html">Mean Field theory of interacting fluids</a></li>





<li class="toctree-l2"><a class="reference internal" href="../5_ising/04_analytic.html">Analytic solutions to 1D Ising model</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../6_fluids/intro.html">Fluids</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../6_fluids/Intro2Fluids.html">Statistical mechanics of fluids</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6_fluids/mc_lj.html">MC simulations of fluids</a></li>

<li class="toctree-l2"><a class="reference internal" href="../6_fluids/Intro2MD.html">Molecular Dynamics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../6_fluids/md_lj.html">MD simulations of fluids</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../7_kinetics/intro.html">Kinetics</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../7_kinetics/langevin.html">Langevin equation and Brownian motion</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../labs/py-lab/intro.html">Python</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../labs/py-lab/intro2py.html">Python3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/py-lab/intro2numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/py-lab/intro2viz.html">Plotting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/py-lab/python_OOP_challenge.html">Tutorial: Building Python Classes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../labs/np-lab/intro.html">Numpy lab</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../labs/np-lab/np_prob.html">Numpy and Probabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/np-lab/sinc_function.html">Numpy Lab: Sinc Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/np-lab/linear_functions.html">Numpy Lab: Linear Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/np-lab/1d_gas_sim.html">Tutorial: Numpy simultions of 1D gas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/np-lab/2d_gas_sim.html">Tutorial: Numpy simultions of 2D gas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../labs/rw-lab/intro.html">Random Walks</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../labs/rw-lab/rw_lab.html">Random walk simulations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/rw-lab/InverseTransform.html">The Inverse Transform of RVs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/rw-lab/change_rv_jax.html">Change of variables with automatic differentiation (autodiff)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/rw-lab/polymer-conf.html">Conformations of Random Polymer Chains</a></li>

<li class="toctree-l2"><a class="reference internal" href="../labs/rw-lab/langevin.html">Langevin Equation for Brownian Motion and Mean Square Displacement (MSD)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../labs/lab4/intro.html">Application of ensembles</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab4/nvt_application.html">Free energy and protein folding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab4/TwoState.html">Two-state system</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab4/mass_action.html">Mass action law</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../labs/lab5/intro.html">Simulating Ising models</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab5/torch_ising.html">Ising-2D simulations using ML libraries</a></li>



<li class="toctree-l2"><a class="reference internal" href="../labs/lab5/Wolf.html">Other Methods for sampling Ising models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab5/enhance-sampling.html">Non-boltzman (enhanced) sampling ideas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../labs/lab6/intro.html">Simulating Fluids</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab6/openmm_double_well.html">Double Well potential using OpenMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab6/openmm_lj_chain.html">Simulating toy polymers using openMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab6/openmm_ethane.html">Simulating Ethane</a></li>
<li class="toctree-l2"><a class="reference internal" href="../labs/lab6/openmm_villin.html">Simulating solvated protein</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/DPotoyan/Statmech4ChemBio/master?urlpath=lab/tree/1_stats/Entropy.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/DPotoyan/Statmech4ChemBio/blob/master/1_stats/Entropy.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/1_stats/Entropy.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Entropy and Information</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surprise">Surprise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#addititivity-of-information">Addititivity of Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-bit-base-two">Why bit (base two)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shannon-entropy-and-information">Shannon Entropy and Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-micro-and-macro-states">Entropy, micro and macro states</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretations-of-entropy">Interpretations of Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-as-a-measure-of-information">1. Entropy as a Measure of Information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-as-a-measure-of-diversity-and-uncertainty">2. Entropy as a Measure of Diversity and Uncertainty</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#physical-and-thermodynamic-implications">3. Physical and Thermodynamic Implications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-information-physical">Is Information Physical?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-entropy-maxent-principle">Maximum Entropy (MaxEnt) Principle</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-maxent">Applications of  MaxEnt</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fair-die-example">1. Fair Die Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#biased-die-example">2. Biased Die Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-entropy">Relative Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assymetry-of-kl-and-irreversibility">Assymetry of KL and irreversibility</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-entropy-in-machine-learning">Relative Entropy in Machine learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problems">Problems</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="entropy-and-information">
<h1>Entropy and Information<a class="headerlink" href="#entropy-and-information" title="Permalink to this heading">#</a></h1>
<div class="admonition-what-you-will-learn admonition">
<p class="admonition-title"><strong>What you will learn</strong></p>
<ul class="simple">
<li><p>Entropy is a measure of information or uncertainty, quantified using probability distributions (discrete or continuous).</p></li>
<li><p>When expressed in <span class="math notranslate nohighlight">\(\log_2\)</span>, entropy represents the number of binary (yes/no) questions needed to identify a microstate within a macrostate.</p></li>
<li><p>Entropy quantifies the diversity of microstates, reaching its maximum for a uniform (flat) distribution.</p></li>
<li><p>The Maximum Entropy (MaxEnt) principle provides the most unbiased way to infer probability distributions given empirical constraints.</p></li>
<li><p>Relative entropy reveals that entropy in MaxEnt is defined relative to a reference system where all microstates are equally probable.</p></li>
<li><p>Relative entropy quantifies the information lost when approximating one probability distribution with another.</p></li>
<li><p>Relative entropy also shows that information (or uncertainty) tends to increase irreversibly in the absence of external interventions.</p></li>
</ul>
</div>
<section id="surprise">
<h2>Surprise<a class="headerlink" href="#surprise" title="Permalink to this heading">#</a></h2>
<p><strong>Which of these two statements conveys the most information?</strong></p>
<ul class="simple">
<li><p>I will eat some food tomorrow.</p></li>
<li><p>I will see a giraffe walking by my apartment.</p></li>
</ul>
<p><strong>A measure of information (whatever it may be) is closely related to the element of… surprise!</strong></p>
<ul class="simple">
<li><p>has very high probability and so conveys little information,</p></li>
<li><p>has very low probability and so conveys much information.</p></li>
</ul>
<blockquote>
<div><p>If we quanitfy suprise we will quantify information</p>
</div></blockquote>
</section>
<section id="addititivity-of-information">
<h2>Addititivity of Information<a class="headerlink" href="#addititivity-of-information" title="Permalink to this heading">#</a></h2>
<p><strong>Knowledge leads to gaining information</strong></p>
<p>Which is more surprising (contains more information)?</p>
<ul class="simple">
<li><p>E1: The card is heart? <span class="math notranslate nohighlight">\(P(E_1) = \frac{1}{4}\)</span></p></li>
<li><p>E2:The card is Queen? <span class="math notranslate nohighlight">\(P(E_2)  =  \frac{4}{52} = \frac{1}{13}\)</span></p></li>
<li><p>E3: The card is Queen of hearts? <span class="math notranslate nohighlight">\(P(E_1 \, and\,  E_2) = \frac{1}{52}\)</span></p></li>
</ul>
<p><strong>Knowledge of event should add up our information: <span class="math notranslate nohighlight">\(I(E) \geq 0\)</span></strong></p>
<ol class="arabic simple">
<li><p>We learn the card is heart <span class="math notranslate nohighlight">\(I(E_1)\)</span></p></li>
<li><p>We learn the card is Queen <span class="math notranslate nohighlight">\(I(E_2)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(I(E_1 and E_2) = I(E_1) + I(E_2)\)</span></p></li>
</ol>
<p><strong>A logarithm of probability is a good candidate function for information!</strong></p>
<div class="math notranslate nohighlight">
\[log_2 P(E_1) P(E_2) = log_2 P(E_1) + log_2(E_2)\]</div>
<ul class="simple">
<li><p>What about the sign?</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I_i = -log_2 p_i\]</div>
</section>
<section id="why-bit-base-two">
<h2>Why bit (base two)<a class="headerlink" href="#why-bit-base-two" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Consider symmetric a 1D random walk with equal jump probabilities. We can view <strong>Random walk = string of Yes/No questions</strong>.</p></li>
<li><p>Imagine driving to a location how many left/right turn informations you need to reach destination?</p></li>
<li><p>You gain one bit of information when you are told Yes/No answer</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(X=0) = I(X=1) = -log_2 \frac{1}{2} = 1\]</div>
<ul class="simple">
<li><p>To decode N step random walk trajectory we need N bits.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[(x_0,x_1,...x_N) = 10111101001010100100\]</div>
</section>
<section id="shannon-entropy-and-information">
<h2>Shannon Entropy and Information<a class="headerlink" href="#shannon-entropy-and-information" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>If we want to understand the overall uncertainty in the system, we need to consider all possible outcomes weighted by their probability of occurrence.</p></li>
<li><p>This means that rather than looking at the surprise of a single event, <strong>we consider the average surprise one would experience over many trials drawn from <span class="math notranslate nohighlight">\(p\)</span>.</strong></p></li>
<li><p>Thus, we take the expectation of surprise over the entire distribution <span class="math notranslate nohighlight">\(\langle -log p \rangle\)</span> arriving at a formula known as Shaonon’s expression of Entropy.</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title"><strong>Shanon Entropy</strong></p>
<div class="math notranslate nohighlight">
\[S = -\sum_i p_i log_2 p_i\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span> Entropy(Information) measured in bits</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i\)</span> probability of microstate <span class="math notranslate nohighlight">\(i\)</span>, e.g coin flip or die roll outcomes</p></li>
</ul>
</div>
<ul class="simple">
<li><p>John von Neumann advice to [To Calude Shanon], “You should call it Entropy, for two reasons. In the first place you uncertainty function has been used in statistical mechanics under that name. In the second place, and more importantly, no one knows what entropy really is, so in a debate you will always have the advantage.”</p></li>
<li><p>One often uses <span class="math notranslate nohighlight">\(H\)</span> to denote Shanon Entropy (<span class="math notranslate nohighlight">\(log_2\)</span>) and letter <span class="math notranslate nohighlight">\(S\)</span> with <span class="math notranslate nohighlight">\(log_e\)</span> for entropy in units of Boltzman constant <span class="math notranslate nohighlight">\(k_B\)</span></p></li>
<li><p>For now lets just roll with <span class="math notranslate nohighlight">\(k_B=1\)</span> we wont be doing any thermodynamics in here.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">binary_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the binary Shannon entropy for a given probability p.</span>
<span class="sd">    Avoid issues with log(0) by ensuring p is never 0 or 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># Generate probability values, avoiding the endpoints to prevent log(0)</span>
<span class="n">p_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">H_vals</span> <span class="o">=</span> <span class="n">binary_entropy</span><span class="p">(</span><span class="n">p_vals</span><span class="p">)</span>

<span class="c1"># Create a figure with two subplots side-by-side</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plot 1: Binary Shannon Entropy Function</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_vals</span><span class="p">,</span> <span class="n">H_vals</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;midnightblue&#39;</span><span class="p">,</span>
           <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$H(p)=-p\log_2(p)-(1-p)\log_2(1-p)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Probability $p$)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Entropy (bits)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Binary Shannon Entropy&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot 2: Example Distributions and Their Entropy</span>
<span class="c1"># Define a few example two-outcome distributions:</span>
<span class="n">distributions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Uniform (0.5, 0.5)&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="s2">&quot;Skewed (0.8, 0.2)&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="s2">&quot;Extreme (0.99, 0.01)&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Colors for each distribution</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;skyblue&quot;</span><span class="p">,</span> <span class="s2">&quot;salmon&quot;</span><span class="p">,</span> <span class="s2">&quot;lightgreen&quot;</span><span class="p">]</span>

<span class="c1"># For visual separation, use offsets for the bars</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span>
<span class="n">x_ticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># positions for the two outcomes</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">distributions</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="c1"># Compute the Shannon entropy for the distribution</span>
    <span class="n">entropy_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">probs</span><span class="p">))</span>
    <span class="c1"># Offset x positions for clarity</span>
    <span class="n">x_positions</span> <span class="o">=</span> <span class="n">x_ticks</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">width</span> <span class="o">-</span> <span class="n">width</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_positions</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
              <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="se">\n</span><span class="s2">Entropy = </span><span class="si">{</span><span class="n">entropy_val</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
    
<span class="c1"># Set labels and title for the bar plot</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x_ticks</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;Outcome 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Outcome 2&quot;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Probability&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Example Distributions&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/9f9d87d5134598a6ad3ed7510bf99227a95c46c4c13511b3b3a4fe6d8361104d.png" src="../_images/9f9d87d5134598a6ad3ed7510bf99227a95c46c4c13511b3b3a4fe6d8361104d.png" />
</div>
</div>
<div class="note dropdown admonition">
<p class="admonition-title"><strong>Exercise: Information per letter <span class="math notranslate nohighlight">\(I(m)\)</span> to decode the message</strong> </p>
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(m\)</span>  represent the letters in an alphabet. For example:</p>
<ul class="simple">
<li><p><strong>Korean:</strong> 24 letters</p></li>
<li><p><strong>English:</strong> 26 letters</p></li>
<li><p><strong>Russian:</strong> 33 letters</p></li>
</ul>
</li>
<li><p>The information content associated with these alphabets satisfies:</p>
<div class="math notranslate nohighlight">
\[
  I(\text{Russian}) &gt; I(\text{English}) &gt; I(\text{Korean})
  \]</div>
</li>
<li><p>The information of a sequence of letters is additive, regardless of the order in which they are transmitted:</p>
<div class="math notranslate nohighlight">
\[
  I(m_1, m_2) = I(m_1) + I(m_2)
  \]</div>
</li>
<li><p><strong>Question</strong> If the symbols of English alphabet (+ blank) appear equally probably, what is the information carried by a single symbol? This must be <span class="math notranslate nohighlight">\(log_2(26 + 1) = 4.755\)</span> bits, but for actual English sentences, it is known to be about <strong><span class="math notranslate nohighlight">\(1.3\)</span> bits. Why?</strong></p></li>
</ul>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<strong>Solution</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<ul class="simple">
<li><p class="sd-card-text">Not every letter has equal probability or frequency of appearing in a sentence!</p></li>
</ul>
</div>
</details></div>
<div class="note dropdown admonition">
<p class="admonition-title"><strong>Exercise: entropy of die rolls</strong> </p>
<ul class="simple">
<li><p>How much knowledge we need to find out outcome of fair dice?</p></li>
<li><p>We are told die shows a digit higher than 2 (3, 4, 5 or 6). How much knowledge does this information carry?</p></li>
</ul>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<strong>Solution</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(H(E_1) = log_2 6\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(H(E_1) - H(E_2) = log_2 6 - log_2 4\)</span></p></li>
</ul>
</div>
</details></div>
<div class="note dropdown admonition">
<p class="admonition-title"><strong>Exercise: Two cats</strong> </p>
<p>There are two kittens. We are told that at least one of them is a male. What is the information we get from this message?</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<strong>Solution</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<div class="math notranslate nohighlight">
\[E_1 = \{mm,mf,fm, ff \} \]</div>
<div class="math notranslate nohighlight">
\[E_2 = \{mm,mf,fm\}\]</div>
<div class="math notranslate nohighlight">
\[H(E_1) -H(E_2) = log_2 4 -log_2 3 = 0.41\]</div>
</div>
</details></div>
<div class="note dropdown admonition">
<p class="admonition-title"><strong>Exercise: Monty Hall problem</strong> </p>
<p>There are five boxes, of which one contains a prize. A game participant is asked to choose one box. After they choose one of the five boxes, the “coordinator” of the game identifies as empty three of the four unchosen boxes. What is the information of this message?</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<strong>Solution</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(H(E_1) = log_2 5 = 2.322\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(H(E_2) = -\frac{1}{5} log_2 5 - \frac{4}{5} log_2 \frac{4}{5} = 0.722\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(H(E_1)-H(E_2) = 1.6\)</span></p></li>
</ul>
</div>
</details></div>
<div class="note dropdown admonition">
<p class="admonition-title"><strong>Exercise: Why are there non-integer number of YES/NO questions??</strong> </p>
<p>Explain the origin of the non-integer information. Why it takes less than one-bit to encode information?</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<strong>Solution</strong><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<ul class="simple">
<li><p class="sd-card-text">We have encountered a fraction of bit of information several times now. What does it imply in terms of number of YES/NO questions. That is becasue in some cases single YES/NO question can rule out more than one elementary event.</p></li>
<li><p class="sd-card-text">In other words we can ask clever questions that can get us to answer faster than doing YES/No on every single possibility</p></li>
<li><p class="sd-card-text">999 blue balls and 1 red ball. how many questions we need to ask to determin the colors of all balls? <span class="math notranslate nohighlight">\(S = 9.97\)</span> bit or 0.01 bit per ball. Divide the container by 500 and 500 and ask where the red ball is? 1 questions rules out 500 balls at once.</p></li>
</ul>
</div>
</details></div>
</section>
<section id="entropy-micro-and-macro-states">
<h2>Entropy, micro and macro states<a class="headerlink" href="#entropy-micro-and-macro-states" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>When all <span class="math notranslate nohighlight">\(\Omega\)</span> number of microstates of the system have equal probability <span class="math notranslate nohighlight">\(p_i=\frac{1}{\Omega}\)</span> the entropy of the system is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[S= -\sum \frac{1}{\Omega} log \frac{1}{\Omega} = -\Omega \cdot  \frac{1}{\Omega} log \frac{1}{\Omega} = log\Omega\]</div>
<ul class="simple">
<li><p>We arrive at an expression of Entropy first obtained by Boltzmann where thermal energy units (Boltzman’s) constant were used.</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title"><strong>Entropy expression for equally probable microstates</strong></p>
<div class="math notranslate nohighlight">
\[S(\Omega) = k_B log \Omega\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega\)</span> <strong>Number of Microsates availible to the system</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(k_B\)</span> Boltzmann’s constant.</p></li>
</ul>
</div>
<ul class="simple">
<li><p><strong>Entropy for a macrostate A</strong> which has <span class="math notranslate nohighlight">\(\Omega(A)\)</span> number of microstates can be written in terms of macrostate probability</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(A) = \frac{\Omega(A)}{\Omega}\]</div>
<div class="math notranslate nohighlight">
\[S(A) = log \Omega(A) = log P(A) + const\]</div>
<ul class="simple">
<li><p>Entropy of a macrostate <strong>quantifies how probable that macrostate is!</strong>. This is yet another manifestation of Large Deviation Theorem we encountered before</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(A) \sim e^{S(A)}\]</div>
<div class="tip dropdown admonition">
<p class="admonition-title"><strong>Flashback to Random Walk, Binomial, and Large Deviation Theorem</strong>  </p>
<ul class="simple">
<li><p><a class="reference external" href="https://dpotoyan.github.io/Statmech4ChemBio/1_stats/Probabilities_Counting.html#large-deviation-theory">Where have we seen the entropy expression before?</a></p></li>
<li><p>When we took the <strong>log of the binomial distribution</strong>! But why did we call it entropy?</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
S(n) = \log \frac{N!}{n! (N-n)!} = N \left[ - f \log f - (1-f) \log (1-f) \right] = N s(f)
\]</div>
<ul>
<li><p>Here, <span class="math notranslate nohighlight">\( f = n/N \)</span> represents the <strong>fraction (empirical probability) of steps to the right</strong> in a random walk.</p></li>
<li><p><span class="math notranslate nohighlight">\( s(f) = S/N \)</span> is the <strong>entropy per particle (or per step)</strong>, while <span class="math notranslate nohighlight">\( S \)</span> is the <strong>total entropy</strong>.</p></li>
<li><p><strong>Different macrostates have different entropy, depending on the number of microstates they contain!</strong></p>
<div class="math notranslate nohighlight">
\[
  P(n) = \frac{\Omega(n)}{\Omega_{\text{total}}} =  \frac{N!}{n! (N-n)!} \cdot \frac{1}{2^N}
  \]</div>
</li>
<li><p>where <span class="math notranslate nohighlight">\( \Omega_{\text{total}} = 2^N \)</span> is the <strong>total number of microstates</strong>.</p></li>
<li><p>Once again, we can think of <strong>the entropy of a macrostate as being related to its probability:</strong></p>
<div class="math notranslate nohighlight">
\[
  S(f) \sim \log P(f)
  \]</div>
</li>
</ul>
<p><strong>Connection to the Large Deviation Theorem</strong><br />
When we express probability in terms of entropy, we recover the <strong>Large Deviation Theorem</strong>, which states that <strong>fluctuations from the most likely macrostates are exponentially suppressed</strong>:</p>
<div class="math notranslate nohighlight">
\[
P(f) \sim e^{N s(f)}
\]</div>
<p>This result highlights how entropy naturally governs the likelihood of macrostates in statistical mechanics.</p>
</div>
</section>
<section id="interpretations-of-entropy">
<h2>Interpretations of Entropy<a class="headerlink" href="#interpretations-of-entropy" title="Permalink to this heading">#</a></h2>
<section id="entropy-as-a-measure-of-information">
<h3>1. Entropy as a Measure of Information<a class="headerlink" href="#entropy-as-a-measure-of-information" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Entropy quantifies the amount of information needed to specify the exact microstate of a system.</p></li>
<li><p>This can be understood as the number of yes/no (binary) questions required to identify a specific microstate.</p></li>
<li><p>Examples:</p>
<ul>
<li><p>Determining the exact trajectory of an <span class="math notranslate nohighlight">\(N\)</span>-step random walk.</p></li>
<li><p>Identifying the detailed molecular distribution of gas particles in a container.</p></li>
</ul>
</li>
</ul>
</section>
<section id="entropy-as-a-measure-of-diversity-and-uncertainty">
<h3>2. Entropy as a Measure of Diversity and Uncertainty<a class="headerlink" href="#entropy-as-a-measure-of-diversity-and-uncertainty" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Entropy reflects the number of accessible microstates in a given macrostate.</p></li>
<li><p>A more <strong>uniform (flat) probability distribution</strong> of microstates corresponds to <strong>higher entropy</strong>, whereas a <strong>more concentrated (narrow) distribution</strong> leads to <strong>lower entropy</strong>.</p></li>
<li><p>Higher entropy implies greater uncertainty:</p>
<ul>
<li><p>In a high-entropy system, identifying the exact microstate is much harder.</p></li>
<li><p>In contrast, a low-entropy system is more predictable, as fewer microstates are accessible.</p></li>
</ul>
</li>
<li><p>When all microstates are equally probable, entropy simplifies to the logarithm of the number of microstates, <span class="math notranslate nohighlight">\(S = k_B \log \Omega\)</span>.</p></li>
<li><p>A system will naturally tend to evolve toward macrostates with higher entropy because they correspond to a larger number of available microstates, making them statistically more likely.</p></li>
</ul>
</section>
<section id="physical-and-thermodynamic-implications">
<h3>3. Physical and Thermodynamic Implications<a class="headerlink" href="#physical-and-thermodynamic-implications" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Systems with high entropy have a vast number of possible microstates, meaning more “work” is needed—in an informational sense—to pinpoint a specific one.</p></li>
<li><p><strong>Reducing entropy requires physical work:</strong></p>
<ul>
<li><p>Example: To reduce the number of yes/no questions needed to specify the position of gas molecules, one must compress the gas, which requires energy.</p></li>
</ul>
</li>
<li><p><strong>Spontaneous and irreversible processes tend to increase entropy:</strong></p>
<ul>
<li><p>Entropy naturally increases in isolated systems because evolution toward more probable (higher entropy) macrostates is statistically favored.</p></li>
<li><p>This principle underlies the <strong>Second Law of Thermodynamics</strong>, which states that entropy never decreases in a closed system.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="is-information-physical">
<h2>Is Information Physical?<a class="headerlink" href="#is-information-physical" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="markdown-fig">
<img alt="diffflux" src="../_images/max-dem.png" />
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">Maxwell’s demon controlling the door that allows the passage of single molecules from one side to the other. The initial hot gas gets hotter at the end of the process while the cold gas gets colder.</span><a class="headerlink" href="#markdown-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>Wheeler’s “It from Bit”:</strong></p>
<ul>
<li><p>Every “it” — every particle, every field, every force, and even the fabric of space-time — derives its function, meaning, and very existence from binary answers to yes-or-no questions. In essence, Wheeler’s idea of “It from Bit” posits that at the most fundamental level, the physical universe is rooted in information. This perspective implies that all aspects of reality are ultimately information-theoretic in origin.</p></li>
</ul>
</li>
<li><p><strong>Maxwell’s Demon:</strong></p>
<ul>
<li><p>Maxwell’s Demon is a thought experiment that challenges the second law of thermodynamics by envisioning a tiny being capable of sorting molecules based on their speeds. By selectively allowing faster or slower molecules to pass through a gate, the demon appears to reduce entropy without expending energy. However, the act of gathering and processing information incurs a thermodynamic cost, ensuring that the overall entropy balance is maintained. This paradox underscores that information is a physical quantity with measurable effects on energy and entropy.</p></li>
</ul>
</li>
</ul>
</section>
<section id="maximum-entropy-maxent-principle">
<h2>Maximum Entropy (MaxEnt) Principle<a class="headerlink" href="#maximum-entropy-maxent-principle" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Probability represents incomplete information. Given partial knowledge, how should we construct a probability distribution that is unbiased beyond what we know?</p></li>
<li><p>The <strong>Maximum Entropy (MaxEnt) Principle</strong> provides the best approach: we choose probabilities <span class="math notranslate nohighlight">\(p_k\)</span> to <strong>maximize Shannon entropy</strong> while satisfying given constraints.</p></li>
<li><p>This ensures the least biased distribution possible, consistent with the available information.</p></li>
<li><p>In MaxEnt, entropy is maximized <strong>relative to an underlying assumption of equal-probability microstates</strong> (i.e., maximum ignorance in the absence of constraints).</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title"><strong>MaxEnt Principle</strong></p>
<ul class="simple">
<li><p><strong>MaxEnt: Maximize Entropy subject to constraints on given observables: <span class="math notranslate nohighlight">\(x^a\)</span>, <span class="math notranslate nohighlight">\(x^b\)</span>, etc</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
S = - \sum_k p_k \log p_k,
\]</div>
<div class="math notranslate nohighlight">
\[
\sum_k p_k \, x_k^a = \langle x^a \rangle, \quad \sum_k p_k \, x_k^b = \langle x^b \rangle, \quad \text{etc.}
\]</div>
<div class="math notranslate nohighlight">
\[
J[p] = S(p) - \lambda_0 \left( \sum_k p_k - 1 \right) - \lambda_1 \left( \sum_k p_k \, x_k^a - \langle x^a \rangle \right) - \cdots,
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J[p]\)</span> is the objective function we are maximizing with respect to probabilities</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda_i\)</span> are <strong>Lagrange multipliers</strong> enforcing the respective constraints. <span class="math notranslate nohighlight">\(\lambda_0\)</span> is special it enforces normalization constant and must always be there.</p></li>
</ul>
</div>
</section>
<section id="applications-of-maxent">
<h2>Applications of  MaxEnt<a class="headerlink" href="#applications-of-maxent" title="Permalink to this heading">#</a></h2>
<section id="fair-die-example">
<h3>1. Fair Die Example<a class="headerlink" href="#fair-die-example" title="Permalink to this heading">#</a></h3>
<p>For a fair <span class="math notranslate nohighlight">\( N \)</span>-sided die, we have no prior information favoring one outcome over another. The only constraint is the normalization condition:</p>
<div class="math notranslate nohighlight">
\[ \sum_{i=1}^{N} p_i = 1. \]</div>
<p>We maximize:</p>
<div class="math notranslate nohighlight">
\[ J[p_1, p_2, ..] = - \sum p_i \log p_i - \lambda \left( \sum_i p_i - 1 \right). \]</div>
<p>Taking the derivative and solving for <span class="math notranslate nohighlight">\( p_i \)</span>, we obtain:</p>
<div class="math notranslate nohighlight">
\[ p_1 = p_2 = ... = p_N = \frac{1}{N}. \]</div>
<p>This confirms our intuition that all outcomes are equally probable.</p>
</section>
<section id="biased-die-example">
<h3>2. Biased Die Example<a class="headerlink" href="#biased-die-example" title="Permalink to this heading">#</a></h3>
<p>Suppose we have additional information: the average outcome of rolling a die is <span class="math notranslate nohighlight">\( \langle x \rangle = 5.5 \)</span>. The entropy function to maximize becomes:</p>
<div class="math notranslate nohighlight">
\[ J[p_1, p_2, ..] = - \sum p_i \log p_i - \lambda \left( \sum_i p_i - 1 \right) - B \left( \sum_i p_i x_i - 5.5 \right). \]</div>
<p>Solving the variational equation, we find that the optimal probability distribution follows an exponential form:</p>
<div class="math notranslate nohighlight">
\[ p_i = \frac{e^{- B x_i}}{Z}, \]</div>
<p>where <span class="math notranslate nohighlight">\( Z \)</span> is the partition function ensuring normalization:</p>
<div class="math notranslate nohighlight">
\[ Z = \sum_{i=1}^{6} e^{- B x_i}. \]</div>
<p>To determine <span class="math notranslate nohighlight">\( B \)</span>, we use the constraint <span class="math notranslate nohighlight">\( \langle x \rangle = 5.5 \)</span>:</p>
<div class="math notranslate nohighlight">
\[ \sum_{i=1}^{6} x_i \frac{e^{- B x_i}}{Z} = 5.5. \]</div>
<p>This equation can be solved numerically for <span class="math notranslate nohighlight">\( B \)</span>. In many cases, Newton’s method or other root-finding techniques can be employed to find the exact value of <span class="math notranslate nohighlight">\( B \)</span>. This distribution resembles the Boltzmann factor in statistical mechanics, where higher outcomes are exponentially less probable.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>

<span class="c1"># Define die outcomes</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>  <span class="c1"># Possible outcomes {1,2,3,4,5,6}</span>

<span class="c1"># Define the target mean constraint</span>
<span class="n">target_mean</span> <span class="o">=</span> <span class="mf">5.5</span>  <span class="c1"># Expected value constraint</span>

<span class="c1"># Define the entropy function to maximize (negative because we minimize in optimization)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">entropy</span><span class="p">(</span><span class="n">logp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the negative entropy (since we minimize in optimization).</span>
<span class="sd">    logp contains log-probabilities to ensure numerical stability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logp</span><span class="p">)</span>  <span class="c1"># Convert log-probabilities back to probabilities</span>
    <span class="n">p</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># Ensure proper normalization</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>  <span class="c1"># Negative entropy for maximization</span>

<span class="c1"># Define the constraint function to enforce the expected value condition</span>
<span class="k">def</span><span class="w"> </span><span class="nf">constraint</span><span class="p">(</span><span class="n">logp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constraint function to enforce the mean constraint ⟨x⟩ = 5.5.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logp</span><span class="p">)</span>  <span class="c1"># Convert log-probabilities back to probabilities</span>
    <span class="n">p</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># Ensure proper normalization</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="n">target_mean</span>  <span class="c1"># Difference from the desired mean</span>

<span class="c1"># Initial guess: uniform log-probabilities (log of 1/6 for each face)</span>
<span class="n">logp0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span>

<span class="c1"># Define optimization constraints</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;eq&quot;</span><span class="p">,</span> <span class="s2">&quot;fun&quot;</span><span class="p">:</span> <span class="n">constraint</span><span class="p">}</span>

<span class="c1"># Perform numerical optimization using Sequential Least Squares Programming (SLSQP)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">entropy</span><span class="p">,</span> <span class="n">logp0</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;SLSQP&quot;</span><span class="p">)</span>

<span class="c1"># Extract the optimized probability distribution</span>
<span class="n">optimal_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
<span class="n">optimal_p</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">optimal_p</span><span class="p">)</span>  <span class="c1"># Ensure normalization</span>

<span class="c1"># Plot the resulting probability distribution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">optimal_p</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;royalblue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Die Outcome&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Probability&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Optimized MaxEnt Probability Distribution for a Biased Die&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="c1"># Display the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Print the optimized probabilities</span>
<span class="n">optimal_p</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/a9c3e669853cb0e9aa1944c6775e77efb2d39b63549ea6934a4e6180933a6700.png" src="../_images/a9c3e669853cb0e9aa1944c6775e77efb2d39b63549ea6934a4e6180933a6700.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([2.28705973e-16, 2.52533784e-11, 1.66666638e-01, 7.65915583e-08,
       1.64515691e-29, 8.33333285e-01])
</pre></div>
</div>
</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title"><strong>Physical constriants on energy, particle number, volume</strong></p>
<p><strong>1. Microcanonical Ensemble (Fixed Energy, Volume, and Particle Number)</strong></p>
<ul class="simple">
<li><p>For an isolated gas with a fixed energy <span class="math notranslate nohighlight">\( E \)</span>, volume <span class="math notranslate nohighlight">\( V \)</span>, and particle number <span class="math notranslate nohighlight">\( N \)</span>, we maximize entropy subject to the constraint that only microstates with energy <span class="math notranslate nohighlight">\( E \)</span> are accessible:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ J = -\sum_k p_k \log p_k - \lambda \left( \sum_k p_k - 1 \right). \]</div>
<ul class="simple">
<li><p>Solving for <span class="math notranslate nohighlight">\( p_k \)</span>, we obtain:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ p_k = \frac{1}{\Omega}, \]</div>
<ul class="simple">
<li><p>where <span class="math notranslate nohighlight">\( \Omega \)</span> is the number of microstates. This is the basis of classical thermodynamics, where entropy is defined as <span class="math notranslate nohighlight">\( S = k_B \log \Omega \)</span>.</p></li>
</ul>
<p><strong>2. Canonical Ensemble (Fixed Temperature, Volume, and Particle Number)</strong></p>
<ul class="simple">
<li><p>If the system is in thermal contact with a heat bath at temperature <span class="math notranslate nohighlight">\( T \)</span>, energy is allowed to fluctuate. The constraint now involves the mean energy <span class="math notranslate nohighlight">\( \langle E \rangle = U \)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ J = -\sum_k p_k \log p_k - \lambda \left( \sum_k p_k - 1 \right) - \beta \left( \sum_k p_k E_k - U \right). \]</div>
<ul class="simple">
<li><p>Solving, we obtain the Boltzmann distribution:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ p_k = \frac{e^{-\beta E_k}}{Z}, \]</div>
<ul class="simple">
<li><p>where <span class="math notranslate nohighlight">\( \beta = 1 / k_B T \)</span> and <span class="math notranslate nohighlight">\( Z = \sum_k e^{-\beta E_k} \)</span> is the partition function. This distribution governs systems in thermal equilibrium.</p></li>
</ul>
</div>
</section>
</section>
<section id="relative-entropy">
<h2>Relative Entropy<a class="headerlink" href="#relative-entropy" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>We mentioned that in MaxEnt derivation we maximized entropy relative to an underlying assumption of equal-probability microstates. We make this idea precise here</p></li>
<li><p>Consider example of 1D brownian particle starting at <span class="math notranslate nohighlight">\(x_0 = 0\)</span> and diffusing freely. Its probability distribution after time <span class="math notranslate nohighlight">\(t\)</span> follows a Gaussian:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(x, t) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{x^2}{4Dt}}
\]</div>
<ul class="simple">
<li><p>The <strong>Shannon entropy</strong> of this distribution is given by:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
S(t) = -\int p(x, t) \log p(x, t) \, dx.
\]</div>
<ul class="simple">
<li><p>Evaluating the integral yields nice compact formulla showing that entropy grows with time as molecule diffuse and sparead all over the container.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
S(t) = \frac{1}{2} \log (4\pi e Dt).
\]</div>
<p><strong>Problem: Grid Dependence of Shannon Entropy</strong></p>
<ul class="simple">
<li><p>All is good but if we tried to evaluate the integral we would run into serious problem!</p></li>
<li><p>Also how to interpret integral expression in terms of binary yes/no quesionts?</p></li>
<li><p>A major issue with Shannon entropy is that it <strong>depends on the choice of units</strong>. If we refine the grid by choosing a smaller <span class="math notranslate nohighlight">\(\Delta x\)</span>, the computed entropy does not converge to a well-defined value—it diverges! This makes it unsuitable for studying entropy change in diffusion.</p></li>
<li><p>To avoid this issue, one can instead use <strong>relative entropy</strong> (Kullback-Leibler divergence), which remains well-defined and independent of discretization.</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title"><strong>Relative Entropy</strong></p>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P || Q) = \sum_x P(x) \ln \frac{P(x)}{Q(x)}
\]</div>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P || Q) = \int P(x) \ln \frac{P(x)}{Q(x)} \, dx.
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q\)</span> <strong>reference probability</strong> distribution</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> <strong>true probability</strong> distribution or the one we are using/observing.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>The Kullback-Leibler (KL) divergence, or relative entropy, measures <strong>how much information is lost when using a reference distribution Q</strong></p></li>
<li><p><strong>KL is non-negative and equals zero if and only if <span class="math notranslate nohighlight">\(P = Q \)</span> everywhere.</strong></p></li>
<li><p>KL divergence is widely used in statistical mechanics, information theory, machine learning and thermodynamics as a measure of information loss when approximating one distribution with another.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title"><strong>Flashback to Random Walk and LDT</strong>  </p>
<ul>
<li><p><a class="reference external" href="https://dpotoyan.github.io/Statmech4ChemBio/1_stats/Probabilities_Counting.html#large-deviation-theory">We have already encountered relative entropy in the random walk problem!</a></p></li>
<li><p>There, we derived what is known as the <strong>Large Deviation Theory (LDT)</strong> expression, which shows that fluctuations are concentrated around the minima of <span class="math notranslate nohighlight">\( I(f) \)</span>—a function that initially seemed mysterious but proved to be fundamental:</p>
<div class="math notranslate nohighlight">
\[
  P_N (f) \sim e^{-N I(f)}.
  \]</div>
</li>
<li><p>Now, we recognize that this function is actually the <strong>relative entropy</strong> between the empirical and true probability distributions as <span class="math notranslate nohighlight">\( N \)</span> increases:</p>
<div class="math notranslate nohighlight">
\[
  I(f) = f_+ \log \frac{f_+}{p_+} + f_- \log \frac{f_-}{p_-} = D(f || p)
  \]</div>
<div class="math notranslate nohighlight">
\[
  P_N (f) \sim e^{-N D(f || p)}
  \]</div>
</li>
<li><p>Thus, we see that <strong>relative entropy quantifies how unlikely it is to observe an empirical fraction <span class="math notranslate nohighlight">\( f \)</span> deviating from the true probability <span class="math notranslate nohighlight">\( p \)</span></strong>. The larger the relative entropy <span class="math notranslate nohighlight">\( D(f || p) \)</span>, the less likely it is to observe such a deviation!</p></li>
</ul>
</div>
</section>
<section id="assymetry-of-kl-and-irreversibility">
<h2>Assymetry of KL and irreversibility<a class="headerlink" href="#assymetry-of-kl-and-irreversibility" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Given two normal distributions:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(x) = \mathcal{N}(\mu_1, \sigma_1^2), \quad Q(x) = \mathcal{N}(\mu_2, \sigma_2^2),
\]</div>
<ul class="simple">
<li><p>We can compute KL divergence analytically showing that the function is assymteric</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P || Q) = \ln \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}.
\]</div>
<div class="tip dropdown admonition">
<p class="admonition-title"><strong>KL Divergence between two Gaussians</strong></p>
<p class="rubric"><strong>Derivation of KL Divergence Between Two Gaussians</strong></p>
<p>Given two normal distributions:</p>
<div class="math notranslate nohighlight">
\[
P(x) = \mathcal{N}(\mu_1, \sigma_1^2), \quad Q(x) = \mathcal{N}(\mu_2, \sigma_2^2),
\]</div>
<p>the <strong>Kullback-Leibler (KL) divergence</strong> is defined as:</p>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P || Q) = \int_{-\infty}^{\infty} p(x) \ln \frac{p(x)}{q(x)} \,dx.
\]</div>
<p><strong>Step 1: Write Out the Gaussian PDFs</strong>
The probability density functions of the two Gaussians are:</p>
<div class="math notranslate nohighlight">
\[
P(x) = \frac{1}{\sqrt{2\pi \sigma_1^2}} e^{-\frac{(x - \mu_1)^2}{2\sigma_1^2}},
\]</div>
<div class="math notranslate nohighlight">
\[
Q(x) = \frac{1}{\sqrt{2\pi \sigma_2^2}} e^{-\frac{(x - \mu_2)^2}{2\sigma_2^2}}.
\]</div>
<p>Taking their ratio:</p>
<div class="math notranslate nohighlight">
\[
\frac{P(x)}{Q(x)} = \frac{\sigma_2}{\sigma_1} \exp \left[ \frac{(x - \mu_2)^2}{2\sigma_2^2} - \frac{(x - \mu_1)^2}{2\sigma_1^2} \right].
\]</div>
<p>Taking the natural logarithm:</p>
<div class="math notranslate nohighlight">
\[
\ln \frac{P(x)}{Q(x)} = \ln \frac{\sigma_2}{\sigma_1} + \frac{(x - \mu_2)^2}{2\sigma_2^2} - \frac{(x - \mu_1)^2}{2\sigma_1^2}.
\]</div>
<p><strong>Step 2: Compute the Expectation <span class="math notranslate nohighlight">\( \mathbb{E}_P[\ln P(x)/Q(x)] \)</span></strong></p>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P || Q) = \mathbb{E}_P \left[ \ln \frac{P(x)}{Q(x)} \right] = \int P(x) \ln \frac{P(x)}{Q(x)} \,dx.
\]</div>
<p>Since expectation under <span class="math notranslate nohighlight">\( P(x) \)</span> means integrating with <span class="math notranslate nohighlight">\( P(x) \)</span>, we evaluate the three terms separately.</p>
<ol class="arabic">
<li><p><strong>First term:</strong><br />
$<span class="math notranslate nohighlight">\(
\mathbb{E}_P \left[ \ln \frac{\sigma_2}{\sigma_1} \right] = \ln \frac{\sigma_2}{\sigma_1}
\)</span>$
since it is a constant.</p></li>
<li><p><strong>Second term:</strong><br />
Using the property of a Gaussian expectation <span class="math notranslate nohighlight">\( \mathbb{E}_P [(x - \mu_1)^2] = \sigma_1^2 \)</span>,</p>
<div class="math notranslate nohighlight">
\[
   \mathbb{E}_P \left[ \frac{(x - \mu_1)^2}{2\sigma_1^2} \right] = \frac{1}{2}.
   \]</div>
</li>
<li><p><strong>Third term:</strong><br />
Expanding <span class="math notranslate nohighlight">\( (x - \mu_2)^2 \)</span>,</p>
<div class="math notranslate nohighlight">
\[
   (x - \mu_2)^2 = (x - \mu_1 + \mu_1 - \mu_2)^2.
   \]</div>
<p>Taking expectation under <span class="math notranslate nohighlight">\( P(x) \)</span>,</p>
<div class="math notranslate nohighlight">
\[
   \mathbb{E}_P \left[ \frac{(x - \mu_2)^2}{2\sigma_2^2} \right] = \frac{1}{2\sigma_2^2} \left( \sigma_1^2 + (\mu_1 - \mu_2)^2 \right).
   \]</div>
</li>
</ol>
<p><strong>Step 3: Final KL Divergence Formula</strong></p>
<p>Combining all terms, we get:</p>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P || Q) = \ln \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}.
\]</div>
</div>
<ul class="simple">
<li><p>Computing <strong>KL divergence</strong> between two gaussians describing diffusion at times <span class="math notranslate nohighlight">\( t_1 \)</span> and <span class="math notranslate nohighlight">\( t_2 \)</span>, where their variances are <span class="math notranslate nohighlight">\( \sigma_1^2 = 2D t_1 \)</span> and <span class="math notranslate nohighlight">\( \sigma_2^2 = 2D t_2 \)</span> results in:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
D(p_1 \| p_2) = \frac{1}{2} \left( \frac{t_1}{t_2} - 1 + \log \frac{t_2}{t_1} \right)
\]</div>
<ul class="simple">
<li><p>For a <strong>diffusion process</strong>, if we compare the <strong>forward evolution</strong>of diffusion where variance is spreading over time <span class="math notranslate nohighlight">\(t_1=t\)</span> with the hypothetical <strong>reversed process</strong> where guassian contraints into a peak over time <span class="math notranslate nohighlight">\(t_2=T-t\)</span> we can see that:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P_{\text{forward}} || P_{\text{backward}}) &gt; 0.
\]</div>
<ul class="simple">
<li><p>This indicates that diffusion is an <strong>irreversible</strong> process in the absence of external driving forces (since it tends to increase entropy). In contrast, a time-reversed diffusion process (all particles contracting back into the initial state) would violate the second law of thermodynamics.</p></li>
<li><p>This statistical distinguishability of time-forward and time-reversed processes is often referred to the <strong>“”thermodynamic arrow of time”</strong> showing that the forward flow of events is distinguishable from its reverse.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">stats</span>

<span class="c1"># Define parameters</span>
<span class="n">D</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Diffusion coefficient</span>
<span class="n">t1</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Time for first Gaussian</span>
<span class="n">t2</span> <span class="o">=</span> <span class="mf">2.0</span>  <span class="c1"># Time for second Gaussian</span>
<span class="n">sigma1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="n">t1</span><span class="p">)</span>  <span class="c1"># Standard deviation at t1</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="n">t2</span><span class="p">)</span>  <span class="c1"># Standard deviation at t2</span>

<span class="c1"># Define grid for discretization</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span>  <span class="c1"># Range of x values</span>
<span class="n">num_bins_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">]</span>  <span class="c1"># Different resolutions</span>

<span class="c1"># Compute Shannon entropy for discretized Gaussian</span>
<span class="n">shannon_entropies</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">num_bins</span> <span class="ow">in</span> <span class="n">num_bins_list</span><span class="p">:</span>
    <span class="n">x_bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">num_bins</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Bin edges</span>
    <span class="n">x_centers</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x_bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># Bin centers</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">x_bins</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_bins</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Bin width</span>
    
    <span class="c1"># Compute probability mass function for discretized Gaussian</span>
    <span class="n">p_i</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_centers</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span>
    <span class="n">p_i</span> <span class="o">=</span> <span class="n">p_i</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p_i</span><span class="p">)</span>  <span class="c1"># Normalize</span>

    <span class="c1"># Compute Shannon entropy</span>
    <span class="n">S_shannon</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p_i</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_i</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">))</span>  <span class="c1"># Avoid log(0)</span>
    <span class="n">shannon_entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">S_shannon</span><span class="p">)</span>

<span class="c1"># Compute differential entropy for continuous Gaussian</span>
<span class="n">S_diff</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span> <span class="o">*</span> <span class="n">D</span> <span class="o">*</span> <span class="n">t1</span><span class="p">)</span>

<span class="c1"># Compute KL divergence between Gaussians at t1 and t2</span>
<span class="n">KL_div</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">t1</span> <span class="o">/</span> <span class="n">t2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">t2</span> <span class="o">/</span> <span class="n">t1</span><span class="p">))</span>

<span class="c1"># Plot Shannon entropy vs. grid resolution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">num_bins_list</span><span class="p">,</span> <span class="n">shannon_entropies</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Shannon Entropy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">S_diff</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Differential Entropy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Bins&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Entropy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Shannon Entropy vs. Grid Resolution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/9257b2f590675541d66528b9ca743b4d32dd155e7b899bd5068399d2f0e26b7e.png" src="../_images/9257b2f590675541d66528b9ca743b4d32dd155e7b899bd5068399d2f0e26b7e.png" />
</div>
</div>
<section id="relative-entropy-in-machine-learning">
<h3>Relative Entropy in Machine learning<a class="headerlink" href="#relative-entropy-in-machine-learning" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\( Q(x) \)</span> <strong>assigns very low probability</strong> to a region where <span class="math notranslate nohighlight">\( P(x) \)</span> is high, the term <span class="math notranslate nohighlight">\( \log \frac{P(x)}{Q(x)} \)</span> becomes large, <strong>strongly penalizing <span class="math notranslate nohighlight">\( Q \)</span> for underestimating <span class="math notranslate nohighlight">\( P \)</span></strong>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\( Q(x) \)</span> <strong>is broader than <span class="math notranslate nohighlight">\( P(x) \)</span>, assigning extra probability mass to unlikely regions</strong>, this does not significantly affect <span class="math notranslate nohighlight">\( D_{\text{KL}}(P || Q) \)</span>, because <span class="math notranslate nohighlight">\( P(x) \)</span> is small in those regions.</p></li>
</ol>
<ul class="simple">
<li><p>This asymmetry explains why <strong>KL divergence is not a true distance metric</strong>. It penalizes <strong>underestimation</strong> of true probability mass much more than <strong>overestimation</strong>, making it particularly useful in <strong>machine learning</strong> where models are trained to avoid assigning near-zero probabilities to observed data.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define two Gaussian distributions with different means and variances</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>  <span class="c1"># Mean and standard deviation for P</span>
<span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>  <span class="c1"># Mean and standard deviation for Q</span>

<span class="n">x_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Define spatial grid</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_values</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma1</span><span class="p">)</span>  <span class="c1"># First Gaussian</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_values</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma2</span><span class="p">)</span>  <span class="c1"># Second Gaussian</span>

<span class="c1"># Compute KL divergences</span>
<span class="n">D_KL_PQ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">P</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">P</span> <span class="o">/</span> <span class="n">Q</span><span class="p">),</span> <span class="n">x_values</span><span class="p">)</span>  <span class="c1"># D_KL(P || Q)</span>
<span class="n">D_KL_QP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">Q</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Q</span> <span class="o">/</span> <span class="n">P</span><span class="p">),</span> <span class="n">x_values</span><span class="p">)</span>  <span class="c1"># D_KL(Q || P)</span>

<span class="c1"># Plot the distributions</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_values</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$P(x) \sim \mathcal</span><span class="si">{N}</span><span class="s1">(0,1)$&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_values</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$Q(x) \sim \mathcal</span><span class="si">{N}</span><span class="s1">(1,2)$&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_values</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Difference between $P$ and $Q$&#39;</span><span class="p">)</span>

<span class="c1"># Annotate KL divergences</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="sa">rf</span><span class="s1">&#39;$D_</span><span class="se">{{</span><span class="s1">KL</span><span class="se">}}</span><span class="s1">(P || Q) = </span><span class="si">{</span><span class="n">D_KL_PQ</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="sa">rf</span><span class="s1">&#39;$D_</span><span class="se">{{</span><span class="s1">KL</span><span class="se">}}</span><span class="s1">(Q || P) = </span><span class="si">{</span><span class="n">D_KL_QP</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="c1"># Labels and legend</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability Density&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Illustration of KL Asymmetry Between Two Gaussians&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/2ae51e0ed16681f28d91c380c3fc6a74ba3b4646b733bb3838fbf09462b3cc1a.png" src="../_images/2ae51e0ed16681f28d91c380c3fc6a74ba3b4646b733bb3838fbf09462b3cc1a.png" />
</div>
</div>
</section>
</section>
<section id="problems">
<h2>Problems<a class="headerlink" href="#problems" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Compute entropy of gaussian distribution. Plot entropy as a function of variance</p></li>
<li><p>Using MaxEnt approach find probability distribution with mean and variance equal to <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> respectively.</p></li>
<li><p>Simulate 1D random walk and compute entropy by first computing probability distribution <span class="math notranslate nohighlight">\(p_N(n)\)</span></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "DPotoyan/Statmech4ChemBio",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./1_stats"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Diffusion.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Diffusion</p>
      </div>
    </a>
    <a class="right-next"
       href="../2_thermo/intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Thermodynamics</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#surprise">Surprise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#addititivity-of-information">Addititivity of Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-bit-base-two">Why bit (base two)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shannon-entropy-and-information">Shannon Entropy and Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-micro-and-macro-states">Entropy, micro and macro states</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretations-of-entropy">Interpretations of Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-as-a-measure-of-information">1. Entropy as a Measure of Information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-as-a-measure-of-diversity-and-uncertainty">2. Entropy as a Measure of Diversity and Uncertainty</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#physical-and-thermodynamic-implications">3. Physical and Thermodynamic Implications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-information-physical">Is Information Physical?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-entropy-maxent-principle">Maximum Entropy (MaxEnt) Principle</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-maxent">Applications of  MaxEnt</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fair-die-example">1. Fair Die Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#biased-die-example">2. Biased Die Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-entropy">Relative Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assymetry-of-kl-and-irreversibility">Assymetry of KL and irreversibility</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-entropy-in-machine-learning">Relative Entropy in Machine learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problems">Problems</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Davit Potoyan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>