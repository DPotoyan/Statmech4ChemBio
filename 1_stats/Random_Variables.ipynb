{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables\n",
    "\n",
    ":::{admonition} What you need to know\n",
    "\n",
    "- Summing independent random variables results in another random variable called **sumple sum**. The mean of the sample sum is different from the population mean or expectation which is an exact quantity we want to approximate by sampling.\n",
    "- The **Law of Large Numbers** is a principle that states that as the number $N$, the sample mean approaches the population mean with a standard deviation falling off as $N^{-1/2}$\n",
    "- The **Central Limit Theorem (CLT)** tells us that summing independent and identically distributed random variables with well-defined means and variances results in Gaussian distribution regardless of the nature of a random variable. \n",
    "- A model of **random walk** describes the erratic, unpredictable motion of atoms and molecules, providing a fundamental model for diffusion processes and molecular motion in fluids.\n",
    "The number of steps to the right (or left) of a 1D random walker results in a binomial probability distribution. Following CLT binomial distribution in the large N limit can be shown to be well approximated by gaussian with the same mean and variance.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A random variable X** is a variable whose value depends on the realization of experiment or simulations. \n",
    "    - $X(\\omega)$ is a function from possible outcomes of a sample space $\\omega \\in \\Omega$.\n",
    "    - For a coin toss $\\Omega={H,T}$ $X(H)=+1$ and $X(T)=-1$. Every time the experiment is done, X returns either +1 or -1. We could also make functions of random variables, e.g., every time X=+1, we ear 25 cents, etc. \n",
    "\n",
    "- Random variables are classified into two main types: **discrete and continuous.**\n",
    "\n",
    "    - **Discrete Random Variable:** It assumes a number of distinct values. Discrete random variables are used to model scenarios where outcomes can be counted, such as the number of particles emitted by a radioactive source in a given time interval or the number of photons hitting a detector in a certain period.\n",
    "\n",
    "    - **Continuous Random Variable:** It can take any value within a continuous range. These variables describe quantities that can vary smoothly, such as the position of a particle in space, the velocity of a molecule in a gas, or the energy levels of an atom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random numbers in python\n",
    "\n",
    "- The [**numpy.random**](https://docs.scipy.org/doc/numpy-1.15.1/reference/routines.random.html) has the fastest random number generators based on low-level code written in C. \n",
    "- The [**Scipy.stats**](https://docs.scipy.org/doc/scipy/reference/stats.html ) has an extensive library of statistical distributions and tools for statistical analysis.\n",
    "\n",
    "- First, we take a look at the most widely used random numbers of numpy, also called standard random numbers. These are rand (uniform random number on interval 0,1) and randn (stnadard average random number with 0 mean and 1 variance). \n",
    "\n",
    "- When running code that uses random numbers results will always differ for every run. If you want code to reproduce the same result, you can fix the seed to get reproducible results: ``` np.random.seed(8376743)```\n",
    "\n",
    "- To convert random variables to probability distributions we need to generate large enough sample then perform histogramming via ```np.hist``` or directly histogram and visualize by one shot via ```plt.hist()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.random.rand(50)\n",
    "\n",
    "print(X)\n",
    "plt.plot(X, '-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Probability Distribution of a Random Variable**\n",
    "\n",
    "- For any random variable $ X $, we are interested in finding the probability distribution over its possible values $ x $, denoted as $ p_X(x) $.\n",
    "- It is important to distinguish between:\n",
    "  - $ x $, which represents a **specific value** the variable can take (e.g., $ 1,2, \\dots, 6 $ for a die).\n",
    "  - $ X $, which is the **random variable itself**, generating values $x$ according to the probability distribution $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogramming\n",
    "\n",
    "- Histograms provide an empirical estimate of distributions.\n",
    "- Continuous distributions require density functions (PDFs), while discrete distributions use probability mass functions (PMFs).\n",
    "- The bin width in histograms affects visualization, especially for discrete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, edges = np.histogram(X, range=(0,1), bins=20)\n",
    "\n",
    "print(counts, edges)\n",
    "\n",
    "plt.hist(X, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, poisson\n",
    "\n",
    "# Generate data for continuous distribution (Normal)\n",
    "np.random.seed(42)\n",
    "x_continuous = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "# Generate data for discrete distribution (Poisson)\n",
    "x_discrete = np.random.poisson(lam=3, size=1000)\n",
    "\n",
    "# Define x values for theoretical curves\n",
    "x_cont_range = np.linspace(-4, 4, 1000)\n",
    "x_disc_range = np.arange(0, 10)\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Continuous distribution (Normal)\n",
    "axes[0].hist(x_continuous, bins=30, density=True, alpha=0.6, color='b', edgecolor='black', label=\"Histogram\")\n",
    "axes[0].plot(x_cont_range, norm.pdf(x_cont_range, loc=0, scale=1), 'r-', lw=2, label=\"PDF\")\n",
    "axes[0].set_title(\"Continuous Distribution (Normal)\")\n",
    "axes[0].set_xlabel(\"Value\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Discrete distribution (Poisson)\n",
    "axes[1].hist(x_discrete, bins=np.arange(11)-0.5, density=True, alpha=0.6, color='g', edgecolor='black', label=\"Histogram\")\n",
    "axes[1].scatter(x_disc_range, poisson.pmf(x_disc_range, mu=3), color='r', label=\"PMF\", zorder=3)\n",
    "axes[1].set_title(\"Discrete Distribution (Poisson)\")\n",
    "axes[1].set_xlabel(\"Value\")\n",
    "axes[1].set_ylabel(\"Probability\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Expectation and Variance**\n",
    "\n",
    "- The expectation of a random variable, $ E[x] $, represents the **theoretical mean**, distinguishing it from the **sample mean** computed in simulations.\n",
    "- For example, consider the difference between:\n",
    "  - The average height of people computed from a sample of cities.\n",
    "  - The true mean height of the entire world population.\n",
    "- As the sample size increases, the sample mean **converges to** the expectation.\n",
    "\n",
    "- Expectation can be applied to:\n",
    "  - The variable itself (**mean**).\n",
    "  - Any function of the variable (e.g., squared deviation for **variance**).\n",
    "\n",
    ":::{admonition} **Expectation of a Random Variable**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "E[f(x)] = \\int f(x) \\cdot p(x) \\,dx\n",
    "$$\n",
    "\n",
    "- When $ f(x) = x $, we obtain the **mean**, denoted by $ \\mu $:\n",
    "\n",
    "$$\n",
    "E[x] = \\int x \\cdot p(x) \\,dx = \\mu\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "- Using the definition of expectation, we define **variance**, which quantifies the spread of $ x $.\n",
    "\n",
    ":::{admonition} **Variance as the Expectation of Mean Fluctuations**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "V[x] = E[(x - E[x])^2] = E[x^2] - E[x]^2 = \\sigma^2\n",
    "$$\n",
    "\n",
    "- We often use the shorthand notation for variance:  \n",
    "  $\\sigma^2 = V[x]$, where $ \\sigma $ is the **standard deviation**.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binomial\n",
    "\n",
    "- A an example of discrete distribution Binomial is defined by a Probability Mass Function (PMF)\n",
    "\n",
    "$$P(n |p, N) =  \\frac{N!}{(N-n)! n!}p^n (1-p)^{N-n}$$\n",
    "\n",
    "- $E[n] = Np$\n",
    "- $V[n] = 4Np(1-p)$\n",
    "\n",
    "\n",
    "**Random Variable**\n",
    "\n",
    "- $B(n, p)$ modeled by ```np.random.binomial(n, p, size)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "r = np.random.binomial(n=10, p=0.6, size=2000) \n",
    "\n",
    "fig, ax = plt.subplots(ncols=2) \n",
    "ax[0].plot(r,  color='blue', label='trajectory')\n",
    "ax[1].hist(r,  density=True, color='red',  label = 'histogram')\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Samples of RN')\n",
    "ax[0].set_ylabel('Values of RN')\n",
    "\n",
    "ax[1].set_xlabel('Values of RN')\n",
    "ax[1].set_ylabel('Probability Density')\n",
    "fig.legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian\n",
    "\n",
    "- A an example of continuous distribution Gaussian is defined by a Probability Distribution Function\n",
    "\n",
    "$$P(x |\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "- $E[x] = \\mu$\n",
    "- $V[x] = \\sigma^2$\n",
    "\n",
    "**Random Variable**\n",
    "\n",
    "- $N(a, b)$ modeled by ```np.random.normal(loc,scale, size=(N, M))```\n",
    "- $N(0, 1)$ modeled by ```np.random.randn(N, M, P, ...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# For a standard normal with sigma=1, mu=0\n",
    "r = np.random.randn(200)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2) \n",
    "ax[0].plot(r,  color='blue', label='trajectory')\n",
    "ax[1].hist(r,  density=True, color='red',  label = 'histogram')\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Samples of RN')\n",
    "ax[0].set_ylabel('Values of RN')\n",
    "\n",
    "ax[1].set_xlabel('Values of RN')\n",
    "ax[1].set_ylabel('Probability Density')\n",
    "fig.legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Distribution\n",
    "\n",
    "- A simple example of a continuous distribution is the **Uniform distribution**, where all values within a given range are equally likely. It is defined by the **Probability Density Function (PDF)**:\n",
    "\n",
    "$$\n",
    "P(x | a, b) =\n",
    "\\begin{cases} \n",
    "\\frac{1}{b - a}, & a \\leq x \\leq b \\\\ \n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Expectation and Variance:**\n",
    "  - $E[x] = \\frac{a + b}{2}$\n",
    "  - \\$V[x] = \\frac{(b - a)^2}{12}$\n",
    "\n",
    "**Random Variable**  \n",
    "\n",
    "- $U(a, b)$ is modeled by:  \n",
    "  ```python\n",
    "  np.random.uniform(low, high, size=(N, M))\n",
    "  ```\n",
    "- $U(0,1)$ (standard uniform) is modeled by:  \n",
    "  ```python\n",
    "  np.random.rand(N, M, P, ...)\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# For a standard uniform\n",
    "r = np.random.random(200)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2) \n",
    "ax[0].plot(r,  color='blue', label='trajectory')\n",
    "ax[1].hist(r,  density=True, color='red',  label = 'histogram')\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Samples of RN')\n",
    "ax[0].set_ylabel('Values of RN')\n",
    "\n",
    "ax[1].set_xlabel('Values of RN')\n",
    "ax[1].set_ylabel('Probability Density')\n",
    "fig.legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact vs sampled probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define range and step size\n",
    "xmin, xmax, step = -4, 4, 200\n",
    "dx = (xmax - xmin) / step\n",
    "\n",
    "# Generate x values and compute normal distribution\n",
    "x = np.linspace(xmin, xmax, step)\n",
    "px = norm.pdf(x)  # Standard normal PDF\n",
    "\n",
    "# Check normalization\n",
    "normalization = np.sum(px * dx)\n",
    "print('Normalization:', normalization)\n",
    "\n",
    "# Generate random samples\n",
    "r = np.random.randn(1000)  # Increased sample size for better histogram resolution\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Histogram of sampled data\n",
    "plt.hist(r, bins=30, density=True, alpha=0.6, color='blue', edgecolor='black',\n",
    "         label=f'Sampled: mean={r.mean():.2f}, var={r.var():.2f}')\n",
    "\n",
    "# Plot theoretical normal distribution\n",
    "plt.plot(x, px, 'k-', linewidth=2, label='Exact: mean=0, var=1')\n",
    "\n",
    "# Formatting\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.ylabel(r'$p(x)$', fontsize=14)\n",
    "plt.xlabel(r'$x$', fontsize=14)\n",
    "plt.title(\"Comparison of Sampled Data with Normal Distribution\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn about Transforming Random Variables  \n",
    "\n",
    "- When a random variable $X $ is transformed by adding, multiplying by a constant, or applying a function $ Y = f(X) $, its probability distribution changes accordingly from $ p(x) $ to $ p(y) $.  \n",
    "- Two commonly used transformations in statistical modeling involve generating specific distributions from standard forms:\n",
    "\n",
    "  - **Generating a Gaussian (Normal) distribution** from a standard normal:  \n",
    "\n",
    "    $$\n",
    "    N(\\mu, \\sigma^2) = \\mu + \\sigma \\cdot N(0,1)\n",
    "    $$\n",
    "\n",
    "  - **Generating a Uniform distribution** from a standard uniform:  \n",
    "\n",
    "    $$\n",
    "    U(a, b) = (b - a) \\cdot U(0,1) + a\n",
    "    $$\n",
    "\n",
    "- These transformations are frequently used to construct random samples from desired distributions in simulations and statistical mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{admonition} **Transforming Random Variables**\n",
    ":class: dropdown, tip\n",
    "\n",
    "- When transforming a random variable $ X $ to a new variable $ Y = f(X) $, the probability density functions are related by a **Jacobian factor** to account for how the transformation stretches or compresses the distribution:\n",
    "\n",
    "$$\n",
    "p(x) dx = p(y) dy\n",
    "$$\n",
    "\n",
    "which gives:\n",
    "\n",
    "$$\n",
    "p(y) = p(x) \\cdot \\Bigg| \\frac{dx}{dy} \\Bigg|\n",
    "$$\n",
    "\n",
    "- **Examples of Simple Transformations:**\n",
    "  1. **Addition:** $ Y = X + a $\n",
    "     - The probability remains unchanged except for a shift:  \n",
    "\n",
    "       $$\n",
    "       p(y) = p(x + a) \\cdot 1\n",
    "       $$\n",
    "\n",
    "  2. **Multiplication:** $ Y = aX $\n",
    "     - The distribution scales with a factor $ \\frac{1}{|a|} $:  \n",
    "\n",
    "       $$\n",
    "       p(y) = p(x) \\cdot \\frac{1}{|a|}\n",
    "       $$\n",
    "\n",
    "- These transformations yield useful properties:\n",
    "  - **Shifting the Mean:**  \n",
    "\n",
    "    $$\n",
    "    E[X + a] = E[X] + a\n",
    "    $$\n",
    "\n",
    "  - **Scaling the Variance:**  \n",
    "\n",
    "    $$\n",
    "    V[aX] = a^2 V[X]\n",
    "    $$\n",
    "\n",
    "- Using these properties, we can generate:\n",
    "\n",
    "  - A **Gaussian (Normal) distribution** from a standard normal:\n",
    "\n",
    "    $$\n",
    "    N(\\mu, \\sigma^2) = \\mu + \\sigma \\cdot N(0,1)\n",
    "    $$\n",
    "\n",
    "  - A **Uniform distribution** from a standard uniform:\n",
    "\n",
    "    $$\n",
    "    U(a, b) = (b - a) \\cdot U(0,1) + a\n",
    "    $$\n",
    "    \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "mu, sigma = 5, 2  # Mean and standard deviation for Gaussian\n",
    "a, b = 2, 8       # Bounds for Uniform\n",
    "\n",
    "# Generate standard distributions\n",
    "std_normal = np.random.randn(10000)  # N(0,1)\n",
    "std_uniform = np.random.rand(10000)  # U(0,1)\n",
    "\n",
    "# Transform distributions\n",
    "normal_dist = mu + sigma * std_normal  # N(mu, sigma^2)\n",
    "uniform_dist = a + (b - a) * std_uniform  # U(a, b)\n",
    "\n",
    "# Plot Distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Normal Distribution\n",
    "axes[0].hist(normal_dist, bins=40, density=True, alpha=0.6, color='b', edgecolor='black')\n",
    "axes[0].set_title(f\"Transformed Normal Distribution N({mu}, {sigma}²)\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "\n",
    "# Uniform Distribution\n",
    "axes[1].hist(uniform_dist, bins=20, density=True, alpha=0.6, color='g', edgecolor='black')\n",
    "axes[1].set_title(f\"Transformed Uniform Distribution U({a}, {b})\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sum of Two Random Variables\n",
    "\n",
    "- Consider the sum of two random variables, such as:\n",
    "  - The sum of numbers obtained from rolling two dice.\n",
    "  - The sum of two coin flips (e.g., heads = 1, tails = 0).\n",
    "  - Sum of kinetic eneries of ideal gas. \n",
    "\n",
    "$$\n",
    "X = X_1 + X_2\n",
    "$$\n",
    "\n",
    "- The sum of random variables is itself a random variable! \n",
    "- We want to understand how to described the properties of summed random variables  as they offer a prototype of how large systems emerge froms mall components. \n",
    "- Given probability distirbution of $X_1$ and $X_2$ how do we find probability distribution of X?\n",
    "\n",
    "### Expectation and Variance of the Sum\n",
    "\n",
    "- Expectation is always a **linear operator**, which follows from the definition of expectation and the linearity of integration:\n",
    "\n",
    "$$\n",
    "E[X_1 + X_2] = E[X_1] + E[X_2]\n",
    "$$\n",
    "\n",
    "- However, variance is **not** generally a linear operator. To see this let us write explicit formula first:\n",
    "\n",
    "$$\n",
    "V[X_1 + X_2] = E\\left[(X_1 + X_2 - E[X_1 + X_2])^2\\right] \n",
    "$$\n",
    "\n",
    "- Defining the **mean-subtracted variables**: $Y_i = X_i - E[X_i]$ we express variance of sum in terms of variances of component random variables \n",
    "\n",
    "$$\n",
    "V[X_1 + X_2] = E\\left[(X_1 - E[X_1] + X_2 - E[X_2])^2\\right] = E\\left[(Y_1 + Y_2)^2\\right]\n",
    "$$\n",
    "\n",
    "- Since $ V[X_i] = E[Y_i^2] $, this simplifies to:\n",
    "\n",
    "$$\n",
    "V[X_1 + X_2] = E[Y^2_1] + V[Y^2_2] + 2E[Y_1 Y_2] = V[X_1] + V[X_2] + 2 Cov[X_1, X_2]\n",
    "$$\n",
    "\n",
    "- The cross term is called **Covariance** which measures the degree to which two random variables **vary together**:\n",
    "\n",
    ":::{admonition} **Covariance and Correlation of Two Random Variables**  \n",
    ":class: important  \n",
    "\n",
    "$$\n",
    "\\text{Cov}[X_1, X_2] = E[(X_1 - E[X_1])(X_2 - E[X_2])]\n",
    "$$\n",
    "\n",
    "- If $Cov > 0 $ or $Cov < 0 $  we have **positive/negative correlation**  and if $Cov=0$ the variables are **uncorrelated** (but not necessarily independent)\n",
    "\n",
    "\n",
    "To obtain a **scale-independent** measure, we define the **correlation coefficient**:\n",
    "\n",
    "$$\n",
    "\\text{Corr}[X_1, X_2] = \\frac{\\text{Cov}[X_1, X_2]}{\\sigma_{X_1} \\sigma_{X_2}}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "- In the special case where $X_1 $ and $X_2 $ are **independent**, covariane is zero and we have additivity of variances!\n",
    "\n",
    "\n",
    "$$V[X_1+X_2] = V[X_1]+V[X_2]$$\n",
    "\n",
    "- This result is **fundamental** in statistical mechanics, probability theory, and the sciences, as it explains why variances add for independent random variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of $ N $ Random Variables  \n",
    "\n",
    "- Consider a sequence of **independent and identically distributed (i.i.d.)** random variables, $ X_1, X_2, \\ldots, X_n $.  \n",
    "- Since they are **identically distributed**, each variable has a well-defined **mean** $ \\mu $ and **variance** $ \\sigma^2 $.  \n",
    "- Our goal is to understand how the **sum** and **mean** of these variables depend on the sample size $ n $.\n",
    "\n",
    ":::{admonition} **Sample Sum and Sample Mean**  \n",
    ":class: important  \n",
    "\n",
    "$$\n",
    "S_n = \\sum_{i=1}^{n} X_i, \\quad M_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "$$\n",
    "\n",
    "- $ S_n $ is the **sample sum**, and $ M_n $ is the **sample mean**.  \n",
    "- These quantities fluctuate with sample size $ n $, but we expect them to **converge to their expectations** for large $ n $ \n",
    "\n",
    ":::\n",
    "\n",
    "- Because the random variables are **independent**, all cross terms vanish for $i \\neq j$ between mean subtracted variables $Y_i = X_i-E[X_i]$\n",
    "\n",
    ":::{admonition} **Mean and Variance of the Sum of i.i.d. Random Variables**  \n",
    ":class: important  \n",
    "\n",
    "- **Expectation of the Sum:**  \n",
    "\n",
    "$$\n",
    "E[S_n] = E\\left[ \\sum_{i=1}^{n} X_i \\right] = \\sum_{i=1}^{n} E[X_i] = n\\mu\n",
    "$$\n",
    "\n",
    "- **Variance of the Sum:**  \n",
    "\n",
    "$$\n",
    "V[S_n] = E\\left[ (S_n - n\\mu)^2 \\right] = \\Bigg[\\sum_{i=1}^{n}  Y_i \\Bigg]^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n}E[Y_i Y_j] = \\sum_{i=1}^{n} V[X_i] = n\\sigma^2\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law of Large Numbers\n",
    "\n",
    "- For the **sample mean** the result of summatiion of i.i.d variables implies\n",
    "\n",
    "$$\n",
    "E[M_n] = \\frac{1}{n} E[S_n] = \\mu\n",
    "$$\n",
    "\n",
    "$$\n",
    "V[M_n] = \\frac{1}{n^2} V[S_n] = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "- Thus, the sample mean is an **unbiased estimator** of $ \\mu $, and its variance decreases as $ 1/n $, meaning that the estimate becomes more stable as $ n $ increases.\n",
    "\n",
    "\n",
    ":::{admonition} **Law of Large Numbers (LLN)**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "E[M_n] \\to \\mu\n",
    "$$  \n",
    "\n",
    "$$V[M_n] \\to \\sigma^2 / n$$\n",
    "\n",
    "**Implication:**  \n",
    "- The sample mean provides a reliable estimate of $\\mu$ for large $n$.  \n",
    "- The variance of $M_n$ decreases as , meaning fluctuations shrink as $1/\\sqrt{n}$.  \n",
    "- This justifies ensemble averaging in statistical mechanics, ensuring macroscopic observables (e.g., temperature, pressure) are stable and predictable.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of trials and runs\n",
    "N, runs = int(1e5), 30\n",
    "\n",
    "# Store fractions of heads for each trial in each run\n",
    "fractions = np.zeros((runs, N))\n",
    "\n",
    "# Simulate coin tosses\n",
    "for run in range(runs):\n",
    "    # Generate coin tosses (0 for tails, 1 for heads)\n",
    "    tosses = np.random.randint(2, size=N)\n",
    "    # Calculate cumulative sum to get the number of heads up to each trial\n",
    "    cum_heads = np.cumsum(tosses)\n",
    "    # Calculate fraction of heads up to each trial\n",
    "    fractions[run, :] = cum_heads / np.arange(1, N+1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot all runs with low opacity\n",
    "for run in range(runs):\n",
    "    plt.plot(fractions[run, :], color='grey', alpha=0.3)\n",
    "\n",
    "# Highlight first run\n",
    "plt.semilogx(fractions[0, :], color='blue', linewidth=2, label='Highlighted Run')\n",
    "\n",
    "# Expected value line\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='Expected Value (0.5)')\n",
    "plt.xlabel('Number of Trials')\n",
    "plt.ylabel('Fraction of Heads')\n",
    "plt.title('Law of Large Numbers: Fraction of Heads in Coin Tossing')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Central Limit Theorem  (CLT)\n",
    "\n",
    "- **Central Limit Theorem** asserts that the probability distribution function or **PDF** of sum of random variables becomes gaussian distribution with mean $n\\mu$ and $n\\sigma^2$. \n",
    "- Note that CLT is based on assumption that the **mean and variance**, $\\mu$ and $\\sigma^2$, **are finite!**. Thus, CLT does not hold for certain power-law distributed random variables. \n",
    "\n",
    ":::{admonition} **Central Limit Theorem  (CLT)**\n",
    ":class: important\n",
    "\n",
    "- Sum of any i.i.d variables (even if they are not gaussian) leads to normally distributed random variable (the sum $s_n$)\n",
    "\n",
    "$$X_1 +X_2+...+X_n \\rightarrow N(n\\mu, n\\sigma^2)$$\n",
    "\n",
    "- The probability density function (PDF) of $S_n$ is approaching gaussian: \n",
    "\n",
    "$$p(s) = \\frac{1}{(2\\pi  n\\sigma^2)^{1/2}}e^{-\\frac{(s-n\\mu)^2}{2 n\\sigma^2}}$$\n",
    "\n",
    ":::\n",
    "\n",
    "- If we subtract mean and scale the sample sum by its standard deviation we will get a standard normal distribution. \n",
    "\n",
    "$$Z_n = \\frac{S_n - n\\mu}{\\sqrt{n}\\sigma} \\rightarrow N(0, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Number of coin tosses in each experiment, number of experiments\n",
    "N, runs    = 100, 1000  \n",
    "\n",
    "# Simulate coin tosses: num_experiments rows, num_tosses_per_experiment columns\n",
    "tosses = np.random.randint(2, size=(N, runs))\n",
    "\n",
    "# Calculate means of each experiment\n",
    "M = np.mean(tosses, axis=0)\n",
    "\n",
    "z = ( M-M.mean() ) / np.std(M)\n",
    "\n",
    "# Plotting the distribution of sample means\n",
    "plt.figure()\n",
    "plt.hist(z, density=True, bins=30)\n",
    "plt.title('Distribution of Sample Means of Coin Tosses')\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "zs = np.linspace(z.min(), z.max(), 1000)\n",
    "plt.plot(zs, norm.pdf(zs),'k', label='mean=0, var=1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Example of CLT applied to random walk problem**\n",
    ":class: note, dropdown\n",
    "\n",
    "Applying the formulas to random walk model we get mean and variance for single step\n",
    "\n",
    "$$E[X_1] = \\theta \\cdot 1 + (1-\\theta) \\cdot (-1) = 2\\theta-1$$ \n",
    "\n",
    "$$V[X_1] = E[X^2_1] -  E[X_1]^2 = \\theta \\cdot 1^2+ (1-\\theta) (-1)^2 - (2\\theta-1)^2 = 4 \\theta(1-\\theta)$$\n",
    "\n",
    "Since steps of a random walker are independent we can compute the variance of a total displacement by multiplying mean and varaince of a single step by N \n",
    "\n",
    "$$E[x]=N(2\\theta -1)$$\n",
    "\n",
    "$$V[x]=N\\bar{\\sigma^2_1} = 4N\\theta (1-\\theta)$$ \n",
    "\n",
    "The variance of the mean $\\bar{x} = x/N$ would then be:\n",
    "\n",
    "$$V[\\bar{x}] = \\frac{4\\theta (1-\\theta)}{N}$$ \n",
    "\n",
    "::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating a 1D unbiased random walk \n",
    "\n",
    "- Each random walker will be modeled by a random variable $X_i$, assuming +1 or -1 values at every step. We will run N random walkers (rows) over n steps (columns)\n",
    "- We then take **cumulative sum  over n steps** thereby summing n random variables for N walkers. This will be done via a convenient ```np.cumsum()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rw_1d(n, N):\n",
    "    \"\"\"\n",
    "    Simulates a 1D symmetric random walk.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Number of steps.\n",
    "    N (int): Number of walkers.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A (n, N) array where each column represents a walker's trajectory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate random steps (-1 or +1) for all walkers\n",
    "    steps = np.random.choice([-1, 1], size=(n, N))\n",
    "    \n",
    "    # Compute cumulative sum to get displacement\n",
    "    rw = np.cumsum(steps, axis=0)\n",
    "\n",
    "    # Ensure the initial position is zero\n",
    "    rw = np.vstack([np.zeros(N), rw])  # Adds a row of zeros at the start\n",
    "\n",
    "    return rw\n",
    "\n",
    "# Example usage: Simulate and plot a few random walks\n",
    "n_steps = 1000\n",
    "n_walkers = 3\n",
    "rw = rw_1d(n_steps, n_walkers)\n",
    "\n",
    "plt.plot(rw)\n",
    "plt.ylabel('X (displacement)')\n",
    "plt.xlabel('n (steps)')\n",
    "plt.title('1D Random Walk')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate 1D random walk\n",
    "def rw_1d(n_max, N):\n",
    "    steps = np.random.choice([-1, 1], size=(n_max, N))\n",
    "    return np.cumsum(steps, axis=0)\n",
    "\n",
    "# Parameters\n",
    "n_max = 1000  # Time steps\n",
    "N = 1000      # Number of walkers\n",
    "rw = rw_1d(n_max, N)\n",
    "\n",
    "# Define time snapshots\n",
    "time_snapshots = [10, 100, 500, 900]\n",
    "\n",
    "# Create a multi-column subplot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=len(time_snapshots), figsize=(15, 6))\n",
    "\n",
    "for i, t in enumerate(time_snapshots):\n",
    "    # Plot random walk trajectories\n",
    "    ax = axes[0, i]\n",
    "    ax.plot(rw[:, :50], alpha=0.3)  # Show 50 trajectories for clarity\n",
    "    ax.axvline(x=t, color='black', lw=2)  # Mark current time step\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('X')\n",
    "    ax.set_title(f'Time t={t}')\n",
    "\n",
    "    # Histogram of positions at time t\n",
    "    ax_hist = axes[1, i]\n",
    "    ax_hist.hist(rw[t, :], bins=30, color='orange', density=True, alpha=0.6, label=f't={t}')\n",
    "    \n",
    "    # Gaussian overlay\n",
    "    x = np.linspace(-100, 100, 1000)\n",
    "    y = stats.norm.pdf(x, 0, np.sqrt(t))\n",
    "    ax_hist.plot(x, y, color='black', lw=2, label='Normal')\n",
    "\n",
    "    ax_hist.set_xlim([-100, 100])\n",
    "    ax_hist.legend()\n",
    "    ax_hist.set_title(f'$\\sigma/t$ = {np.var(rw[t, :])/t:.3f}')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean square displacement (MSD) of a random walker\n",
    "\n",
    "- After time n number of steps (or time t) how far has random walker moved from the origin?\n",
    "\n",
    "$$R_n = \\sum^{n-1}_{i=0}X_n$$\n",
    "\n",
    "- We quantify this by computing **Mean Square Displacement (MSD)**. Note that the mean is computed over N number of simulated trajectories (ensemble average). Invoking central limit theorem, or simply realizing that off diagonal terms drop off we end up with the same result as in LLN.\n",
    "\n",
    "$$\n",
    "MSD(n)= \\Big\\langle \\big ( R_n - R_0 \\big)^2 \\Big \\rangle \\sim n\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "n, N = 2000, 1000\n",
    "rw = rw_1d(n, N)\n",
    "\n",
    "t = np.arange(n)\n",
    "\n",
    "R2 = (rw[:, :] - rw[0, :])**2 # Notice we subtract initial time\n",
    "\n",
    "msd =  np.mean(R2, axis=1)    # Notice we average over N\n",
    "\n",
    "plt.loglog(t, np.sqrt(msd), lw=3) \n",
    "\n",
    "plt.loglog(t, np.sqrt(t), '--')\n",
    "\n",
    "plt.title('Compute mean square deviation of 1D random walker',fontsize=15)\n",
    "plt.xlabel('Number of steps, n',fontsize=15)\n",
    "plt.ylabel(r'$MSD(n)$',fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rw_2d(n, N):\n",
    "    \"\"\"\n",
    "    Simulates a 2D symmetric random walk.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Number of steps.\n",
    "    N (int): Number of trajectories.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A (n+1, N, 2) array where each trajectory is stored in the last dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define possible step directions (right, up, left, down)\n",
    "    steps = np.array([(1, 0), (0, 1), (-1, 0), (0, -1)])\n",
    "    \n",
    "    # Generate random step indices and map to step directions\n",
    "    random_steps = steps[np.random.choice(4, size=(n, N))]\n",
    "    \n",
    "    # Prepend an initial position at (0,0) for all walkers\n",
    "    rw = np.zeros((n + 1, N, 2), dtype=int)\n",
    "    rw[1:] = np.cumsum(random_steps, axis=0)  # Compute displacement over time\n",
    "\n",
    "    return rw\n",
    "\n",
    "# Example usage: Simulate and plot first three random walkers\n",
    "n_steps = 1000\n",
    "n_walkers = 100\n",
    "traj = rw_2d(n_steps, n_walkers)\n",
    "\n",
    "plt.plot(traj[:, :3, 0], traj[:, :3, 1])  # Plot first three random walkers\n",
    "plt.xlabel('X (displacement)')\n",
    "plt.ylabel('Y (displacement)')\n",
    "plt.title('2D Random Walk')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**The mighty little books**\n",
    "-  [\"Random Walks in Biology\",  H Berg (1993)](https://www.amazon.com/Random-Walks-Biology-Howard-Berg/dp/0691000646)\n",
    "-  [\"Physical models of Living systems\",  P Nelson (2015)](https://www.amazon.com/gp/product/1464140294/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&psc=1)\n",
    "\n",
    "**More in depth**\n",
    " - [\"Simple Brownian Diffusion: An Introduction to the Standard Theoretical Models\", D Gillespie](https://www.amazon.com/Simple-Brownian-Diffusion-Introduction-Theoretical/dp/0199664501/ref=sr_1_1?keywords=diffusion+brownian&qid=1579882520&sr=8-1)\n",
    " - [\"Stochastic Processes for Physicists\" K Jacobs](https://www.amazon.com/Stochastic-Processes-Physicists-Understanding-Systems/dp/0521765420/ref=sr_1_1?keywords=kurt+jacobs+stochastic&qid=1579882738&sr=8-1)\n",
    " \n",
    "**On the applied side**\n",
    "- [Brownian Motion: Elements of Colloid Dynamics A P Philipse (2018)](https://www.amazon.com/Brownian-Motion-Elements-Dynamics-Undergraduate/dp/3319980521/ref=sr_1_7?keywords=einstein+brownian&qid=1579882356&sr=8-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "#### Problem 1 Binomial as generator of Gaussian and Poisson distributions\n",
    "\n",
    "- Show that in large number limit binomial distribution tends to gaussian. Show is by expanding binomial distirbution $logp(n)$ in power series showing that terms beyond quadratic can be ignored. \n",
    "\n",
    "- In the limit $N\\rightarrow \\infty$ but for very small values of $p \\rightarrow 0$ such that $\\lambda =pN=const$ there is another distribution that better approximates Binomial distribution: $p(x)=\\frac{\\lambda^k}{k!}e^{-\\lambda} $ It is known as Poisson distribution. <br>\n",
    "Poisson distribution is an excellent approximation for probabilities of rare events. Such as, infrequently firing neurons in the brain, radioactive decay events of Plutonium or rains in the desert. <br>  Derive Poisson distribution by taking the limit of $p\\rightarrow 0$ in binomial distribution.\n",
    "\n",
    "- Using numpy and matplotlib plot binomial probability distribution\n",
    "against Gaussian and Poisson distributions for different values of N=(10,100,1000,10000). <br>\n",
    "- For a value N=10000 do four plots with the following values \n",
    "p=0.0001, 0.001, 0.01, 0.1. You can use  subplot functionality to make a pretty 4 column plot. (See plotting module)\n",
    "\n",
    "```python\n",
    "fig, ax =  plt.subplots(nrows=1, ncols=4)\n",
    "ax[0].plot()\n",
    "ax[1].plot()\n",
    "ax[2].plot()\n",
    "ax[3].plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem-2 Confined diffusion.\n",
    "\n",
    "Simulate 2D random walk in a circular confinement. Re-write 2D random walk  code to simulate diffusion of a particle which is stuck inside a sphere. \n",
    "Study how root mean square deviation of position scales with time. \n",
    "- Carry out simulations for different confinement sizes. \n",
    "- Make plots of simulated trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem-3 Return to the origin!\n",
    "\n",
    "- Simulate random walk in 1D and 2D for a different number of steps $N=10, 10^2,10^3, 10^4, 10^5$\n",
    "- Compute average number of returns to the origin $\\langle n_{orig} \\rangle$. That is number of times a random walker returns to the origin $0$ for 1D  or (0,0)$ for 2D . You may want to use some 1000 trajectories to obtain average. \n",
    "- Plot how $\\langle n_{orig} \\rangle$ depends on number of steps N for 1D and 2D walker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem-4 Breaking the CLT; Cauchy vs Normal random walk in 2D\n",
    "\n",
    "For this problem we are going to simulate two kinds of random walks in continuum space (not lattice): Levy flights and Normal distributd random walk. \n",
    "\n",
    "To simulate a 2D continuum space random walk we need to generate random step sizes $r_x$, $r_y$. \n",
    "Also you will need unifrom random namber to sample angles in 2D giving you a conitnuum random walk in 2D space: $x = r_x sin\\theta$ and $y=r_ycos\\theta$\n",
    "\n",
    "- Normally: $r\\sim N(0,1)$\n",
    "- Cauchy distribution (long tails, infinite variance) $r\\sim Cauchy(0,1)$\n",
    "- Unform angles $\\theta \\sim U(0,1)$\n",
    "\n",
    "Visualize random walk using matplotlib and study statistics of random walkers the way that is done for normal random walk/brownian motion examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem-5 Continuous time random walk (CTRW)\n",
    "\n",
    "Simulate 1D random walk but instead of picking times at regular intervals pick them from  exponential distribution. <br>\n",
    "Hint: you may want to use random variables from scipy.stats.exp <br>\n",
    "\n",
    "[scipy.stats.expon](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html) <br>\n",
    "\n",
    "Study the root mean square deviation as a function of exponential decay parameter $\\lambda$ of exponential distribution $e^{-\\lambda x}$. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
