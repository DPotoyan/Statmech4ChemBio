{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Information\n",
    "\n",
    ":::{admonition} **What you will learn**\n",
    "- Entropy is a measure of information or uncertainty, quantified using probability distributions.  \n",
    "- When expressed in $\\log_2$, entropy represents the number of binary (yes/no) questions needed to identify a microstate within a macrostate.  \n",
    "- Entropy quantifies the diversity of microstates, reaching its maximum for a uniform (flat) distributions.  \n",
    "- The Maximum Entropy (MaxEnt) principle provides the most unbiased way to infer probability distributions given empirical constraints.  \n",
    "- In MaxEnt Entropy what we are maximizing is Relative Entropy with respect to reference system where all microstates are equally probable.  \n",
    "- Relative entropy quantifies the information lost when approximating one probability distribution with another.  \n",
    "- Relative entropy also shows that information (or uncertainty) tends to increase irreversibly in the absence of external interventions.\n",
    "- Relative entropy encodes statistical Distinguishability between forwrad and reverse processes known as the thermodynamic arrow of time.  \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which of these two statements conveys the most information?**\n",
    "\n",
    "- I will eat some food tomorrow.\n",
    "- I will see a giraffe walking by my apartment. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A measure of information (whatever it may be) is closely related to the element of... surprise!**\n",
    "\n",
    "- has very high probability and so conveys little information,\n",
    "- has very low probability and so conveys much information. \n",
    "\n",
    "> If we quanitfy suprise we will quantify information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addititivity of Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge leads to gaining information**\n",
    "\n",
    "Which is more surprising (contains more information)?\n",
    "\n",
    "- E1: The card is heart? $P(E_1) = \\frac{1}{4}$\n",
    "\n",
    "- E2:The card is Queen? $P(E_2)  =  \\frac{4}{52} = \\frac{1}{13}$\n",
    "\n",
    "- E3: The card is Queen of hearts? $P(E_1 \\, and\\,  E_2) = \\frac{1}{52}$ \n",
    "\n",
    "**Knowledge of event should add up our information: $I(E) \\geq 0$**\n",
    "\n",
    "1. We learn the card is heart $I(E_1)$\n",
    "\n",
    "2. We learn the card is Queen $I(E_2)$\n",
    "\n",
    "3. $I(E_1 and E_2) = I(E_1) + I(E_2)$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A logarithm of probability is a good candidate function for information!**\n",
    "\n",
    "$$log_2 P(E_1) P(E_2) = log_2 P(E_1) + log_2(E_2)$$\n",
    "\n",
    "- What about the sign? \n",
    "\n",
    "$$I_i = -log_2 p_i$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why bit (base two)\n",
    "\n",
    "- Consider symmetric a 1D random walk with equal jump probabilities. We can view **Random walk = string of Yes/No questions**. \n",
    "- Imagine driving to a location how many left/right turn informations you need to reach destination? \n",
    "\n",
    "- You gain one bit of information when you are told Yes/No answer\n",
    "\n",
    "$$I(X=0) = I(X=1) = -log_2 \\frac{1}{2} = 1$$\n",
    "\n",
    "- To decode N step random walk trajectory we need N bits. \n",
    "\n",
    "$$(x_0,x_1,...x_N) = 10111101001010100100$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy and Information \n",
    "\n",
    "- If we want to understand the overall uncertainty in the system, we need to consider all possible outcomes weighted by their probability of occurrence.\n",
    "- This means that rather than looking at the surprise of a single event, **we consider the average surprise one would experience over many trials drawn from $p$.**\n",
    "- Thus, we take the expectation of surprise over the entire distribution $\\langle -log p \\rangle$ arriving at a formula known as Shaonon's expression of Entropy.\n",
    "\n",
    ":::{admonition} **Shanon Entropy**\n",
    ":class: important \n",
    "\n",
    "$$S = -\\sum_i p_i log_2 p_i$$\n",
    "\n",
    "- $S$ Entropy(Information) measured in bits\n",
    "- $p_i$ probability of microstate $i$, e.g coin flip or die roll outcomes\n",
    "\n",
    ":::\n",
    "\n",
    "- John von Neumann advice to [To Calude Shanon], \"You should call it Entropy, for two reasons. In the first place you uncertainty function has been used in statistical mechanics under that name. In the second place, and more importantly, no one knows what entropy really is, so in a debate you will always have the advantage.” \n",
    "\n",
    "- One often uses $H$ to denote Shanon Entropy ($log_2$) and letter $S$ with $log_e$ for entropy in units of Boltzman constant $k_B$\n",
    "- For now lets just roll with $k_B=1$ we wont be doing any thermodynamics in here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def binary_entropy(p):\n",
    "    \"\"\"\n",
    "    Compute the binary Shannon entropy for a given probability p.\n",
    "    Avoid issues with log(0) by ensuring p is never 0 or 1.\n",
    "    \"\"\"\n",
    "    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "# Generate probability values, avoiding the endpoints to prevent log(0)\n",
    "p_vals = np.linspace(0.001, 0.999, 1000)\n",
    "H_vals = binary_entropy(p_vals)\n",
    "\n",
    "# Create a figure with two subplots side-by-side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Binary Shannon Entropy Function\n",
    "ax[0].plot(p_vals, H_vals, lw=2, color='midnightblue',\n",
    "           label=r\"$H(p)=-p\\log_2(p)-(1-p)\\log_2(1-p)$\")\n",
    "ax[0].set_xlabel(r\"Probability $p$)\", fontsize=14)\n",
    "ax[0].set_ylabel(\"Entropy (bits)\", fontsize=14)\n",
    "ax[0].set_title(\"Binary Shannon Entropy\", fontsize=16)\n",
    "ax[0].legend(fontsize=12)\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Plot 2: Example Distributions and Their Entropy\n",
    "# Define a few example two-outcome distributions:\n",
    "distributions = {\n",
    "    \"Uniform (0.5, 0.5)\": [0.5, 0.5],\n",
    "    \"Skewed (0.8, 0.2)\": [0.8, 0.2],\n",
    "    \"Extreme (0.99, 0.01)\": [0.99, 0.01]\n",
    "}\n",
    "\n",
    "# Colors for each distribution\n",
    "colors = [\"skyblue\", \"salmon\", \"lightgreen\"]\n",
    "\n",
    "# For visual separation, use offsets for the bars\n",
    "width = 0.25\n",
    "x_ticks = np.arange(2)  # positions for the two outcomes\n",
    "\n",
    "for i, (label, probs) in enumerate(distributions.items()):\n",
    "    # Compute the Shannon entropy for the distribution\n",
    "    entropy_val = -np.sum(np.array(probs) * np.log2(probs))\n",
    "    # Offset x positions for clarity\n",
    "    x_positions = x_ticks + i * width - width\n",
    "    ax[1].bar(x_positions, probs, width=width, color=colors[i],\n",
    "              label=f\"{label}\\nEntropy = {entropy_val:.2f} bits\")\n",
    "    \n",
    "# Set labels and title for the bar plot\n",
    "ax[1].set_xticks(x_ticks)\n",
    "ax[1].set_xticklabels([\"Outcome 1\", \"Outcome 2\"], fontsize=12)\n",
    "ax[1].set_ylabel(\"Probability\", fontsize=14)\n",
    "ax[1].set_title(\"Example Distributions\", fontsize=16)\n",
    "ax[1].legend(fontsize=12)\n",
    "ax[1].grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{admonition} **Exercise: Information per letter $I(m)$ to decode the message** \n",
    ":class: note, dropdown\n",
    "\n",
    "- Let $m$  represent the letters in an alphabet. For example:\n",
    "\n",
    "  - **Korean:** 24 letters\n",
    "  - **English:** 26 letters\n",
    "  - **Russian:** 33 letters\n",
    "\n",
    "- The information content associated with these alphabets satisfies:\n",
    "  \n",
    "  $$\n",
    "  I(\\text{Russian}) > I(\\text{English}) > I(\\text{Korean})\n",
    "  $$\n",
    "\n",
    "- The information of a sequence of letters is additive, regardless of the order in which they are transmitted:\n",
    "\n",
    "  $$\n",
    "  I(m_1, m_2) = I(m_1) + I(m_2)\n",
    "  $$\n",
    "\n",
    "- **Question** If the symbols of English alphabet (+ blank) appear equally probably, what is the information carried by a single symbol? This must be $log_2(26 + 1) = 4.755$ bits, but for actual English sentences, it is known to be about **$1.3$ bits. Why?**\n",
    "\n",
    ":::{dropdown} **Solution**\n",
    "\n",
    "- Not every letter has equal probability or frequency of appearing in a sentence!\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{admonition} **Exercise: entropy of die rolls** \n",
    ":class: note, dropdown\n",
    "\n",
    "- How much knowledge we need to find out outcome of fair dice?\n",
    "\n",
    "- We are told die shows a digit higher than 2 (3, 4, 5 or 6). How much knowledge does this information carry? \n",
    "\n",
    "\n",
    ":::{dropdown} **Solution**\n",
    "\n",
    " - $H(E_1) = log_2 6$\n",
    " \n",
    " \n",
    " - $H(E_1) - H(E_2) = log_2 6 - log_2 4$\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{admonition} **Exercise: Two cats** \n",
    ":class: note, dropdown\n",
    "\n",
    "There are two kittens. We are told that at least one of them is a male. What is the information we get from this message?\n",
    "\n",
    ":::{dropdown} **Solution**\n",
    "\n",
    "$$E_1 = \\{mm,mf,fm, ff \\} $$\n",
    "\n",
    "$$E_2 = \\{mm,mf,fm\\}$$\n",
    "\n",
    "$$H(E_1) -H(E_2) = log_2 4 -log_2 3 = 0.41$$\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{admonition} **Exercise: Monty Hall problem** \n",
    ":class: note, dropdown\n",
    "\n",
    "There are five boxes, of which one contains a prize. A game participant is asked to choose one box. After they choose one of the five boxes, the “coordinator” of the game identifies as empty three of the four unchosen boxes. What is the information of this message? \n",
    "\n",
    ":::{dropdown} **Solution**\n",
    "\n",
    "- $H(E_1) = log_2 5 = 2.322$\n",
    "\n",
    "\n",
    "- $H(E_2) = -\\frac{1}{5} log_2 5 - \\frac{4}{5} log_2 \\frac{4}{5} = 0.722$\n",
    "\n",
    "- $H(E_1)-H(E_2) = 1.6$\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{admonition} **Exercise: Why are there non-integer number of YES/NO questions??** \n",
    ":class: note, dropdown\n",
    "\n",
    "Explain the origin of the non-integer information. Why it takes less than one-bit to encode information? \n",
    "\n",
    ":::{dropdown} **Solution**\n",
    "\n",
    "\n",
    "- We have encountered a fraction of bit of information several times now. What does it imply in terms of number of YES/NO questions. That is becasue in some cases single YES/NO question can rule out more than one elementary event.\n",
    "\n",
    "- In other words we can ask clever questions that can get us to answer faster than doing YES/No on every single possibility\n",
    "\n",
    "- 999 blue balls and 1 red ball. how many questions we need to ask to determin the colors of all balls? $S = 9.97$ bit or 0.01 bit per ball. Divide the container by 500 and 500 and ask where the red ball is? 1 questions rules out 500 balls at once. \n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy, micro and macro states \n",
    "\n",
    "\n",
    "- When all $\\Omega$ number of microstates of the system have equal probability $p_i=\\frac{1}{\\Omega}$ the entropy of the system is: \n",
    "\n",
    "$$S= -\\sum \\frac{1}{\\Omega} log \\frac{1}{\\Omega} = -\\Omega \\cdot  \\frac{1}{\\Omega} log \\frac{1}{\\Omega} = log\\Omega$$ \n",
    "\n",
    "- We arrive at an expression of Entropy first obtained by Boltzmann where thermal energy units (Boltzman's) constant were used. \n",
    "\n",
    ":::{admonition} **Entropy expression for equally probable microstates**\n",
    ":class: important \n",
    "\n",
    "$$S(\\Omega) = k_B log \\Omega$$\n",
    "\n",
    "- $\\Omega$ **Number of Microsates availible to the system**\n",
    "- $k_B$ Boltzmann's constant.\n",
    ":::\n",
    "\n",
    "- **Entropy for a macrostate A** which has $\\Omega(A)$ number of microstates can be written in terms of macrostate probability \n",
    "\n",
    "$$P(A) = \\frac{\\Omega(A)}{\\Omega}$$\n",
    "\n",
    "$$S(A) = log \\Omega(A) = log P(A) + const$$\n",
    "\n",
    "- Entropy of a macrostate **quantifies how probable that macrostate is!**. This is yet another manifestation of Large Deviation Theorem we encountered before\n",
    "\n",
    "$$P(A) \\sim e^{S(A)}$$\n",
    "\n",
    "::: {admonition} **Flashback to Random Walk, Binomial, and Large Deviation Theorem**  \n",
    ":class: tip, dropdown  \n",
    "\n",
    "- [Where have we seen the entropy expression before?](https://dpotoyan.github.io/Statmech4ChemBio/1_stats/Probabilities_Counting.html#large-deviation-theory)\n",
    "- When we took the **log of the binomial distribution**! But why did we call it entropy?  \n",
    "\n",
    "$$\n",
    "S(n) = \\log \\frac{N!}{n! (N-n)!} = N \\left[ - f \\log f - (1-f) \\log (1-f) \\right] = N s(f)\n",
    "$$\n",
    "\n",
    "- Here, $ f = n/N $ represents the **fraction (empirical probability) of steps to the right** in a random walk.  \n",
    "- $ s(f) = S/N $ is the **entropy per particle (or per step)**, while $ S $ is the **total entropy**.  \n",
    "- **Different macrostates have different entropy, depending on the number of microstates they contain!**  \n",
    "\n",
    "  $$\n",
    "  P(n) = \\frac{\\Omega(n)}{\\Omega_{\\text{total}}} =  \\frac{N!}{n! (N-n)!} \\cdot \\frac{1}{2^N}\n",
    "  $$\n",
    "\n",
    "- where $ \\Omega_{\\text{total}} = 2^N $ is the **total number of microstates**.  \n",
    "- Once again, we can think of **the entropy of a macrostate as being related to its probability:**  \n",
    "\n",
    "  $$\n",
    "  S(f) \\sim \\log P(f)\n",
    "  $$\n",
    "\n",
    "**Connection to the Large Deviation Theorem**  \n",
    "When we express probability in terms of entropy, we recover the **Large Deviation Theorem**, which states that **fluctuations from the most likely macrostates are exponentially suppressed**:\n",
    "\n",
    "$$\n",
    "P(f) \\sim e^{N s(f)}\n",
    "$$\n",
    "\n",
    "This result highlights how entropy naturally governs the likelihood of macrostates in statistical mechanics.  \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretations of Entropy  \n",
    "\n",
    "#### 1. Entropy as a Measure of Information \n",
    "- Entropy quantifies the amount of information needed to specify the exact microstate of a system.  \n",
    "- This can be understood as the number of yes/no (binary) questions required to identify a specific microstate.  \n",
    "- Examples:  \n",
    "  - Determining the exact trajectory of an $N$-step random walk.  \n",
    "  - Identifying the detailed molecular distribution of gas particles in a container.  \n",
    "\n",
    "#### 2. Entropy as a Measure of Diversity and Uncertainty  \n",
    "- Entropy reflects the number of accessible microstates in a given macrostate.  \n",
    "- A more **uniform (flat) probability distribution** of microstates corresponds to **higher entropy**, whereas a **more concentrated (narrow) distribution** leads to **lower entropy**.  \n",
    "- Higher entropy implies greater uncertainty:  \n",
    "  - In a high-entropy system, identifying the exact microstate is much harder.  \n",
    "  - In contrast, a low-entropy system is more predictable, as fewer microstates are accessible.  \n",
    "- When all microstates are equally probable, entropy simplifies to the logarithm of the number of microstates, $S = k_B \\log \\Omega$.  \n",
    "- A system will naturally tend to evolve toward macrostates with higher entropy because they correspond to a larger number of available microstates, making them statistically more likely.  \n",
    "\n",
    "#### 3. Physical and Thermodynamic Implications  \n",
    "- Systems with high entropy have a vast number of possible microstates, meaning more \"work\" is needed—in an informational sense—to pinpoint a specific one.  \n",
    "- **Reducing entropy requires physical work:**  \n",
    "  - Example: To reduce the number of yes/no questions needed to specify the position of gas molecules, one must compress the gas, which requires energy.  \n",
    "- **Spontaneous and irreversible processes tend to increase entropy:**  \n",
    "  - Entropy naturally increases in isolated systems because evolution toward more probable (higher entropy) macrostates is statistically favored.  \n",
    "  - This principle underlies the **Second Law of Thermodynamics**, which states that entropy never decreases in a closed system.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is Information Physical?\n",
    "\n",
    ":::{figure-md} markdown-fig  \n",
    "\n",
    "<img src=\"./figs/max-dem.png\" alt=\"diffflux\" style=\"width:35%\">\n",
    "\n",
    "Maxwell’s demon controlling the door that allows the passage of single molecules from one side to the other. The initial hot gas gets hotter at the end of the process while the cold gas gets colder.\n",
    ":::  \n",
    "\n",
    "- **Wheeler's \"It from Bit\":**  \n",
    "  - Every \"it\" — every particle, every field, every force, and even the fabric of space-time — derives its function, meaning, and very existence from binary answers to yes-or-no questions. In essence, Wheeler's idea of \"It from Bit\" posits that at the most fundamental level, the physical universe is rooted in information. This perspective implies that all aspects of reality are ultimately information-theoretic in origin.\n",
    "\n",
    "- **Maxwell's Demon:**  \n",
    "  - Maxwell's Demon is a thought experiment that challenges the second law of thermodynamics by envisioning a tiny being capable of sorting molecules based on their speeds. By selectively allowing faster or slower molecules to pass through a gate, the demon appears to reduce entropy without expending energy. However, the act of gathering and processing information incurs a thermodynamic cost, ensuring that the overall entropy balance is maintained. This paradox underscores that information is a physical quantity with measurable effects on energy and entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy (MaxEnt) Principle\n",
    "\n",
    "- Probability represents our incomplete information. Given partial knowledge abotu some variables how should we construct a probability distribution that is unbiased beyond what we know?  \n",
    "- The **Maximum Entropy (MaxEnt) Principle** provides the best approach: we choose probabilities $p_k$ to **maximize Shannon entropy** while satisfying given constraints.  \n",
    "- This ensures the least biased distribution possible, consistent with the available information.  \n",
    "- In MaxEnt, entropy is maximized **relative to an underlying assumption of equal-probability microstates** (i.e., maximum ignorance in the absence of constraints).  \n",
    "- Below we show that that the **MaxEnt** leads to **exponential distributions**, we proceed by maximizing the **entropy functional** subject to given constraints.\n",
    "\n",
    "$$\n",
    "S = - \\sum_k p_k \\log p_k.\n",
    "$$\n",
    "\n",
    "- We seek to maximize $ S(p) $ subject to the constraints:\n",
    "\n",
    "$$\n",
    "\\sum_k p_k = 1, \\quad \\sum_k p_k \\, x_k^a = \\langle x^a \\rangle, \\quad \\sum_k p_k \\, x_k^b = \\langle x^b \\rangle, \\quad \\text{etc.}\n",
    "$$\n",
    "\n",
    "- To enforce these constraints, we introduce **Lagrange multipliers** $ \\lambda_0, \\lambda_1, \\lambda_2, \\dots $, leading to the **Lagrangian** (also called the objective function):\n",
    "\n",
    "$$\n",
    "J[p] = S(p) - \\lambda_0 \\left( \\sum_k p_k - 1 \\right) - \\lambda_1 \\left( \\sum_k p_k \\, x_k^a - \\langle x^a \\rangle \\right) - \\lambda_2 \\left( \\sum_k p_k \\, x_k^b - \\langle x^b \\rangle \\right) - \\dots\n",
    "$$\n",
    "\n",
    "\n",
    "- To maximize $ J[p] $, we take its functional derivative with respect to $ p_k $:\n",
    "\n",
    "$$\n",
    "\\frac{\\delta J}{\\delta p_k} = - (1 + \\log p_k) - \\lambda_0 - \\lambda_1 x_k^a - \\lambda_2 x_k^b - \\dots = 0.\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\log p_k = - 1 - \\lambda_0 - \\lambda_1 x_k^a - \\lambda_2 x_k^b - \\dots\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_k = e^{-1 - \\lambda_0} e^{- \\lambda_1 x_k^a} e^{- \\lambda_2 x_k^b} \\dots\n",
    "$$\n",
    "\n",
    "- Since $ e^{-1 - \\lambda_0} $ is simply a normalization factor, we define:\n",
    "\n",
    "$$\n",
    "Z = e^{1+\\lambda_0},\n",
    "$$\n",
    "\n",
    "- so that $ \\log Z = 1 + \\lambda_0 $, giving:\n",
    "\n",
    "$$\n",
    "p_k = \\frac{1}{Z} e^{- \\lambda_1 x_k^a} e^{- \\lambda_2 x_k^b} \\dots\n",
    "$$\n",
    "\n",
    "- **Interperation of MaxEnt**\n",
    "    - The probability distribution takes an **exponential form** in the constrained variables $ x_k^a, x_k^b, \\dots $.\n",
    "    - The normalization constant $ Z $ (also called the **partition function**) ensures that the probabilities sum to 1.\n",
    "    - The Lagrange multipliers $ \\lambda_1, \\lambda_2, \\dots $ encode the specific constraints imposed on the system.\n",
    "    - This result is fundamental in **statistical mechanics**, where it leads to **Boltzmann distributions**, and in **machine learning**, where it underpins **maximum entropy models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition}  **MaxEnt: Maximize Entropy Subject to Constraints**  \n",
    ":class: important  \n",
    "\n",
    "- **Step 1: Construct the Entropy Functional with Constraints**  \n",
    "\n",
    "  $$\n",
    "  J[p] = - \\sum_k p_k \\log p_k - \\lambda_0 \\left( \\sum_k p_k - 1 \\right) - \\sum_{i=1} \\lambda_i \\left( \\sum_k p_k x_k^i - \\langle x^i \\rangle \\right).\n",
    "  $$\n",
    "\n",
    "- **Step 2: Maximize $J[p] $ by Setting $\\delta J[p] = 0 $ to Derive the Distribution**  \n",
    "\n",
    "  $$\n",
    "  p_k = \\frac{1}{Z} e^{- \\sum_i \\lambda_i x_k^i}, \\quad Z = \\sum_k e^{- \\sum_i \\lambda_i x_k^i}.\n",
    "  $$\n",
    "\n",
    "- **Step 3: Solve for $\\lambda_i $ to Satisfy the Constraints**  \n",
    "\n",
    "  $$\n",
    "  \\sum_k p_k \\, x_k^i = \\langle x^i \\rangle.\n",
    "  $$  \n",
    "  \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application MaxEnt: Biased Die Example\n",
    "\n",
    "- If we are given a fair die MaxENt would predict $p_i=1/6$ as there are no constraints. \n",
    "- But suppose we are given a biased die the average outcome of which is rolling on average a number $ \\langle x \\rangle = 5.5 $. The entropy function to maximize becomes:\n",
    "\n",
    "$$ J[p_1, p_2, ..] = - \\sum p_i \\log p_i - \\lambda \\left( \\sum_i p_i - 1 \\right) - B \\left( \\sum_i p_i x_i - 5.5 \\right). $$\n",
    "\n",
    "- Solving the variational equation, we find that the optimal probability distribution follows an exponential form:\n",
    "\n",
    "$$ p_i = \\frac{e^{- B x_i}}{Z}, $$\n",
    "\n",
    "- where $ Z $ is the partition function ensuring normalization:\n",
    "\n",
    "$$ Z = \\sum_{i=1}^{6} e^{- B x_i}. $$\n",
    "\n",
    "- To determine $ B $, we use the constraint $ \\langle x \\rangle = 5.5 $:\n",
    "\n",
    "$$ \\sum_{i=1}^{6} x_i \\frac{e^{- B x_i}}{Z} = 5.5. $$\n",
    "\n",
    "- This equation can be solved numerically for $ B $. In many cases, Newton's method or other root-finding techniques can be employed to find the exact value of $ B $. This distribution resembles the Boltzmann factor in statistical mechanics, where higher outcomes are exponentially less probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Newton's Method for Optimization**\n",
    ":class: tip, dropdown\n",
    "\n",
    "- Newton's method is used to find a root $ B^* $ of a function $ f(B) $, meaning we want to solve:\n",
    "\n",
    "$$\n",
    "f(B) = 0\n",
    "$$\n",
    "\n",
    "- To approximate the solution, we start with an initial guess $ B_0 $ and iteratively refine it.\n",
    "\n",
    "**Taylor Expansion of $ f(B) $**\n",
    "\n",
    "- Using a first-order Taylor expansion around $ B_n $:\n",
    "\n",
    "$$\n",
    "f(B) \\approx f(B_n) + f'(B_n)(B - B_n)\n",
    "$$\n",
    "\n",
    "- Setting $ f(B) = 0 $ to approximate the root:\n",
    "\n",
    "$$\n",
    "0 \\approx f(B_n) + f'(B_n)(B - B_n)\n",
    "$$\n",
    "\n",
    "**Solving for $ B $**\n",
    "\n",
    "- Rearranging for $ B $:\n",
    "\n",
    "$$\n",
    "B = B_n - \\frac{f(B_n)}{f'(B_n)}\n",
    "$$\n",
    "\n",
    "- This gives the iterative update rule:\n",
    "\n",
    "$$\n",
    "B_{n+1} = B_n - \\frac{f(B_n)}{f'(B_n)}\n",
    "$$\n",
    "\n",
    "**Applying to Optimization**\n",
    "\n",
    "- In optimization, we often minimize a function $ g(B) $. To find its critical points, we set its derivative $ f(B) = g'(B) $ to zero and apply Newton’s method:\n",
    "\n",
    "$$\n",
    "B_{n+1} = B_n - \\frac{g'(B_n)}{g''(B_n)}\n",
    "$$\n",
    "\n",
    "- where $ g''(B) $ is the second derivative (Hessian in multiple dimensions).\n",
    "\n",
    "- This equation is widely used in optimization, including **Maximum Entropy (MaxEnt)** problems, where we adjust $ B $ to satisfy constraints.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import newton\n",
    "\n",
    "def partition_function(B):\n",
    "    x = np.array([1, 2, 3, 4, 5, 6])\n",
    "    return np.sum(np.exp(-B * x))\n",
    "\n",
    "def expected_value(B):\n",
    "    x = np.array([1, 2, 3, 4, 5, 6])\n",
    "    Z = partition_function(B)\n",
    "    p = np.exp(-B * x) / Z\n",
    "    return np.sum(x * p)\n",
    "\n",
    "def objective_function(B):\n",
    "    return expected_value(B) - 5.5\n",
    "\n",
    "def objective_derivative(B):\n",
    "    x = np.array([1, 2, 3, 4, 5, 6])\n",
    "    Z = partition_function(B)\n",
    "    p = np.exp(-B * x) / Z\n",
    "    return -np.sum(x**2 * p) + np.sum(x * p) ** 2\n",
    "\n",
    "# Find optimal B using Newton's method\n",
    "B_opt = newton(objective_function, x0=0.1, fprime=objective_derivative)\n",
    "\n",
    "# Compute final probability distribution\n",
    "x = np.array([1, 2, 3, 4, 5, 6])\n",
    "Z = partition_function(B_opt)\n",
    "optimal_p = np.exp(-B_opt * x) / Z\n",
    "\n",
    "# Print results\n",
    "print(f\"Optimal B: {B_opt}\")\n",
    "print(\"Optimized probability distribution:\")\n",
    "for i, p in enumerate(p_opt, 1):\n",
    "    print(f\"P({i}) = {p:.4f}\")\n",
    "\n",
    "\n",
    "# Plot the resulting probability distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x, optimal_p, color='royalblue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel(\"Die Outcome\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.title(\"Optimized MaxEnt Probability Distribution for a Biased Die\", fontsize=16)\n",
    "plt.xticks(x)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Physical constriants on energy, particle number, volume**\n",
    ":class: tip, dropdown\n",
    "\n",
    "**1. Microcanonical Ensemble (Fixed Energy, Volume, and Particle Number)**\n",
    "\n",
    "- For an isolated gas with a fixed energy $ E $, volume $ V $, and particle number $ N $, we maximize entropy subject to the constraint that only microstates with energy $ E $ are accessible:\n",
    "\n",
    "$$ J = -\\sum_k p_k \\log p_k - \\lambda \\left( \\sum_k p_k - 1 \\right). $$\n",
    "\n",
    "- Solving for $ p_k $, we obtain:\n",
    "\n",
    "$$ p_k = \\frac{1}{\\Omega}, $$\n",
    "\n",
    "- where $ \\Omega $ is the number of microstates. This is the basis of classical thermodynamics, where entropy is defined as $ S = k_B \\log \\Omega $.\n",
    "\n",
    "**2. Canonical Ensemble (Fixed Temperature, Volume, and Particle Number)**\n",
    "\n",
    "- If the system is in thermal contact with a heat bath at temperature $ T $, energy is allowed to fluctuate. The constraint now involves the mean energy $ \\langle E \\rangle = U $:\n",
    "\n",
    "$$ J = -\\sum_k p_k \\log p_k - \\lambda \\left( \\sum_k p_k - 1 \\right) - \\beta \\left( \\sum_k p_k E_k - U \\right). $$\n",
    "\n",
    "- Solving, we obtain the Boltzmann distribution:\n",
    "\n",
    "$$ p_k = \\frac{e^{-\\beta E_k}}{Z}, $$\n",
    "\n",
    "- where $ \\beta = 1 / k_B T $ and $ Z = \\sum_k e^{-\\beta E_k} $ is the partition function. This distribution governs systems in thermal equilibrium.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy\n",
    "\n",
    "- We mentioned that in MaxEnt derivation we maximized entropy relative to an underlying assumption of equal-probability microstates. We make this idea precise here\n",
    "\n",
    "- Consider example of 1D brownian particle starting at $x_0 = 0$ and diffusing freely. Its probability distribution after time $t$ follows a Gaussian:  \n",
    "\n",
    "$$\n",
    "p(x, t) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{x^2}{4Dt}}\n",
    "$$\n",
    "\n",
    "- The **Shannon entropy** of this distribution is given by:\n",
    "\n",
    "$$\n",
    "S(t) = -\\int p(x, t) \\log p(x, t) \\, dx.\n",
    "$$\n",
    "\n",
    "- Evaluating the integral yields nice compact formulla showing that entropy grows with time as molecule diffuse and sparead all over the container. \n",
    "\n",
    "$$\n",
    "S(t) = \\frac{1}{2} \\log (4\\pi e Dt).\n",
    "$$\n",
    "\n",
    "**Problem: Grid Dependence of Shannon Entropy**  \n",
    "\n",
    "- All is good but if we tried to evaluate the integral we would run into serious problem! \n",
    "- Also how to interpret integral expression in terms of binary yes/no quesionts?\n",
    "- A major issue with Shannon entropy is that it **depends on the choice of units**. If we refine the grid by choosing a smaller $\\Delta x$, the computed entropy does not converge to a well-defined value—it diverges! This makes it unsuitable for studying entropy change in diffusion.  \n",
    "- To avoid this issue, one can instead use **relative entropy** (Kullback-Leibler divergence), which remains well-defined and independent of discretization.\n",
    "\n",
    "\n",
    ":::{admonition} **Relative Entropy**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P || Q) = \\sum_x P(x) \\ln \\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P || Q) = \\int P(x) \\ln \\frac{P(x)}{Q(x)} \\, dx.\n",
    "$$\n",
    "\n",
    "- $Q$ **reference probability** distribution\n",
    "- $P$ **true probability** distribution or the one we are using/observing. \n",
    "\n",
    ":::\n",
    "\n",
    "- The Kullback-Leibler (KL) divergence, or relative entropy, measures **how much information is lost when using a reference distribution Q** \n",
    "- **KL is non-negative and equals zero if and only if $P = Q $ everywhere.** \n",
    "- KL divergence is widely used in statistical mechanics, information theory, machine learning and thermodynamics as a measure of information loss when approximating one distribution with another.\n",
    "\n",
    "\n",
    ":::{admonition} **Flashback to Random Walk and LDT**  \n",
    ":class: tip, dropdown  \n",
    "\n",
    "- [We have already encountered relative entropy in the random walk problem!](https://dpotoyan.github.io/Statmech4ChemBio/1_stats/Probabilities_Counting.html#large-deviation-theory)  \n",
    "\n",
    "- There, we derived what is known as the **Large Deviation Theory (LDT)** expression, which shows that fluctuations are concentrated around the minima of $ I(f) $—a function that initially seemed mysterious but proved to be fundamental:  \n",
    "\n",
    "  $$\n",
    "  P_N (f) \\sim e^{-N I(f)}.\n",
    "  $$\n",
    "\n",
    "- Now, we recognize that this function is actually the **relative entropy** between the empirical and true probability distributions as $ N $ increases:  \n",
    "\n",
    "  $$\n",
    "  I(f) = f_+ \\log \\frac{f_+}{p_+} + f_- \\log \\frac{f_-}{p_-} = D(f || p)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  P_N (f) \\sim e^{-N D(f || p)}\n",
    "  $$\n",
    "\n",
    "- Thus, we see that **relative entropy quantifies how unlikely it is to observe an empirical fraction $ f $ deviating from the true probability $ p $**. The larger the relative entropy $ D(f || p) $, the less likely it is to observe such a deviation!  \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assymetry of KL and irreversibility\n",
    "\n",
    "- Given two normal distributions:\n",
    "\n",
    "$$\n",
    "P(x) = \\mathcal{N}(\\mu_1, \\sigma_1^2), \\quad Q(x) = \\mathcal{N}(\\mu_2, \\sigma_2^2),\n",
    "$$\n",
    "\n",
    "- We can compute KL divergence analytically showing that the function is assymteric\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P || Q) = \\ln \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    ":::{admonition} **KL Divergence between two Gaussians**\n",
    ":class: tip, dropdown\n",
    "\n",
    "### **Derivation of KL Divergence Between Two Gaussians**  \n",
    "\n",
    "Given two normal distributions:\n",
    "\n",
    "$$\n",
    "P(x) = \\mathcal{N}(\\mu_1, \\sigma_1^2), \\quad Q(x) = \\mathcal{N}(\\mu_2, \\sigma_2^2),\n",
    "$$\n",
    "\n",
    "the **Kullback-Leibler (KL) divergence** is defined as:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\ln \\frac{p(x)}{q(x)} \\,dx.\n",
    "$$\n",
    "\n",
    "**Step 1: Write Out the Gaussian PDFs**\n",
    "The probability density functions of the two Gaussians are:\n",
    "\n",
    "$$\n",
    "P(x) = \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}} e^{-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(x) = \\frac{1}{\\sqrt{2\\pi \\sigma_2^2}} e^{-\\frac{(x - \\mu_2)^2}{2\\sigma_2^2}}.\n",
    "$$\n",
    "\n",
    "Taking their ratio:\n",
    "\n",
    "$$\n",
    "\\frac{P(x)}{Q(x)} = \\frac{\\sigma_2}{\\sigma_1} \\exp \\left[ \\frac{(x - \\mu_2)^2}{2\\sigma_2^2} - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2} \\right].\n",
    "$$\n",
    "\n",
    "Taking the natural logarithm:\n",
    "\n",
    "$$\n",
    "\\ln \\frac{P(x)}{Q(x)} = \\ln \\frac{\\sigma_2}{\\sigma_1} + \\frac{(x - \\mu_2)^2}{2\\sigma_2^2} - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2}.\n",
    "$$\n",
    "\n",
    "**Step 2: Compute the Expectation $ \\mathbb{E}_P[\\ln P(x)/Q(x)] $**\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P || Q) = \\mathbb{E}_P \\left[ \\ln \\frac{P(x)}{Q(x)} \\right] = \\int P(x) \\ln \\frac{P(x)}{Q(x)} \\,dx.\n",
    "$$\n",
    "\n",
    "Since expectation under $ P(x) $ means integrating with $ P(x) $, we evaluate the three terms separately.\n",
    "\n",
    "1. **First term:**  \n",
    "   $$\n",
    "   \\mathbb{E}_P \\left[ \\ln \\frac{\\sigma_2}{\\sigma_1} \\right] = \\ln \\frac{\\sigma_2}{\\sigma_1}\n",
    "   $$\n",
    "   since it is a constant.\n",
    "\n",
    "2. **Second term:**  \n",
    "   Using the property of a Gaussian expectation $ \\mathbb{E}_P [(x - \\mu_1)^2] = \\sigma_1^2 $,\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}_P \\left[ \\frac{(x - \\mu_1)^2}{2\\sigma_1^2} \\right] = \\frac{1}{2}.\n",
    "   $$\n",
    "\n",
    "3. **Third term:**  \n",
    "   Expanding $ (x - \\mu_2)^2 $,\n",
    "\n",
    "   $$\n",
    "   (x - \\mu_2)^2 = (x - \\mu_1 + \\mu_1 - \\mu_2)^2.\n",
    "   $$\n",
    "\n",
    "   Taking expectation under $ P(x) $,\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}_P \\left[ \\frac{(x - \\mu_2)^2}{2\\sigma_2^2} \\right] = \\frac{1}{2\\sigma_2^2} \\left( \\sigma_1^2 + (\\mu_1 - \\mu_2)^2 \\right).\n",
    "   $$\n",
    "\n",
    " **Step 3: Final KL Divergence Formula**\n",
    "\n",
    "Combining all terms, we get:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P || Q) = \\ln \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}.\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "- Computing **KL divergence** between two gaussians describing diffusion at times $ t_1 $ and $ t_2 $, where their variances are $ \\sigma_1^2 = 2D t_1 $ and $ \\sigma_2^2 = 2D t_2 $ results in: \n",
    "\n",
    "$$\n",
    "D(p_1 \\| p_2) = \\frac{1}{2} \\left( \\frac{t_1}{t_2} - 1 + \\log \\frac{t_2}{t_1} \\right)\n",
    "$$\n",
    "\n",
    "- For a **diffusion process**, if we compare the **forward evolution**of diffusion where variance is spreading over time $t_1=t$ with the hypothetical **reversed process** where guassian contraints into a peak over time $t_2=T-t$ we can see that:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P_{\\text{forward}} || P_{\\text{backward}}) > 0.\n",
    "$$\n",
    "\n",
    "- This indicates that diffusion is an **irreversible** process in the absence of external driving forces (since it tends to increase entropy). In contrast, a time-reversed diffusion process (all particles contracting back into the initial state) would violate the second law of thermodynamics.\n",
    "\n",
    "- This statistical distinguishability of time-forward and time-reversed processes is often referred to the **“\"thermodynamic arrow of time”** showing that the forward flow of events is distinguishable from its reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time points\n",
    "t_forward = np.linspace(0.1, 2, 100)\n",
    "t_reverse = np.linspace(2, 0.1, 100)\n",
    "\n",
    "# Define probability distributions for forward and reverse processes\n",
    "x = np.linspace(-3, 3, 100)\n",
    "sigma_forward = np.sqrt(t_forward[:, None])  # Diffusion spreads over time\n",
    "sigma_reverse = np.sqrt(t_reverse[:, None])  # Reverse \"contracts\" over time\n",
    "\n",
    "P_forward = np.exp(-x**2 / (2 * sigma_forward**2)) / (np.sqrt(2 * np.pi) * sigma_forward)\n",
    "P_reverse = np.exp(-x**2 / (2 * sigma_reverse**2)) / (np.sqrt(2 * np.pi) * sigma_reverse)\n",
    "\n",
    "# Compute Kullback-Leibler divergence D_KL(P_forward || P_reverse)\n",
    "D_KL = np.sum(P_forward * np.log(P_forward / P_reverse), axis=1)\n",
    "\n",
    "# Plot the distributions and KL divergence\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot forward and reverse distributions\n",
    "ax[0].imshow(P_forward.T, extent=[t_forward.min(), t_forward.max(), x.min(), x.max()], aspect='auto', origin='lower', cmap='Blues', alpha=0.7, label='P_forward')\n",
    "ax[0].imshow(P_reverse.T, extent=[t_reverse.min(), t_reverse.max(), x.min(), x.max()], aspect='auto', origin='lower', cmap='Reds', alpha=0.5, label='P_reverse')\n",
    "\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[0].set_title(\"Forward (Blue) vs Reverse (Red) Diffusion\")\n",
    "ax[0].arrow(0.2, 2.5, 1.5, 0, head_width=0.3, head_length=0.2, fc='black', ec='black')  # Forward arrow\n",
    "ax[0].arrow(1.8, -2.5, -1.5, 0, head_width=0.3, head_length=0.2, fc='black', ec='black')  # Reverse arrow\n",
    "\n",
    "# Plot KL divergence over time\n",
    "ax[1].plot(t_forward, D_KL, label=r'$D_{KL}(P_{\\mathrm{forward}} || P_{\\mathrm{reverse}})$', color='black')\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[1].set_ylabel(\"KL Divergence\")\n",
    "ax[1].set_title(\"Arrow of Time and Irreversibility\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative Entropy in Machine learning\n",
    "\n",
    "1. If $ Q(x) $ **assigns very low probability** to a region where $ P(x) $ is high, the term $ \\log \\frac{P(x)}{Q(x)} $ becomes large, **strongly penalizing $ Q $ for underestimating $ P $**.  \n",
    "2. If $ Q(x) $ **is broader than $ P(x) $, assigning extra probability mass to unlikely regions**, this does not significantly affect $ D_{\\text{KL}}(P || Q) $, because $ P(x) $ is small in those regions.  \n",
    "\n",
    "- This asymmetry explains why **KL divergence is not a true distance metric**. It penalizes **underestimation** of true probability mass much more than **overestimation**, making it particularly useful in **machine learning** where models are trained to avoid assigning near-zero probabilities to observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_gaussians(mu1=0, sigma1=1, mu2=1, sigma2=2):\n",
    "    x_values = np.linspace(-5, 5, 1000)  # Define spatial grid\n",
    "    P = norm.pdf(x_values, loc=mu1, scale=sigma1)  # First Gaussian\n",
    "    Q = norm.pdf(x_values, loc=mu2, scale=sigma2)  # Second Gaussian\n",
    "    \n",
    "    # Avoid division by zero in KL computation\n",
    "    mask = (P > 0) & (Q > 0)\n",
    "    D_KL_PQ = np.trapz(P[mask] * np.log(P[mask] / Q[mask]), x_values[mask])  # D_KL(P || Q)\n",
    "    D_KL_QP = np.trapz(Q[mask] * np.log(Q[mask] / P[mask]), x_values[mask])  # D_KL(Q || P)\n",
    "    \n",
    "    # Plot the distributions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x_values, P, label=fr'$P(x) \\sim \\mathcal{{N}}({mu1},{sigma1**2})$', linewidth=2)\n",
    "    plt.plot(x_values, Q, label=fr'$Q(x) \\sim \\mathcal{{N}}({mu2},{sigma2**2})$', linewidth=2, linestyle='dashed')\n",
    "    plt.fill_between(x_values, P, Q, color='gray', alpha=0.3, label=r'Difference between $P$ and $Q$')\n",
    "    \n",
    "    # Annotate KL divergences\n",
    "    plt.text(-4, 0.15, rf'$D_{{KL}}(P || Q) = {D_KL_PQ:.3f}$', fontsize=12, color='blue')\n",
    "    plt.text(-4, 0.12, rf'$D_{{KL}}(Q || P) = {D_KL_QP:.3f}$', fontsize=12, color='red')\n",
    "    \n",
    "    # Labels and legend\n",
    "    plt.xlabel('$x$', fontsize=14)\n",
    "    plt.ylabel('Probability Density', fontsize=14)\n",
    "    plt.title('Interactive KL Divergence Between Two Gaussians', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_gaussians, \n",
    "         mu1=widgets.FloatSlider(min=-3, max=3, step=0.1, value=0, description='μ1'),\n",
    "         sigma1=widgets.FloatSlider(min=0.1, max=3, step=0.1, value=1, description='σ1'),\n",
    "         mu2=widgets.FloatSlider(min=-3, max=3, step=0.1, value=1, description='μ2'),\n",
    "         sigma2=widgets.FloatSlider(min=0.1, max=3, step=0.1, value=2, description='σ2'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "- Compute entropy of gaussian distribution. Plot entropy as a function of variance\n",
    "\n",
    "- Using MaxEnt approach find probability distribution with mean and variance equal to $\\mu$ and $\\sigma^2$ respectively. \n",
    "\n",
    "- Simulate 1D random walk and compute entropy by first computing probability distribution $p_N(n)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
