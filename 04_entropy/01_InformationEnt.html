
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. What is Information? &#8212; Statistical Mechanics for Chemistry and Biology</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="F. Phase transitions and Ising models" href="../05_ising.html" />
    <link rel="prev" title="E. Entropy and Information" href="../04_entropy.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.jpg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Statistical Mechanics for Chemistry and Biology</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../00_python.html">
   A. Python refresher
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../00_python/intro2py.html">
     1. Why Python?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00_python/intro2numpy.html">
     2. NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00_python/intro2viz.html">
     3. Plotting with Matplotlib
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../01_stats.html">
   B. Probability and randomness
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../01_stats/01_SetsCounting.html">
     1. Probability by counting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01_stats/02_RandVar.html">
     2. Random variable, stochastic process and simulation.
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01_stats/04_MC.html">
     3. “Monte Carlo and the power of randomness”
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01_stats/05_InverseTransform.html">
     4. The Inverse Transform of RVs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../02_thermo.html">
   C. Thermodynamics refresher
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../02_thermo/01_Thermo.html">
     1. Review of the main principles of thermodynamics.
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../03_ensembles.html">
   D. Statistical ensembles
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../03_ensembles/01_NVE.html">
     1. NVE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03_ensembles/02_NVT.html">
     2. NVT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03_ensembles/03_NPT.html">
     3. NPT
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../04_entropy.html">
   E. Entropy and Information
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     1. What is Information?
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../05_ising.html">
   F. Phase transitions and Ising models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../05_ising/01_MCMC_Ising.html">
     1. Ising models and Metropolis-Hastings algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05_ising/02_ising_analytic.html">
     2. Phase transitions through the lense of Ising models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05_ising/02_ising_analytic.html#a-bit-of-history-ising-model-and-phase-transitions">
     3. A. bit of history: Ising model and phase transitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05_ising/02_ising_analytic.html#homework-7">
     4. Homework-7
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05_ising/03_EnhanceSampl.html">
     5. Non-boltzman (enhanced) sampling ideas
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/04_entropy/01_InformationEnt.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/DPotoyan/Statmech4ChemBio/master?urlpath=lab/tree/04_entropy/01_InformationEnt.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/DPotoyan/Statmech4ChemBio/blob/master/04_entropy/01_InformationEnt.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-candidate-function-for-information">
   1.1. A candidate function for information!
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-bit-base-two">
   1.2. Why bit (base two)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-many-yes-or-no-on-average-to-reconstruct-random-walk">
   1.3. How many yes or no on average to reconstruct random walk?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#back-to-the-roots-of-infromation-theory-decoding-secret-messages">
   1.4. Back to the roots of Infromation theory: decoding (secret) messages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#alphabets-are-not-random-hghjxcjxcc">
   1.5. Alphabets are not random! hghjxcjxcc
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#shanon-measure-of-information">
   1.6. Shanon Measure of Information
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#information-per-letter-of-english">
   1.7. Information per Letter of English
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#game-of-message-decoding">
   1.8. Game of message decoding:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantifying-the-knowledge-through-shannon-measure-of-information">
   1.9. Quantifying the knowledge ( through Shannon measure of information)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#two-kittens">
   1.10. Two kittens
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#monty-hall-problem">
   1.11. Monty Hall problem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-integer-number-of-yes-no-questions">
   1.12. Non integer number of YES/NO questions??
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-entropy">
   1.13. What is Entropy?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#back-to-statistical-mechanics-and-thermodynamics">
   1.14. Back to statistical mechanics and thermodynamics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#thermodynamic-unit-of-information">
     1.14.1. Thermodynamic unit of information
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#is-information-physical">
     1.14.2. Is information physical?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relation-between-heat-and-information-entropy">
     1.14.3. Relation between Heat and information (Entropy)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jayne-s-maxent-maximum-entropy-principle">
     1.14.4. Jayne’s MaxEnt (Maximum Entropy principle)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#large-deviation-theory-and-sanov-s-theorem">
     1.14.5. Large deviation Theory and Sanov’s theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#increasing-entropy-surprising-or-not">
     1.14.6. Increasing Entropy: surprising or not?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-maxwell-s-demon">
   1.15. What is Maxwell’s demon?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#playing-maxwell-s-demon-s-advocate">
   1.16. Playing Maxwell’s demon’s advocate
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#information-erasure-and-landauer-s-principle">
   1.17. Information erasure and Landauer’s principle
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#szilard-s-engine">
     1.17.1. Szilard’s Engine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-entropy">
     1.17.2. Conditional Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mutual-infromation">
     1.17.3. Mutual Infromation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#additional-references">
       1.17.3.1. Additional References
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="what-is-information">
<h1><span class="section-number">1. </span>What is Information?<a class="headerlink" href="#what-is-information" title="Permalink to this headline">¶</a></h1>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Entropy_flip_2_coins.jpg" alt="Information" style="width:50%"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">widgets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sci</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Which of these two statements conveys the most information?</strong></p>
<ul class="simple">
<li><p>I will eat some food tomorrow.</p></li>
<li><p>I will see a giraffe walking by my apartment.</p></li>
</ul>
<p><strong>A measure of information (whatever it may be) is closely related to the element of… surprise!</strong></p>
<ul class="simple">
<li><p>has very high probability and so conveys little information,</p></li>
<li><p>has very low probability and so conveys much information.</p></li>
</ul>
<blockquote>
<div><p>If we quanitfy suprise we will quantify information</p>
</div></blockquote>
<p><strong>Playing cards</strong></p>
<p>Which is more surprising (contains more information)?</p>
<ul class="simple">
<li><p>E1: The card is heart?</p></li>
<li><p>E2:The card is Queen?</p></li>
<li><p>E3: The card is Queen of hearts?</p></li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(E_1) = \frac{1}{4}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(E_2)  =  \frac{4}{52} = \frac{1}{13}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(E_1 and E_2) = \frac{1}{52}\)</span></p></li>
</ul>
<p><strong>Gaining information</strong></p>
<ol class="simple">
<li><p>We learn the card is heart <span class="math notranslate nohighlight">\(I(E_1)\)</span></p></li>
<li><p>We learn the card is Queen <span class="math notranslate nohighlight">\(I(E_2)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(I(E_1 and E_2) = I(E_1) + I(E_2)\)</span></p></li>
</ol>
<blockquote>
<div><ol class="simple">
<li><p>Knowledge of event can add to information but neer take away from it: <span class="math notranslate nohighlight">\(I(E) \geq 0\)</span></p></li>
</ol>
</div></blockquote>
<div class="section" id="a-candidate-function-for-information">
<h2><span class="section-number">1.1. </span>A candidate function for information!<a class="headerlink" href="#a-candidate-function-for-information" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[I(E) = - C' log_D p(E)\]</div>
<div class="math notranslate nohighlight">
\[I(E) = - C log_2 p(E) \]</div>
<blockquote>
<div><p>Constant <span class="math notranslate nohighlight">\(C', D\)</span> can be combined into one $<span class="math notranslate nohighlight">\(log_a y = \frac{log_b y}{log_b a}\)</span>$ if we agree on log base</p>
</div></blockquote>
</div>
<div class="section" id="why-bit-base-two">
<h2><span class="section-number">1.2. </span>Why bit (base two)<a class="headerlink" href="#why-bit-base-two" title="Permalink to this headline">¶</a></h2>
<p>Consider symmetric Bernouli process, e.g 1D random walk with equal jump probabilities:</p>
<div class="math notranslate nohighlight">
\[I(X=0) = I(X=1) = -log_2 \frac{1}{2} = 1\]</div>
<div class="math notranslate nohighlight">
\[(x_0,x_1,...x_N) = 10111101001010100100\]</div>
<blockquote>
<div><p><strong>Random walk = string of Yes/No questions</strong></p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">I_bern</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">p</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">I_bern</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;I(p)&#39;</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.7.9/x64/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log
  
/opt/hostedtoolcache/Python/3.7.9/x64/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in multiply
  
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;p&#39;)
</pre></div>
</div>
<img alt="../_images/01_InformationEnt_10_2.png" src="../_images/01_InformationEnt_10_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1">#trials</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c1"># probability of sucess</span>

<span class="n">W_bin</span> <span class="o">=</span> <span class="p">[</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)]</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">W_bin</span><span class="p">),</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7fa405c40e50&gt;]
</pre></div>
</div>
<img alt="../_images/01_InformationEnt_11_1.png" src="../_images/01_InformationEnt_11_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">entropy</span>

<span class="n">die</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">die</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="n">die</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">die</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span>
<span class="nb">print</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">die</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="n">die</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">die</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">/</span><span class="mi">3</span>
<span class="nb">print</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">die</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.584962500721156
2.2516291673878226
1.6644977792004614
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="how-many-yes-or-no-on-average-to-reconstruct-random-walk">
<h2><span class="section-number">1.3. </span>How many yes or no on average to reconstruct random walk?<a class="headerlink" href="#how-many-yes-or-no-on-average-to-reconstruct-random-walk" title="Permalink to this headline">¶</a></h2>
<p><strong>Answer</strong>
<br>
$<span class="math notranslate nohighlight">\(H(X) = \langle -log p(X) \rangle\)</span>$</p>
<p><strong>Shannon  Measure of Information (SMI)</strong>
$<span class="math notranslate nohighlight">\(H = -\sum_i p_i log p_i\)</span>$</p>
<p><strong>Surprise</strong></p>
<div class="math notranslate nohighlight">
\[I_i = -log p_i\]</div>
<blockquote>
<div><p>Information is an average of surprise.!!! How surprised are you on average?</p>
</div></blockquote>
</div>
<div class="section" id="back-to-the-roots-of-infromation-theory-decoding-secret-messages">
<h2><span class="section-number">1.4. </span>Back to the roots of Infromation theory: decoding (secret) messages<a class="headerlink" href="#back-to-the-roots-of-infromation-theory-decoding-secret-messages" title="Permalink to this headline">¶</a></h2>
<p>Information per letter <span class="math notranslate nohighlight">\(I(m)\)</span> to decode the message</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(m:\)</span> Letters in the alphabet (Russian: 33,Enlgish: 26,  Korean: 24)</p></li>
<li><p><span class="math notranslate nohighlight">\(I(Russian) &gt; I(English) &gt; I(Korean)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(I(m_1, m_2) = I(m_1) + I(m_2)\)</span> reagrdless of the order letters are sent!</p></li>
</ul>
<blockquote>
<div><p>One bit is an amount of information one can obtain from the answer to a single yes–no question. The number of bits to decode a message grows witht the lengt of an alphabet and length of the word.</p>
</div></blockquote>
</div>
<div class="section" id="alphabets-are-not-random-hghjxcjxcc">
<h2><span class="section-number">1.5. </span>Alphabets are not random! hghjxcjxcc<a class="headerlink" href="#alphabets-are-not-random-hghjxcjxcc" title="Permalink to this headline">¶</a></h2>
<p>Some letters happen more often than the others! Probability of each letter in an independent sequence is <span class="math notranslate nohighlight">\(p_m = \frac{1}{m}\)</span></p>
<div class="math notranslate nohighlight">
\[\boxed{H(p) = - \sum_m p_m log_m p_m}\]</div>
<blockquote>
<div><p>We must send a message explaining how to combine the transferred symbols as a part of the message, but the length of the needed message is finite and independent of the length of the actual message we wish to send, so in the long message limit we may ignore this overhead.</p>
</div></blockquote>
</div>
<div class="section" id="shanon-measure-of-information">
<h2><span class="section-number">1.6. </span>Shanon Measure of Information<a class="headerlink" href="#shanon-measure-of-information" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[\boxed{H = -\sum_i p_i log p_i}\]</div>
<blockquote>
<div><p><strong>[To Shanon], You should call it Entropy, for two reasons.
In the first place you uncertainty function has been used in statistical mechanics under that name.
In the second place, and more importantly, no one knows what entropy really is, so in a debate you will always have the advantage.” J von Neumann</strong></p>
</div></blockquote>
</div>
<div class="section" id="information-per-letter-of-english">
<h2><span class="section-number">1.7. </span>Information per Letter of English<a class="headerlink" href="#information-per-letter-of-english" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>If the symbols of English alphabet (+ blank) appear equally probably, what is the information carried by a single symbol? This must be <span class="math notranslate nohighlight">\(log_2(26 + 1) = 4.755\)</span> bits, but for actual English sentences, it is known to be about <strong><span class="math notranslate nohighlight">\(1.3\)</span> bits. Why?</strong></p>
</div></blockquote>
</div>
<div class="section" id="game-of-message-decoding">
<h2><span class="section-number">1.8. </span>Game of message decoding:<a class="headerlink" href="#game-of-message-decoding" title="Permalink to this headline">¶</a></h2>
<p>Given some 70 letters decode a 250 letter paragraph!</p>
<div class="math notranslate nohighlight">
\[\frac{70}{250}log_2 27 = 1.3\]</div>
<blockquote>
<div><p>Cover T. M. and King, R. C. (1978). “A convergent gambling estimate of the entropy of English” IEEE Trans. Info. Theory, 24, 413–421</p>
</div></blockquote>
</div>
<div class="section" id="quantifying-the-knowledge-through-shannon-measure-of-information">
<h2><span class="section-number">1.9. </span>Quantifying the knowledge ( through Shannon measure of information)<a class="headerlink" href="#quantifying-the-knowledge-through-shannon-measure-of-information" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>How much knowledge we need to find out outcome of fair dice?</p></li>
<li><p>We are told die shows a digit higher than 2 (3, 4,5 or6). How much knowledge does this information carry?</p></li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H(E_1) = log_2 6\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H(E_1) - H(E_2) = log_2 6 - log_2 4\)</span></p></li>
</ul>
</div>
<div class="section" id="two-kittens">
<h2><span class="section-number">1.10. </span>Two kittens<a class="headerlink" href="#two-kittens" title="Permalink to this headline">¶</a></h2>
<p>There are two kittens. We are told that at least one of them is a male. What is the information we get from this message?</p>
<div class="math notranslate nohighlight">
\[E_1 = \{mm,mf,fm, ff \} \]</div>
<div class="math notranslate nohighlight">
\[E_2 = \{mm,mf,fm\}\]</div>
<div class="math notranslate nohighlight">
\[H(E_1) -H(E_2) = log_2 4 -log_2 3 = 0.41\]</div>
</div>
<div class="section" id="monty-hall-problem">
<h2><span class="section-number">1.11. </span>Monty Hall problem<a class="headerlink" href="#monty-hall-problem" title="Permalink to this headline">¶</a></h2>
<p>There are five boxes, of which one contains a prize. A game participant is asked to choose one box. After they choose one of the five boxes, the “coordinator” of the game identifies as empty three of the four unchosen boxes. What is the information of this message?</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H(E_1) = log_2 5 = 2.322\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H(E_2) = -\frac{1}{5} log_2 5 - \frac{4}{5} log_2 \frac{4}{5} = 0.722\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H(E_1)-H(E_2) = 1.6\)</span></p></li>
</ul>
</div>
<div class="section" id="non-integer-number-of-yes-no-questions">
<h2><span class="section-number">1.12. </span>Non integer number of YES/NO questions??<a class="headerlink" href="#non-integer-number-of-yes-no-questions" title="Permalink to this headline">¶</a></h2>
<p>We have encountered a fraction of bit of information several times now. What does it imply in terms of number of YES/NO questions. That is becasue in some cases single YES/NO question can rule out more than one elementary event.</p>
<blockquote>
<div><p>999 blue balls and 1 red ball. how many questions we need to ask to determin the colors of all balls? <span class="math notranslate nohighlight">\(S = 9.97\)</span> bit or 0.01 bit per ball. Divide the container by 500 and 500 and ask where the red ball is? 1 questions rules out 500 balls at once.</p>
</div></blockquote>
</div>
<div class="section" id="what-is-entropy">
<h2><span class="section-number">1.13. </span>What is Entropy?<a class="headerlink" href="#what-is-entropy" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><strong>Clausius (1865)</strong> $<span class="math notranslate nohighlight">\(\boxed{dS = \frac{\delta Q}{T}}\)</span>$</p></li>
</ul>
<blockquote>
<div><p>Entropy foliates thermodynamic space into isentropic surfaces called adiabats. One can only go from one adiabat to another by exchanging heat.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>Boltzman (1872)</strong> $<span class="math notranslate nohighlight">\(\boxed{S = k_B log \Omega}\)</span>$</p></li>
</ul>
<blockquote>
<div><p>Entropy quantifies number of microstates consistent with a macrostate.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>Gibbs (1878)</strong>  $<span class="math notranslate nohighlight">\(\boxed{S = -k_B \sum_i p_i log p_i}\)</span>$</p></li>
</ul>
<blockquote>
<div><p>Entropy is related to the probability  <span class="math notranslate nohighlight">\(p_i\)</span> by which microstates are weighted in an ensemble.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>Shannon (1942)</strong> $<span class="math notranslate nohighlight">\(\boxed{H = -\sum_i p_i log p_i}\)</span>$</p></li>
</ul>
<blockquote>
<div><p>Entropy quantifies how much of knowledge (information) we need to specify a microstate of a system.</p>
</div></blockquote>
</div>
<div class="section" id="back-to-statistical-mechanics-and-thermodynamics">
<h2><span class="section-number">1.14. </span>Back to statistical mechanics and thermodynamics<a class="headerlink" href="#back-to-statistical-mechanics-and-thermodynamics" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[S = -k_B \sum_i p_i log p_i\]</div>
<ul class="simple">
<li><p>Adiaatic expasion (doubling of volume) of an ideal gas:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Delta S = N_A k_B log 2 = R log 2\]</div>
<blockquote>
<div><p>How many YES/NO questions do we need to ask to pinpoint single microsate among the ensemble of microstates of a macrostate.</p>
</div></blockquote>
<div class="section" id="thermodynamic-unit-of-information">
<h3><span class="section-number">1.14.1. </span>Thermodynamic unit of information<a class="headerlink" href="#thermodynamic-unit-of-information" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[k_B log_e W \rightarrow C log_2 W \]</div>
<p><strong>C = 1 bit/molecule = 9.57 × 10^{-24} J/(K·molecule))</strong></p>
<blockquote>
<div><p>Small number becasue in thermodynamics we do not think microscopically and there are huge numbers of molecules.</p>
</div></blockquote>
</div>
<div class="section" id="is-information-physical">
<h3><span class="section-number">1.14.2. </span>Is information physical?<a class="headerlink" href="#is-information-physical" title="Permalink to this headline">¶</a></h3>
<p><strong>Wheeler’s It from bit.</strong></p>
<blockquote>
<div><p>Every it — every particle, every field of force, even the space-time continuum itself — derives its function, its meaning, its very existence entirely — even if in some contexts indirectly — from the apparatus-elicited answers to yes-or-no questions, binary choices, bits. It from bit symbolizes the idea that every item of the physical world has at bottom — a very deep bottom, in most instances — an immaterial source and explanation; that which we call reality arises in the last analysis from the posing of yes-no questions and the registering of equipment-evoked responses; in short, that <strong>all things physical are information-theoretic in origin</strong>” John Weeler</p>
</div></blockquote>
</div>
<div class="section" id="relation-between-heat-and-information-entropy">
<h3><span class="section-number">1.14.3. </span>Relation between Heat and information (Entropy)<a class="headerlink" href="#relation-between-heat-and-information-entropy" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[U(p(x), E(x)) = \langle E(x) \rangle = \int p(x) E(x) dx\]</div>
<div class="math notranslate nohighlight">
\[S(p(x)) = \langle -log p(x) \rangle = \int p(x) log p(x) dx\]</div>
<p><strong>Microscopic origin of the First law</strong></p>
<div class="math notranslate nohighlight">
\[d U = \frac{\partial U}{\partial V} dV +\frac{\partial U}{\partial p}dp \]</div>
<div class="math notranslate nohighlight">
\[d U = \delta W +\delta Q\]</div>
<div class="math notranslate nohighlight">
\[\delta W = \Big [ \frac{\partial E_V(x)}{\partial V}\delta x \Big ]dV\]</div>
<div class="math notranslate nohighlight">
\[\delta Q = \Big [ E_V(x)\delta x \Big ] dp\]</div>
<p><strong>Microscopic origin of heat and entropy</strong>
$<span class="math notranslate nohighlight">\(dS = - \int log p(x) \delta x dp\)</span>$</p>
<div class="math notranslate nohighlight">
\[p(x) = \frac{e^{-\beta E(x)}}{e^{-\beta F}} \]</div>
<div class="math notranslate nohighlight">
\[dS = - \int \beta(-E(x) + F) \delta x dp =  \int \beta E(x) \delta x dp = \beta \delta Q\]</div>
</div>
<div class="section" id="jayne-s-maxent-maximum-entropy-principle">
<h3><span class="section-number">1.14.4. </span>Jayne’s MaxEnt (Maximum Entropy principle)<a class="headerlink" href="#jayne-s-maxent-maximum-entropy-principle" title="Permalink to this headline">¶</a></h3>
<p>Probability is an expression of incomplete information. Given that we have some
information, how should we construct a probability distribution that reflects that
knowledge, but is otherwise unbiased? The best general procedure, known as Jaynes
Maximum Entropy Principle , is to choose the probabilities <span class="math notranslate nohighlight">\(p_k\)</span> to maximize the Shanon Measure of Information of the distribution, subject to constraints that express what we do know</p>
<ul class="simple">
<li><p><strong>NVE. Do MaxEnt with no constraints:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[- \sum p_i log p_i - \lambda \sum_i p_i\]</div>
<div class="math notranslate nohighlight">
\[p_1  = p_2 = ... =  p_N \]</div>
<ul class="simple">
<li><p><strong>NVT. Do MaxEnt with constraint on average energy:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[- \sum p_i log p_i - \lambda \sum_i p_i - \beta \sum_i p_i E_i\]</div>
<div class="math notranslate nohighlight">
\[p_i \sim e^{-\beta E_i}\]</div>
<ul class="simple">
<li><p><strong>Roll a fair die:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[- \sum p_i log p_i - \lambda \sum_i p_i\]</div>
<div class="math notranslate nohighlight">
\[p_1  = p_2 = p_3 = p_4= p_5 = p_6\]</div>
<ul class="simple">
<li><p><strong>NVT. Do MaxEnt with biased die which on average gives <span class="math notranslate nohighlight">\(\langle x \rangle = 5.5\)</span></strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[- \sum p_i log p_i - \lambda \sum_i p_i - B \sum_i p_i x_i \]</div>
<div class="math notranslate nohighlight">
\[p_i \sim e^{-B x_i}\]</div>
</div>
<div class="section" id="large-deviation-theory-and-sanov-s-theorem">
<h3><span class="section-number">1.14.5. </span>Large deviation Theory and Sanov’s theorem<a class="headerlink" href="#large-deviation-theory-and-sanov-s-theorem" title="Permalink to this headline">¶</a></h3>
<p>Consider a symmetric random walk again with <span class="math notranslate nohighlight">\(p=1/2\)</span></p>
<div class="math notranslate nohighlight">
\[W_N (n) = \frac{N!}{(N-n)! n!} \Big(\frac{1}{2} \Big)^N\]</div>
<div class="math notranslate nohighlight">
\[ log W_N (n) = N \Big[ - \frac{n}{N} log \frac{n}{N} - \frac{N-n}{N} log \frac{N-n}{N} + log2 \Big]\]</div>
<div class="math notranslate nohighlight">
\[W_N \sim e^{-N I(n)}\]</div>
<p><strong>A large deviation function = Entropy (or Free energy)</strong></p>
<div class="math notranslate nohighlight">
\[s(n) = -\sum_i p_i log p_i = - \frac{n}{N} log \frac{n}{N} - \frac{N-n}{N} log \frac{N-n}{N}\]</div>
<p><strong>Deviation from average <span class="math notranslate nohighlight">\(\langle n \rangle =pN\)</span> are expenentially suppressed with number of steps N:</strong></p>
<div class="math notranslate nohighlight">
\[p(n - \langle n \rangle) \sim e^{-N I'(n)}\]</div>
<ul class="simple">
<li><p>True distirbution (picutre a histogram) of events <span class="math notranslate nohighlight">\(\{q_i \}, i= 1,...\)</span></p></li>
<li><p>Empirical distirbution (picutre a histogram) of events <span class="math notranslate nohighlight">\(\{\pi_i \}, i= 1,...\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\pi_i \rightarrow q_i \,\,\,\, N \rightarrow +\infty\]</div>
<p><strong>What is a probability to find some arbitrary <span class="math notranslate nohighlight">\(\pi = p\)</span> empirical distirbution?</strong></p>
<p>Since we are sampling empirical distirbution from true one:</p>
<div class="math notranslate nohighlight">
\[P(\pi = p) = \prod_i  \frac{N!}{(Np_i)!} q_i^{Np_i}\]</div>
<div class="math notranslate nohighlight">
\[P(\pi = p) \approx e^{-N \sum p_i log \frac{p_i}{q_i}}\]</div>
<p><strong>Relative Entropy (Kulback-Leibler measure)</strong></p>
<div class="math notranslate nohighlight">
\[K(p|q) = \sum_i p_i log \frac{p_i}{q_i}\]</div>
<blockquote>
<div><p>MaxEnt assumes <span class="math notranslate nohighlight">\(q_1 =q_2 = ...q_N\)</span> hence a microcanonical assumption is still built into it.</p>
</div></blockquote>
</div>
<div class="section" id="increasing-entropy-surprising-or-not">
<h3><span class="section-number">1.14.6. </span>Increasing Entropy: surprising or not?<a class="headerlink" href="#increasing-entropy-surprising-or-not" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Entropy of an isolated system increases <span class="math notranslate nohighlight">\(S_{high}-S_{low}\geq 0\)</span> for a simple, almost tautological reason that a majority of states of a system are consistent with higher entropy values <span class="math notranslate nohighlight">\(S_{high}\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Delta S \geq 0\]</div>
<div class="math notranslate nohighlight">
\[I \geq 0 \]</div>
<ul class="simple">
<li><p>If entropy is interpreted as (the lack of) information then the second law may be stated as:</p></li>
<li><p>Information never increases by itself, a decrease of (the lack of) information can only be accomplished by an import of information.</p></li>
</ul>
<blockquote>
<div><p>You should never be surprised by or feel the need to explain why any physical system is in a high entropy state. B Greene</p>
</div></blockquote>
</div>
</div>
<div class="section" id="what-is-maxwell-s-demon">
<h2><span class="section-number">1.15. </span>What is Maxwell’s demon?<a class="headerlink" href="#what-is-maxwell-s-demon" title="Permalink to this headline">¶</a></h2>
<img src="https://upload.wikimedia.org/wikipedia/commons/8/8b/Maxwell%27s_demon.svg" alt="MaxDemon" style="width:80%">
</div>
<div class="section" id="playing-maxwell-s-demon-s-advocate">
<h2><span class="section-number">1.16. </span>Playing Maxwell’s demon’s advocate<a class="headerlink" href="#playing-maxwell-s-demon-s-advocate" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">run</span> ./code/maxwells_demon.py
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TclError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="o">~/</span><span class="n">work</span><span class="o">/</span><span class="n">Statmech4ChemBio</span><span class="o">/</span><span class="n">Statmech4ChemBio</span><span class="o">/</span><span class="mi">04</span><span class="n">_entropy</span><span class="o">/</span><span class="n">code</span><span class="o">/</span><span class="n">maxwells_demon</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">    </span><span class="mi">405</span>     <span class="c1">#the window!)</span>
<span class="g g-Whitespace">    </span><span class="mi">406</span> 
<span class="ne">--&gt; </span><span class="mi">407</span> <span class="n">run</span><span class="p">()</span>

<span class="nn">~/work/Statmech4ChemBio/Statmech4ChemBio/04_entropy/code/maxwells_demon.py</span> in <span class="ni">run</span><span class="nt">()</span>
<span class="g g-Whitespace">    </span><span class="mi">388</span>     <span class="c1"># create the root and the canvas</span>
<span class="g g-Whitespace">    </span><span class="mi">389</span>     <span class="k">global</span> <span class="n">canvas</span>
<span class="ne">--&gt; </span><span class="mi">390</span>     <span class="n">root</span> <span class="o">=</span> <span class="n">Tk</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">391</span>     <span class="n">canvas</span> <span class="o">=</span> <span class="n">Canvas</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">392</span>     <span class="n">canvas</span><span class="o">.</span><span class="n">pack</span><span class="p">()</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.9/x64/lib/python3.7/tkinter/__init__.py</span> in <span class="ni">__init__</span><span class="nt">(self, screenName, baseName, className, useTk, sync, use)</span>
<span class="g g-Whitespace">   </span><span class="mi">2021</span>                 <span class="n">baseName</span> <span class="o">=</span> <span class="n">baseName</span> <span class="o">+</span> <span class="n">ext</span>
<span class="g g-Whitespace">   </span><span class="mi">2022</span>         <span class="n">interactive</span> <span class="o">=</span> <span class="mi">0</span>
<span class="ne">-&gt; </span><span class="mi">2023</span>         <span class="bp">self</span><span class="o">.</span><span class="n">tk</span> <span class="o">=</span> <span class="n">_tkinter</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">screenName</span><span class="p">,</span> <span class="n">baseName</span><span class="p">,</span> <span class="n">className</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">wantobjects</span><span class="p">,</span> <span class="n">useTk</span><span class="p">,</span> <span class="n">sync</span><span class="p">,</span> <span class="n">use</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2024</span>         <span class="k">if</span> <span class="n">useTk</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2025</span>             <span class="bp">self</span><span class="o">.</span><span class="n">_loadtk</span><span class="p">()</span>

<span class="ne">TclError</span>: no display name and no $DISPLAY environment variable
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div>
</div>
</div>
</div>
<p>Learn more about making GUIs with python from the following links:</p>
<blockquote>
<div><p><strong><a class="reference external" href="https://docs.python.org/3/library/tkinter.html">Tkinter</a></strong></p>
</div></blockquote>
<blockquote>
<div><p><strong><a class="reference external" href="https://pysimplegui.readthedocs.io/en/latest/">PySimpleGUI</a></strong></p>
</div></blockquote>
<img src="./figs/LandauerExperiment.png" alt="landauer1" style="width:40%"> </div>
<div class="section" id="information-erasure-and-landauer-s-principle">
<h2><span class="section-number">1.17. </span>Information erasure and Landauer’s principle<a class="headerlink" href="#information-erasure-and-landauer-s-principle" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>When a physical system performs a logical operation that erases or loses information, without a copy being preserved, it must transfer a minimum amount of heat, <span class="math notranslate nohighlight">\(k_BT log(2)\)</span>, to the environment.</p>
</div></blockquote>
<blockquote>
<div><p>Put another way: if an observer loses information about a physical system, the observer loses the ability to extract work from that system. There is a minimal price to be payed for erasing one bit it is <span class="math notranslate nohighlight">\(k_BT log(2)\)</span> (Landauer’s bound)</p>
</div></blockquote>
<img src="./figs/LandauerExperiment2.jpg" alt="landauer1" style="width:40%">  <div class="section" id="szilard-s-engine">
<h3><span class="section-number">1.17.1. </span>Szilard’s Engine<a class="headerlink" href="#szilard-s-engine" title="Permalink to this headline">¶</a></h3>
<p>Recording inforation we can extract work from the environment: <span class="math notranslate nohighlight">\(W = k_B log 2\)</span> and <span class="math notranslate nohighlight">\(Q = k_B log 2\)</span></p>
<ul class="simple">
<li><p><strong>No ghost principle:</strong> information must be carried by something. Information is physical!</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Delta S = \Delta S + k_B I \geq \frac{Q}{T}\]</div>
<div class="math notranslate nohighlight">
\[\Delta S = \Delta S  \geq \frac{Q}{T} - k_B I\]</div>
<ul class="simple">
<li><p>Therefore Information can be used to do work. Knowledge is power!</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Delta F \geq W + k_B T I \]</div>
</div>
<div class="section" id="conditional-entropy">
<h3><span class="section-number">1.17.2. </span>Conditional Entropy<a class="headerlink" href="#conditional-entropy" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>System + Aparatus: S, A. After a measurment we have the following ucnertainty about system+apparatus states:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[H(S,A) = - \sum_{i,j} p(s_i, a_j)log p(s_i a_j)\]</div>
<div class="math notranslate nohighlight">
\[H(S) = - \sum_{s_i} p_S(s_i)log p_S(s_i)\]</div>
<div class="math notranslate nohighlight">
\[H(S | A) = - \sum_{s_i} p_S(s_i)log p_S(s_i)\]</div>
<div class="math notranslate nohighlight">
\[H(S | A) = H(S) - H(S,A) =  -\sum_{s_i} p(s_i, a_j)log \frac{p(s_i, a_j)}{p_A(a_j)}\]</div>
</div>
<div class="section" id="mutual-infromation">
<h3><span class="section-number">1.17.3. </span>Mutual Infromation<a class="headerlink" href="#mutual-infromation" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[I(S,A) = H(S) - H(S | A) = H(S) +H(A) - H(S A) = \sum_{s_i} p(s_i, a_j)log \frac{p(s_i, a_j)}{p_S(s_i)p_A(a_j)}\geq 0\]</div>
<div class="section" id="additional-references">
<h4><span class="section-number">1.17.3.1. </span>Additional References<a class="headerlink" href="#additional-references" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p>JMR Parrondo, JM Horowtiz, T Segawa “Thermodynamics of Information” <a class="reference external" href="https://www.nature.com/articles/nphys3230">Nature Physics volume 11, 131 (2015)</a>
<br></p></li>
<li><p>Z Lu, C Jarzynski, “Engineering Maxwell’s Demon”<a class="reference external" href="https://physicstoday.scitation.org/doi/10.1063/PT.3.2490">Physics Today 67, (2014)</a>
<br></p></li>
<li><p>O Maroney <a class="reference external" href="https://plato.stanford.edu/entries/information-entropy/">Information Processing and Thermodynamic Entropy</a>
<br></p></li>
<li><p>Observing a quantum Maxwell demon at work <a class="reference internal" href="../04_entropy.html"><span class="doc std std-doc">E. Entropy and Information</span></a></p></li>
</ol>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./04_entropy"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../04_entropy.html" title="previous page">E. Entropy and Information</a>
    <a class='right-next' id="next-link" href="../05_ising.html" title="next page">F. Phase transitions and Ising models</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Davit Potoyan<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>