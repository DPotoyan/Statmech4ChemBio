{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables\n",
    "\n",
    ":::{admonition} **What you will learn**\n",
    "\n",
    "- Summing independent random variables results in another random variable called **sumple sum**. The mean of the sample sum is different from the population mean or expectation which is an exact quantity we want to approximate by sampling.\n",
    "- The **Law of Large Numbers** is a principle that states that as the number $N$, the sample mean approaches the population mean with a standard deviation falling off as $N^{-1/2}$\n",
    "- The **Central Limit Theorem (CLT)** tells us that summing independent and identically distributed random variables with well-defined means and variances results in Gaussian distribution regardless of the nature of a random variable. \n",
    "- A model of **random walk** describes the erratic, unpredictable motion of atoms and molecules, providing a fundamental model for diffusion processes and molecular motion in fluids.\n",
    "The number of steps to the right (or left) of a 1D random walker results in a binomial probability distribution. Following CLT binomial distribution in the large N limit can be shown to be well approximated by gaussian with the same mean and variance.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"./figs/RV.png\" alt=\"compton\" class=\"bg-primary mb-1\" width=\"500px\">\n",
    "\n",
    "A random variable is what we interact with in experiments and simulations to infer probability distributions over the sample space.\n",
    ":::\n",
    "\n",
    "- **A random variable X** is a variable whose value depends on the realization of experiment or simulations. \n",
    "    - $X(\\omega)$ is a function from possible outcomes of a sample space $\\omega \\in \\Omega$.\n",
    "    - For a coin toss $\\Omega={H,T}$ $X(H)=+1$ and $X(T)=-1$. Every time the experiment is done, X returns either +1 or -1. We could also make functions of random variables, e.g., every time X=+1, we ear 25 cents, etc. \n",
    "\n",
    "- Random variables are classified into two main types: **discrete and continuous.**\n",
    "\n",
    "    - **Discrete Random Variable:** It assumes a number of distinct values. Discrete random variables are used to model scenarios where outcomes can be counted, such as the number of particles emitted by a radioactive source in a given time interval or the number of photons hitting a detector in a certain period.\n",
    "\n",
    "    - **Continuous Random Variable:** It can take any value within a continuous range. These variables describe quantities that can vary smoothly, such as the position of a particle in space, the velocity of a molecule in a gas, or the energy levels of an atom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Numbers in Python \n",
    "\n",
    "- The [**numpy.random**](https://numpy.org/doc/stable/reference/random/index.html) module provides highly efficient random number generators, implemented in optimized C code for fast performance.  \n",
    "\n",
    "- The most commonly used random number generators in NumPy are:  \n",
    "  - `np.random.rand()` – Generates **uniform** random numbers in the interval \\([0,1]\\).  \n",
    "  - `np.random.randn()` – Generates **standard normal** (Gaussian) random numbers with a **mean of 0** and **variance of 1**.  \n",
    "\n",
    "- Since random numbers are inherently unpredictable, running the same code multiple times will produce different results. To ensure **reproducibility**, you can set a fixed random seed before generating random numbers using:  \n",
    "\n",
    "  ```python\n",
    "  np.random.seed(8376743)\n",
    "  ```\n",
    "\n",
    "  Setting the seed ensures that the same sequence of random numbers is generated each time the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.random.rand(50)\n",
    "\n",
    "print(X)\n",
    "plt.plot(X, '-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Probability Distribution of a Random Variable**\n",
    "\n",
    "- For any random variable $ X $, we are interested in finding the probability distribution over its possible values $ x $, denoted as $ p_X(x) $.\n",
    "- It is important to distinguish between:\n",
    "  - $ x $, which represents a **specific value** the variable can take (e.g., $ 1,2, \\dots, 6 $ for a die).\n",
    "  - $ X $, which is the **random variable itself**, generating values $x$ according to the probability distribution $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Histogram**\n",
    "\n",
    "- A histogram provides an empirical estimate of a distribution by grouping data into bins and counting occurrences within each bin.  \n",
    "- For **continuous distributions**, histograms approximate the **probability density function (PDF)**.  \n",
    "- For **discrete distributions**, histograms approximate the **probability mass function (PMF)**.  \n",
    "- The choice of **bin width** significantly impacts visualization:  \n",
    "  - Too few bins can obscure details. Too many bins can introduce noise, making patterns less clear.  \n",
    "\n",
    "\n",
    "**Histogramming in numpy**\n",
    "- `np.histogram(X, bins=20)`: Divides the range of values of X into e.g. 20 bins and counts how many data points fall into each bin. histogram returns:\n",
    "    - `bin_edges`: The boundaries of each bin.\n",
    "    - `counts`: The number of values in each bin.\n",
    "\n",
    "**Visualization**\n",
    "- `plt.bar(...)`: Plots the histogram using a bar chart.\n",
    "- `plt.hist()`: Can directly plot histogram of random variable\n",
    "- The Seaborn library provides convenient visualization tools for random numbers. For example, ```sns.histplot(np.random.randn(1000), kde=True)``` can be used to visualize the distribution of 1000 normally distributed random numbers with a smooth density curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-code"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate 1000 random values from a normal distribution\n",
    "X = np.random.rand(1000)\n",
    "\n",
    "# Compute the histogram using NumPy\n",
    "counts, bin_edges = np.histogram(X, bins=20)\n",
    "\n",
    "# Print the histogram X\n",
    "print(\"Bin edges:\", bin_edges)\n",
    "print(\"Counts per bin:\", counts)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', alpha=0.7)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of a Random Variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(np.random.rand(1000), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, poisson\n",
    "\n",
    "# Generate data for continuous distribution (Normal)\n",
    "np.random.seed(42)\n",
    "x_continuous = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "# Generate data for discrete distribution (Poisson)\n",
    "x_discrete = np.random.poisson(lam=3, size=1000)\n",
    "\n",
    "# Define x values for theoretical curves\n",
    "x_cont_range = np.linspace(-4, 4, 1000)\n",
    "x_disc_range = np.arange(0, 10)\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Continuous distribution (Normal)\n",
    "axes[0].hist(x_continuous, bins=30, density=True, alpha=0.6, color='b', edgecolor='black', label=\"Histogram\")\n",
    "axes[0].plot(x_cont_range, norm.pdf(x_cont_range, loc=0, scale=1), 'r-', lw=2, label=\"PDF\")\n",
    "axes[0].set_title(\"Continuous Distribution (Normal)\")\n",
    "axes[0].set_xlabel(\"Value\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Discrete distribution (Poisson)\n",
    "axes[1].hist(x_discrete, bins=np.arange(11)-0.5, density=True, alpha=0.6, color='g', edgecolor='black', label=\"Histogram\")\n",
    "axes[1].scatter(x_disc_range, poisson.pmf(x_disc_range, mu=3), color='r', label=\"PMF\", zorder=3)\n",
    "axes[1].set_title(\"Discrete Distribution (Poisson)\")\n",
    "axes[1].set_xlabel(\"Value\")\n",
    "axes[1].set_ylabel(\"Probability\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Expectation and Variance**\n",
    "\n",
    "- The expectation of a random variable, $ E[x] $, represents the **theoretical mean**, distinguishing it from the **sample mean** computed in simulations.\n",
    "- For example, consider the difference between:\n",
    "  - The average height of people computed from a sample of cities.\n",
    "  - The true mean height of the entire world population.\n",
    "- As the sample size increases, the sample mean **converges to** the expectation.\n",
    "\n",
    "- Expectation can be applied to:\n",
    "  - The variable itself (**mean**) and is denoted by $ \\mu $:\n",
    "  - Any function of the variable (e.g., squared deviation for **variance**).\n",
    "\n",
    ":::{admonition} **Expectation of a Random Variable**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "E[f(x)] = \\int f(x) \\cdot p(x) \\,dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[x] = \\int x \\cdot p(x) \\,dx = \\mu\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "- Using the definition of expectation, we define **variance**, which quantifies the spread of $ x $.\n",
    "\n",
    ":::{admonition} **Variance as the Expectation of Mean Fluctuations**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "V[x] = E[(x - E[x])^2] = E[x^2] - E[x]^2 = \\sigma^2\n",
    "$$\n",
    "\n",
    "- We often use the shorthand notation for variance:  \n",
    "  $\\sigma^2 = V[x]$, where $ \\sigma $ is the **standard deviation**.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binomial\n",
    "\n",
    "- A an example of discrete distribution Binomial is defined by a Probability Mass Function (PMF)\n",
    "\n",
    "$$P(n |p, N) =  \\frac{N!}{(N-n)! n!}p^n (1-p)^{N-n}$$\n",
    "\n",
    "- $E[n] = Np$\n",
    "- $V[n] = 4Np(1-p)$\n",
    "\n",
    "\n",
    "**Random Variable**\n",
    "\n",
    "- $B(n, p)$ modeled by ```np.random.binomial(n, p, size)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "r = np.random.binomial(n=1, p=0.6, size=2000) \n",
    "\n",
    "fig, ax = plt.subplots(ncols=2) \n",
    "ax[0].plot(r,  color='blue', label='trajectory')\n",
    "ax[1].hist(r,  density=True, color='red',  label = 'histogram')\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Samples of RN')\n",
    "ax[0].set_ylabel('Values of RN')\n",
    "\n",
    "ax[1].set_xlabel('Values of RN')\n",
    "ax[1].set_ylabel('Probability Density')\n",
    "fig.legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian\n",
    "\n",
    "- A an example of continuous distribution Gaussian is defined by a Probability Distribution Function\n",
    "\n",
    "$$P(x |\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "- $E[x] = \\mu$\n",
    "- $V[x] = \\sigma^2$\n",
    "\n",
    "**Random Variable**\n",
    "\n",
    "- $N(a, b)$ modeled by ```np.random.normal(loc,scale, size=(N, M))```\n",
    "- $N(0, 1)$ modeled by ```np.random.randn(N, M, P, ...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# For a standard normal with sigma=1, mu=0\n",
    "r = np.random.randn(200)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2) \n",
    "ax[0].plot(r,  color='blue', label='trajectory')\n",
    "ax[1].hist(r,  density=True, color='red',  label = 'histogram')\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Samples of RN')\n",
    "ax[0].set_ylabel('Values of RN')\n",
    "\n",
    "ax[1].set_xlabel('Values of RN')\n",
    "ax[1].set_ylabel('Probability Density')\n",
    "fig.legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Distribution\n",
    "\n",
    "- A simple example of a continuous distribution is the **Uniform distribution**, where all values within a given range are equally likely. It is defined by the **Probability Density Function (PDF)**:\n",
    "\n",
    "$$\n",
    "P(x | a, b) =\n",
    "\\begin{cases} \n",
    "\\frac{1}{b - a}, & a \\leq x \\leq b \\\\ \n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Expectation and Variance:**\n",
    "  - $E[x] = \\frac{a + b}{2}$\n",
    "  - $V[x] = \\frac{(b - a)^2}{12}$\n",
    "\n",
    "**Random Variable**  \n",
    "\n",
    "- $U(a, b)$ is modeled by:  \n",
    "  ```python\n",
    "  np.random.uniform(low, high, size=(N, M))\n",
    "  ```\n",
    "- $U(0,1)$ (standard uniform) is modeled by:  \n",
    "  ```python\n",
    "  np.random.rand(N, M, P, ...)\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# For a standard uniform\n",
    "r = np.random.random(200)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2) \n",
    "ax[0].plot(r,  color='blue', label='trajectory')\n",
    "ax[1].hist(r,  density=True, color='red',  label = 'histogram')\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('Samples of RN')\n",
    "ax[0].set_ylabel('Values of RN')\n",
    "\n",
    "ax[1].set_xlabel('Values of RN')\n",
    "ax[1].set_ylabel('Probability Density')\n",
    "fig.legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact vs sampled probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Simplified and optimized version of the code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Generate x values and compute standard normal PDF\n",
    "x = np.linspace(-4, 4, 200)\n",
    "px = norm.pdf(x)\n",
    "\n",
    "# Generate random samples\n",
    "r = np.random.randn(1000)\n",
    "\n",
    "# Plot histogram and theoretical normal distribution\n",
    "plt.hist(r, bins=30, density=True, alpha=0.6, color='blue', edgecolor='black', \n",
    "         label=f'Sampled: mean={r.mean():.2f}, var={r.var():.2f}')\n",
    "         \n",
    "plt.plot(x, px, 'k-', linewidth=2, label='Exact: mean=0, var=1')\n",
    "\n",
    "# Formatting\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.xlabel(r'$x$', fontsize=14)\n",
    "plt.ylabel(r'$p(x)$', fontsize=14)\n",
    "plt.title(\"Comparison of Sampled Data with Normal Distribution\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn about Transforming Random Variables  \n",
    "\n",
    "- When a random variable $X $ is transformed by adding, multiplying by a constant, or applying a function $ Y = f(X) $, its probability distribution changes accordingly from $ p(x) $ to $ p(y) $.  \n",
    "- Two commonly used transformations in statistical modeling involve generating specific distributions from standard forms:\n",
    "\n",
    "  - **Generating a Gaussian (Normal) distribution** from a standard normal:  \n",
    "\n",
    "    $$\n",
    "    N(\\mu, \\sigma^2) = \\mu + \\sigma \\cdot N(0,1)\n",
    "    $$\n",
    "\n",
    "  - **Generating a Uniform distribution** from a standard uniform:  \n",
    "\n",
    "    $$\n",
    "    U(a, b) = (b - a) \\cdot U(0,1) + a\n",
    "    $$\n",
    "\n",
    "- These transformations are frequently used to construct random samples from desired distributions in simulations and statistical mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{admonition} **Transforming Random Variables**\n",
    ":class: dropdown, tip\n",
    "\n",
    "- When transforming a random variable $ X $ to a new variable $ Y = f(X) $, the probability density functions are related by a **Jacobian factor** to account for how the transformation stretches or compresses the distribution:\n",
    "\n",
    "$$\n",
    "p(x) dx = p(y) dy\n",
    "$$\n",
    "\n",
    "which gives:\n",
    "\n",
    "$$\n",
    "p(y) = p(x) \\cdot \\Bigg| \\frac{dx}{dy} \\Bigg|\n",
    "$$\n",
    "\n",
    "- **Examples of Simple Transformations:**\n",
    "  1. **Addition:** $ Y = X + a $\n",
    "     - The probability remains unchanged except for a shift:  \n",
    "\n",
    "       $$\n",
    "       p(y) = p(x + a) \\cdot 1\n",
    "       $$\n",
    "\n",
    "  2. **Multiplication:** $ Y = aX $\n",
    "     - The distribution scales with a factor $ \\frac{1}{|a|} $:  \n",
    "\n",
    "       $$\n",
    "       p(y) = p(x) \\cdot \\frac{1}{|a|}\n",
    "       $$\n",
    "\n",
    "- These transformations yield useful properties:\n",
    "  - **Shifting the Mean:**  \n",
    "\n",
    "    $$\n",
    "    E[X + a] = E[X] + a\n",
    "    $$\n",
    "\n",
    "  - **Scaling the Variance:**  \n",
    "\n",
    "    $$\n",
    "    V[aX] = a^2 V[X]\n",
    "    $$\n",
    "\n",
    "- Using these properties, we can generate:\n",
    "\n",
    "  - A **Gaussian (Normal) distribution** from a standard normal:\n",
    "\n",
    "    $$\n",
    "    N(\\mu, \\sigma^2) = \\mu + \\sigma \\cdot N(0,1)\n",
    "    $$\n",
    "\n",
    "  - A **Uniform distribution** from a standard uniform:\n",
    "\n",
    "    $$\n",
    "    U(a, b) = (b - a) \\cdot U(0,1) + a\n",
    "    $$\n",
    "    \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "mu, sigma = 5, 2  # Mean and standard deviation for Gaussian\n",
    "a, b = 2, 8       # Bounds for Uniform\n",
    "\n",
    "# Generate standard distributions\n",
    "std_normal = np.random.randn(10000)  # N(0,1)\n",
    "std_uniform = np.random.rand(10000)  # U(0,1)\n",
    "\n",
    "# Transform distributions\n",
    "normal_dist = mu + sigma * std_normal  # N(mu, sigma^2)\n",
    "uniform_dist = a + (b - a) * std_uniform  # U(a, b)\n",
    "\n",
    "# Plot Distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Normal Distribution\n",
    "axes[0].hist(normal_dist, bins=40, density=True, alpha=0.6, color='b', edgecolor='black')\n",
    "axes[0].set_title(f\"Transformed Normal Distribution N({mu}, {sigma}²)\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "\n",
    "# Uniform Distribution\n",
    "axes[1].hist(uniform_dist, bins=20, density=True, alpha=0.6, color='g', edgecolor='black')\n",
    "axes[1].set_title(f\"Transformed Uniform Distribution U({a}, {b})\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Two Random Variables\n",
    "\n",
    "- Consider the sum of two random variables, such as:\n",
    "  - The sum of numbers obtained from rolling two dice.\n",
    "  - The sum of two coin flips (e.g., heads = 1, tails = 0).\n",
    "  - Sum of kinetic eneries of ideal gas. \n",
    "\n",
    "$$\n",
    "X = X_1 + X_2\n",
    "$$\n",
    "\n",
    "- The sum of random variables is itself a random variable! \n",
    "- We want to understand how to described the properties of summed random variables  as they offer a prototype of how large systems emerge froms mall components. \n",
    "- Given probability distirbution of $X_1$ and $X_2$ how do we find probability distribution of X?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Play with sums of RVs**\n",
    ":class: note, dropdown\n",
    "Take unifiorm random number between 0 an 1. Generate 10, 20, 100 and see how are they behaving. Here are some helpful tips to generate Random Variables. \n",
    "- ```np.random.random(n)``` generates array of random variables of size $n$\n",
    "- ```np.random.random((n, m))``` generates array of random variables of shape $(n, m)$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Expectation and Variance of the Sum\n",
    "\n",
    "- Expectation is always a **linear operator**, which follows from the definition of expectation and the linearity of integration:\n",
    "\n",
    "$$\n",
    "E[X_1 + X_2] = E[X_1] + E[X_2]\n",
    "$$\n",
    "\n",
    "- However, variance is **not** generally a linear operator. To see this let us write explicit formula first:\n",
    "\n",
    "$$\n",
    "V[X_1 + X_2] = E\\left[(X_1 + X_2 - E[X_1 + X_2])^2\\right] \n",
    "$$\n",
    "\n",
    "- Defining the **mean-subtracted variables**: $Y_i = X_i - E[X_i]$ we express variance of sum in terms of variances of component random variables \n",
    "\n",
    "$$\n",
    "V[X_1 + X_2] = E\\left[(X_1 - E[X_1] + X_2 - E[X_2])^2\\right] = E\\left[(Y_1 + Y_2)^2\\right]\n",
    "$$\n",
    "\n",
    "- Since $ V[X_i] = E[Y_i^2] $, this simplifies to:\n",
    "\n",
    "$$\n",
    "V[X_1 + X_2] = E[Y^2_1] + V[Y^2_2] + 2E[Y_1 Y_2] = V[X_1] + V[X_2] + 2 Cov[X_1, X_2]\n",
    "$$\n",
    "\n",
    "- The cross term is called **Covariance** which measures the degree to which two random variables **vary together**.\n",
    "- To obtain a **scale-independent** measure, we then define the **correlation coefficient**. Corr the sign of which shows if correlation is positivie or negative. \n",
    "\n",
    ":::{admonition} **Covariance and Correlation of Two Random Variables**  \n",
    ":class: important  \n",
    "\n",
    "$$\n",
    "\\text{Cov}[X_1, X_2] = E[(X_1 - E[X_1])(X_2 - E[X_2])]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Corr}[X_1, X_2] = \\frac{\\text{Cov}[X_1, X_2]}{\\sigma_{X_1} \\sigma_{X_2}}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "- In the special case where $X_1 $ and $X_2 $ are **statistically independent**, covariance (or correlation) is zero and we have additivity of variances!\n",
    "\n",
    "\n",
    "$$V[X_1+X_2] = V[X_1]+V[X_2]$$\n",
    "\n",
    "- This result is **fundamental** in statistical mechanics, probability theory, and the sciences, as it explains why variances add for independent random variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Generate random data with different correlation coefficients\n",
    "np.random.seed(42)\n",
    "num_points = 300\n",
    "\n",
    "# Define correlation levels\n",
    "correlations = [0.0, 0.3, 0.6, 0.9]\n",
    "fig, axes = plt.subplots(1, len(correlations), figsize=(16, 4))\n",
    "\n",
    "for i, corr in enumerate(correlations):\n",
    "    mean = [0, 0]\n",
    "    cov_matrix = [[1, corr], [corr, 1]]  # Covariance matrix based on correlation\n",
    "    data = np.random.multivariate_normal(mean, cov_matrix, num_points)\n",
    "    \n",
    "    ax = axes[i]\n",
    "    sns.scatterplot(x=data[:, 0], y=data[:, 1], alpha=0.6, ax=ax, edgecolor=None)\n",
    "    ax.set_title(f'Correlation = {corr}', fontsize=12)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    \n",
    "# Overall title\n",
    "plt.suptitle(\"Illustration of Correlation Coefficient Between Two Random Variables\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of $ N $ Random Variables  \n",
    "\n",
    "- Consider a sequence of **independent and identically distributed (i.i.d.)** random variables, $ X_1, X_2, \\ldots, X_n $.  \n",
    "- Since they are **identically distributed**, each variable has a well-defined **mean** $ \\mu $ and **variance** $ \\sigma^2 $.  \n",
    "- Our goal is to understand how the **sum** and **mean** of these variables depend on the sample size $ n $.\n",
    "- For convenience we also introduce notation for **zero mean random variables** $Y_i = X_i-E[X_i]$, since $E[Y_i]=0$\n",
    "\n",
    ":::{admonition} **Sample Sum and Sample Mean**  \n",
    ":class: important  \n",
    "\n",
    "$$\n",
    "S_n = \\sum_{i=1}^{n} X_i, \\quad M_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "$$\n",
    "\n",
    "- $ S_n $ is the **sample sum**, and $ M_n $ is the **sample mean**.  \n",
    "- These quantities fluctuate with sample size $ n $, but we expect them to **converge to their expectations** for large $ n $ \n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    ":::{admonition} **Mean and Variance of the Sum of i.i.d. Random Variables**  \n",
    ":class: important  \n",
    "\n",
    "- **Expectation of the Sum:**  \n",
    "\n",
    "$$\n",
    "E[S_n] = E\\left[ \\sum_{i=1}^{n} X_i \\right] = \\sum_{i=1}^{n} E[X_i] = n\\mu\n",
    "$$\n",
    "\n",
    "- **Variance of the Sum:**  \n",
    "\n",
    "$$\n",
    "V[S_n] = E\\left[ (S_n - n\\mu)^2 \\right] = \\Bigg[\\sum_{i=1}^{n}  Y_i \\Bigg]^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n}E[Y_i Y_j] = \\sum_{i=1}^{n} V[X_i] = n\\sigma^2\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law of Large Numbers\n",
    "\n",
    "- For the **sample mean** the result of summatiion of i.i.d variables implies\n",
    "\n",
    "$$\n",
    "E[M_n] = \\frac{1}{n} E[S_n] = \\mu\n",
    "$$\n",
    "\n",
    "$$\n",
    "V[M_n] = \\frac{1}{n^2} V[S_n] = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "- Thus, the sample mean is an **unbiased estimator** of $ \\mu $, and its variance decreases as $ 1/n $, meaning that the estimate becomes more stable as $ n $ increases.\n",
    "\n",
    "\n",
    ":::{admonition} **Law of Large Numbers (LLN)**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "E[M_n] \\to \\mu\n",
    "$$  \n",
    "\n",
    "$$V[M_n] \\to \\sigma^2 / n$$\n",
    "\n",
    "**Implication:**  \n",
    "- The sample mean provides a reliable estimate of $\\mu$ for large $n$.  \n",
    "- The variance of $M_n$ decreases as , meaning fluctuations shrink as $1/\\sqrt{n}$.  \n",
    "- This justifies ensemble averaging in statistical mechanics, ensuring macroscopic observables (e.g., temperature, pressure) are stable and predictable.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of trials and runs\n",
    "N, runs = int(1e5), 30\n",
    "\n",
    "# Store fractions of heads for each trial in each run\n",
    "fractions = np.zeros((runs, N))\n",
    "\n",
    "# Simulate coin tosses\n",
    "for run in range(runs):\n",
    "    # Generate coin tosses (0 for tails, 1 for heads)\n",
    "    tosses = np.random.randint(2, size=N)\n",
    "    # Calculate cumulative sum to get the number of heads up to each trial\n",
    "    cum_heads = np.cumsum(tosses)\n",
    "    # Calculate fraction of heads up to each trial\n",
    "    fractions[run, :] = cum_heads / np.arange(1, N+1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot all runs with low opacity\n",
    "for run in range(runs):\n",
    "    plt.plot(fractions[run, :], color='grey', alpha=0.3)\n",
    "\n",
    "# Highlight first run\n",
    "plt.semilogx(fractions[0, :], color='blue', linewidth=2, label='Highlighted Run')\n",
    "\n",
    "# Expected value line\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='Expected Value (0.5)')\n",
    "plt.xlabel('Number of Trials')\n",
    "plt.ylabel('Fraction of Heads')\n",
    "plt.title('Law of Large Numbers: Fraction of Heads in Coin Tossing')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Central Limit Theorem  (CLT)\n",
    "\n",
    "- **Central Limit Theorem** asserts that the probability distribution function or **PDF** of sum of random variables becomes gaussian distribution with mean $n\\mu$ and $n\\sigma^2$. \n",
    "- Note that CLT is based on assumption that the **mean and variance**, $\\mu$ and $\\sigma^2$, **are finite!**. Thus, CLT does not hold for certain power-law distributed random variables. \n",
    "\n",
    ":::{admonition} **Central Limit Theorem  (CLT)**\n",
    ":class: important\n",
    "\n",
    "- Sum of any i.i.d variables (even if they are not gaussian) leads to normally distributed random variable (the sum $s_n$)\n",
    "\n",
    "$$X_1 +X_2+...+X_n \\rightarrow N(n\\mu, n\\sigma^2)$$\n",
    "\n",
    "- The probability density function (PDF) of $S_n$ is approaching gaussian: \n",
    "\n",
    "$$p(s) = \\frac{1}{(2\\pi  n\\sigma^2)^{1/2}}e^{-\\frac{(s-n\\mu)^2}{2 n\\sigma^2}}$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Parameters\n",
    "num_samples = 10000  # Number of samples per distribution\n",
    "num_bins = 50  # Number of bins for histogram\n",
    "sample_sizes = [1, 2, 5, 10, 30]  # Different sample sizes\n",
    "bin_edges = np.linspace(0, 1, num_bins + 1)\n",
    "\n",
    "# Create a figure with a 3D axis\n",
    "fig = plt.figure(figsize=(10, 7))  # Initialize the figure\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Generate distributions and plot stacked histograms\n",
    "for i, n in enumerate(sample_sizes):\n",
    "    samples = np.mean(np.random.uniform(0, 1, (num_samples, n)), axis=1)  # Compute sample means\n",
    "\n",
    "    hist, bins = np.histogram(samples, bins=bin_edges, density=True)\n",
    "\n",
    "    # Centers of bins\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    # Offset along the y-axis for stacking\n",
    "    y_offset = i  \n",
    "\n",
    "    # Plot histogram as bars\n",
    "    ax.bar(bin_centers, hist, zs=y_offset, zdir='y', alpha=0.7, width=0.02, edgecolor='black', label=f'n={n}')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Sample Mean Bins', fontsize=12)\n",
    "ax.set_ylabel('Sample Size (n)', fontsize=12)\n",
    "ax.set_zlabel('Probability Density', fontsize=12)\n",
    "ax.set_yticks(range(len(sample_sizes)))\n",
    "ax.set_yticklabels([f'n={n}' for n in sample_sizes])\n",
    "ax.set_title(\"Illustrating CLT with sum of Uniform Random Numbers\", fontsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardized random variables\n",
    "\n",
    "- Standardized means random variables have zero mean and unit variance, $E[Z_i] = 0, V[Z_i]=1$. This can be achieved by subtracting mean and deviding by standard deviation either individual variables or their sums.\n",
    "\n",
    "$$Z_i = \\frac{X_i-\\mu}{\\sigma}$$\n",
    "\n",
    "$$S'_n = \\frac{S_n - E[S_n]}{V[S_n]^{1/2}} = \\frac{S_n - n\\mu}{n^{1/2}\\sigma}$$\n",
    "\n",
    "$$S'_n = Z_1+Z_2 +... \\rightarrow N(0, 1)$$\n",
    "\n",
    "- Notice that **we are dividing the sum by $n^{1/2}$** if we have devide sum by $n$ then we have mean the variance of which goes. This is Law of Large Numbers. \n",
    "- The CLT motivates us to consuder dividing sum by $n^{1/2}$ which gives us another gaussian distributed variable, a more simple one with no parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of coin tosses in each experiment, number of experiments\n",
    "N, runs = 1000, 10000  \n",
    "\n",
    "# Simulate coin tosses: num_experiments rows, num_tosses_per_experiment columns\n",
    "tosses = np.random.randint(2, size=(N, runs))\n",
    "\n",
    "# Calculate sums of each run\n",
    "Sn = np.sum(tosses, axis=0)\n",
    "\n",
    "# Compute numerical mean and standard deviation of Sn\n",
    "mu_Sn = np.mean(Sn)  \n",
    "sigma_Sn = np.std(Sn, ddof=1)  # Using sample standard deviation (ddof=1)\n",
    "\n",
    "# Compute z-scores numerically\n",
    "z = (Sn - mu_Sn) / sigma_Sn\n",
    "\n",
    "# Plot histogram of z-scores\n",
    "plt.figure()\n",
    "plt.hist(z, density=True, bins=40, alpha=0.6, label=\"Simulated\")\n",
    "plt.title('Distribution of Standardized Sums of Coin Tosses')\n",
    "plt.xlabel('Standardized Sum (Z-score)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Overlay standard normal distribution\n",
    "zs = np.linspace(z.min(), z.max(), 1000)\n",
    "plt.plot(zs, norm.pdf(zs), 'k', label='N(0,1)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Example of CLT applied to random walk problem**\n",
    ":class: note, dropdown\n",
    "\n",
    "Applying the formulas to random walk model we get mean and variance for single step\n",
    "\n",
    "$$E[X_1] = f \\cdot 1 + (1-f) \\cdot (-1) = 2f-1$$ \n",
    "\n",
    "$$V[X_1] = E[X^2_1] -  E[X_1]^2 = f \\cdot 1^2+ (1-f) (-1)^2 - (2f-1)^2 = 4 f(1-f)$$\n",
    "\n",
    "Since steps of a random walker are independent we can compute the variance of a total displacement by multiplying mean and varaince of a single step by N \n",
    "\n",
    "$$E[x]=N(2f -1)$$\n",
    "\n",
    "$$V[x]=N\\bar{\\sigma^2_1} = 4Nf (1-f)$$ \n",
    "\n",
    "The variance of the mean $\\bar{x} = x/N$ would then be:\n",
    "\n",
    "$$V[\\bar{x}] = \\frac{4f (1-f)}{N}$$ \n",
    "\n",
    "::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating a 1D unbiased random walk \n",
    "\n",
    "- Each random walker will be modeled by a random variable $X_i$, assuming +1 or -1 values at every step. We will run N random walkers (rows) over n steps (columns)\n",
    "- We then take **cumulative sum  over n steps** thereby summing n random variables for **N walkers**. This will be done via a convenient ```np.cumsum()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rw_1d(n, N):\n",
    "    \"\"\"\n",
    "    Simulates a 1D symmetric random walk.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Number of steps.\n",
    "    N (int): Number of walkers.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A (n, N) array where each column represents a walker's trajectory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate random steps (-1 or +1) for all walkers\n",
    "    steps = np.random.choice([-1, 1], size=(n, N))\n",
    "    \n",
    "    # Compute cumulative sum to get displacement\n",
    "    rw = np.cumsum(steps, axis=0)\n",
    "\n",
    "    # Ensure the initial position is zero\n",
    "    rw = np.vstack([np.zeros(N), rw])  # Adds a row of zeros at the start\n",
    "\n",
    "    return rw\n",
    "\n",
    "# Example usage: Simulate and plot a few random walks\n",
    "n_steps = 1000\n",
    "n_walkers = 3\n",
    "rw = rw_1d(n_steps, n_walkers)\n",
    "\n",
    "plt.plot(rw)\n",
    "plt.ylabel('X (displacement)')\n",
    "plt.xlabel('n (steps)')\n",
    "plt.title('1D Random Walk')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate 1D random walk\n",
    "def rw_1d(n_max, N):\n",
    "    \"\"\"Generates a 1D random walk for N walkers over n_max steps.\"\"\"\n",
    "    steps = np.random.choice([-1, 1], size=(n_max, N))  # Random steps\n",
    "    return np.cumsum(steps, axis=0)  # Cumulative sum gives positions\n",
    "\n",
    "# Parameters\n",
    "n_max = 1000  # Number of steps\n",
    "N_max = 1000  # Number of walkers\n",
    "rw = rw_1d(n_max, N_max)\n",
    "\n",
    "# Define time snapshots\n",
    "snapshots = [10, 100, 500, 900]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=len(snapshots), figsize=(15, 6), constrained_layout=True)\n",
    "\n",
    "for i, n in enumerate(snapshots):\n",
    "    # Plot random walk trajectories\n",
    "    ax = axes[0, i]\n",
    "    ax.plot(rw[:, :50], alpha=0.3)  # Show 50 trajectories for clarity\n",
    "    ax.axvline(x=n, color='black', lw=2, linestyle=\"--\")  # Mark current time step\n",
    "    ax.set_xlabel('Step (n)')\n",
    "    ax.set_ylabel('Position (X)')\n",
    "    ax.set_title(f'Step n = {n}')\n",
    "\n",
    "    # Histogram of positions at step n\n",
    "    ax_hist = axes[1, i]\n",
    "    ax_hist.hist(rw[n, :], bins=30, color='orange', density=True, alpha=0.6, label=f'n = {n}')\n",
    "    \n",
    "    # Gaussian overlay\n",
    "    x = np.linspace(-100, 100, 1000)\n",
    "    y = stats.norm.pdf(x, loc=0, scale=np.sqrt(n))\n",
    "    ax_hist.plot(x, y, color='black', lw=2, label='N(0,1)')\n",
    "\n",
    "    ax_hist.set_xlim([-100, 100])\n",
    "    ax_hist.legend()\n",
    "    ax_hist.set_title(f'$\\sigma^2$ / n = {np.var(rw[n - 1, :]) / n:.3f}')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rw_2d(n, N):\n",
    "    \"\"\"\n",
    "    Simulates a 2D symmetric random walk.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Number of steps.\n",
    "    N (int): Number of trajectories.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A (n+1, N, 2) array where each trajectory is stored in the last dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define possible step directions (right, up, left, down)\n",
    "    steps = np.array([(1, 0), (0, 1), (-1, 0), (0, -1)])\n",
    "    \n",
    "    # Generate random step indices and map to step directions\n",
    "    random_steps = steps[np.random.choice(4, size=(n, N))]\n",
    "    \n",
    "    # Prepend an initial position at (0,0) for all walkers\n",
    "    rw = np.zeros((n + 1, N, 2), dtype=int)\n",
    "    rw[1:] = np.cumsum(random_steps, axis=0)  # Compute displacement over time\n",
    "\n",
    "    return rw\n",
    "\n",
    "# Example usage: Simulate and plot first three random walkers\n",
    "n_steps = 1000\n",
    "n_walkers = 100\n",
    "traj = rw_2d(n_steps, n_walkers)\n",
    "\n",
    "plt.plot(traj[:, :3, 0], traj[:, :3, 1])  # Plot first three random walkers\n",
    "plt.xlabel('X (displacement)')\n",
    "plt.ylabel('Y (displacement)')\n",
    "plt.title('2D Random Walk')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Re-import necessary libraries due to execution state reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Parameters\n",
    "num_steps = 200  # Number of steps in the random walk\n",
    "step_size = 1  # Step size\n",
    "\n",
    "# Generate random steps in 3D\n",
    "np.random.seed(42)\n",
    "steps = np.random.choice([-step_size, step_size], size=(num_steps, 3))  # Random steps in x, y, z\n",
    "positions = np.cumsum(steps, axis=0)  # Cumulative sum to get positions\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim([positions[:, 0].min()-1, positions[:, 0].max()+1])\n",
    "ax.set_ylim([positions[:, 1].min()-1, positions[:, 1].max()+1])\n",
    "ax.set_zlim([positions[:, 2].min()-1, positions[:, 2].max()+1])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('3D Random Walk')\n",
    "\n",
    "# Initialize the line and point\n",
    "line, = ax.plot([], [], [], 'b-', lw=2)\n",
    "point, = ax.plot([], [], [], 'ro')\n",
    "\n",
    "# Update function for animation\n",
    "def update(frame):\n",
    "    line.set_data(positions[:frame+1, 0], positions[:frame+1, 1])\n",
    "    line.set_3d_properties(positions[:frame+1, 2])\n",
    "    point.set_data([positions[frame, 0]], [positions[frame, 1]])\n",
    "    point.set_3d_properties([positions[frame, 2]])\n",
    "    return line, point\n",
    "\n",
    "# Create animation\n",
    "ani = animation.FuncAnimation(fig, update, frames=num_steps, interval=50, blit=False)\n",
    "\n",
    "# Convert animation to HTML\n",
    "html_anim = HTML(ani.to_jshtml())\n",
    "plt.close(fig)  # Prevent duplicate display\n",
    "\n",
    "# Display the animation\n",
    "html_anim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "#### Problem 1 Binomial as generator of Gaussian and Poisson distributions\n",
    "\n",
    "- Show that in large number limit binomial distribution tends to gaussian. Show is by expanding binomial distirbution $logp(n)$ in power series showing that terms beyond quadratic can be ignored. \n",
    "\n",
    "- In the limit $N\\rightarrow \\infty$ but for very small values of $p \\rightarrow 0$ such that $\\lambda =pN=const$ there is another distribution that better approximates Binomial distribution: $p(x)=\\frac{\\lambda^k}{k!}e^{-\\lambda} $ It is known as Poisson distribution. <br>\n",
    "Poisson distribution is an excellent approximation for probabilities of rare events. Such as, infrequently firing neurons in the brain, radioactive decay events of Plutonium or rains in the desert. <br>  Derive Poisson distribution by taking the limit of $p\\rightarrow 0$ in binomial distribution.\n",
    "\n",
    "- Using numpy and matplotlib plot binomial probability distribution\n",
    "against Gaussian and Poisson distributions for different values of N=(10,100,1000,10000). <br>\n",
    "- For a value N=10000 do four plots with the following values \n",
    "p=0.0001, 0.001, 0.01, 0.1. You can use  subplot functionality to make a pretty 4 column plot. (See plotting module)\n",
    "\n",
    "```python\n",
    "fig, ax =  plt.subplots(nrows=1, ncols=4)\n",
    "ax[0].plot()\n",
    "ax[1].plot()\n",
    "ax[2].plot()\n",
    "ax[3].plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem-2 Confined diffusion.\n",
    "\n",
    "Simulate 2D random walk in a circular confinement. Re-write 2D random walk  code to simulate diffusion of a particle which is stuck inside a sphere. \n",
    "Study how root mean square deviation of position scales with time. \n",
    "- Carry out simulations for different confinement sizes. \n",
    "- Make plots of simulated trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem-3 Return to the origin!\n",
    "\n",
    "- Simulate random walk in 1D and 2D for a different number of steps $N=10, 10^2,10^3, 10^4, 10^5$\n",
    "- Compute average number of returns to the origin $\\langle n_{orig} \\rangle$. That is number of times a random walker returns to the origin $0$ for 1D  or (0,0)$ for 2D . You may want to use some 1000 trajectories to obtain average. \n",
    "- Plot how $\\langle n_{orig} \\rangle$ depends on number of steps N for 1D and 2D walker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem-4 Breaking the CLT; Cauchy vs Normal random walk in 2D\n",
    "\n",
    "For this problem we are going to simulate two kinds of random walks in continuum space (not lattice): Levy flights and Normal distributd random walk. \n",
    "\n",
    "To simulate a 2D continuum space random walk we need to generate random step sizes $r_x$, $r_y$. \n",
    "Also you will need unifrom random namber to sample angles in 2D giving you a conitnuum random walk in 2D space: $x = r_x sin\\theta$ and $y=r_ycos\\theta$\n",
    "\n",
    "- Normally: $r\\sim N(0,1)$\n",
    "- Cauchy distribution (long tails, infinite variance) $r\\sim Cauchy(0,1)$\n",
    "- Unform angles $\\theta \\sim U(0,1)$\n",
    "\n",
    "Visualize random walk using matplotlib and study statistics of random walkers the way that is done for normal random walk/brownian motion examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem-5 Continuous time random walk (CTRW)\n",
    "\n",
    "Simulate 1D random walk but instead of picking times at regular intervals pick them from  exponential distribution. <br>\n",
    "Hint: you may want to use random variables from scipy.stats.exp <br>\n",
    "\n",
    "[scipy.stats.expon](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html) <br>\n",
    "\n",
    "Study the root mean square deviation as a function of exponential decay parameter $\\lambda$ of exponential distribution $e^{-\\lambda x}$. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
