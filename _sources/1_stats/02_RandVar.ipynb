{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Random walk and CLT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    ":::{admonition} What you need to know\n",
    "\n",
    "- Summing independent random variables creates another random variable that could be used to compute means and variances in the sample.\n",
    "- The Law of Large Numbers is a principle that states that as the number of observations increases, the average of the results will get closer to the expected value, meaning that more data leads to more accurate outcomes.\n",
    "- The Central Limit Theorem (CLT) tells us that if you take a lot of samples from any kind of population and look at the average of those samples, those averages will tend to form a normal distribution, even if the original population is not shaped that way.\n",
    "- In chemistry and physics, a random walk model describes the erratic, unpredictable motion of atoms and molecules, providing a fundamental model for diffusion processes, molecular motion in fluids, and the propagation of light in disordered media.\n",
    "- Learn to simulate random walk and diffusive processes using python\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Sum of random variables\n",
    "\n",
    "- Consider a sequence $X_1, X_2, \\ldots$ of **i.i.d. (independent identically distributed)** random variables. \n",
    "- The sum of n terms $S_n$ is called a **sample sum** and is also a random variable. Similarly, one can define a sample mean $M_n =S_n/n$\n",
    "\n",
    "$$S_n = \\sum_{i=1}^n X_i$$\n",
    "\n",
    "- Each of the $X_i$ has the same mean $\\mu = E(X_1)$ and variance $\\sigma^2 = V(X_1)$.  \n",
    "\n",
    "- Because we are dealing with **independent random variables**, the variance and the expectation of sample sum are a linear function of $n$ number of steps. In other words, no cross-terms survive averaging. \n",
    "\n",
    "$$E\\left(S_n\\right) = \\sum_{i=1}^n E\\left(X_i\\right) = n \\mu$$\n",
    "\n",
    "$$V\\left(S_n\\right) =  \\sum_i \\sum_j E\\big[(X_i-E(x))\\cdot (X_j-E(x))\\big]=  \\sum_i E\\big[(X_i-E(x))^2\\big] =\\sum_{i=1}^n V\\left(X_i\\right) = n \\sigma^2$$\n",
    "\n",
    "- The sample mean converges to the exact mean with variance growing down as $n^{-1/2}$. This is known **Law of Large Numbers (LLN)** \n",
    "\n",
    "$$\\boxed{\\frac{V(S_n)^{1/2}}{E(S_n)} = \\frac{1}{n^{1/2}}\\frac{\\sigma}{\\mu }}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24403624529395723"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_test_lln(n):\n",
    "    \n",
    "    S = np.random.randn(n)\n",
    "    \n",
    "    return np.std(S)/(n * np.mean(S))\n",
    "\n",
    "simple_test_lln(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The Central Limit Theorem  (CLT)\n",
    "\n",
    "- Let $X_1, X_2, \\ldots $ be a sequence of i.i.d. random variables with common mean $\\mu$ and variance $\\sigma^2$. \n",
    "\n",
    "- We scale our random variables to make them **de-meaned** and **scaled** sample mean to make $Z_n$ have 0 mean and 1 variance.\n",
    "\n",
    "$$Z_n = \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}}$$\n",
    "\n",
    "- **Central Limit Theorem** asserts that the probability distribution function or **PDF** of $p_Z(z)$ converges to the standard normal distribution in the limit of large number of n steps:\n",
    "\n",
    "$$p_Z\\left(z\\right) \\rightarrow \\frac{1}{\\sqrt{2\\pi}}  e^{-z^2/2}$$\n",
    "\n",
    "- There is an implicit assumption that the **mean and variance**, $\\mu$ and $\\sigma^2$, **are finite**. Thus, CLT does not hold for certain power-law distributed RVs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Simulating a 1D unbiased random walk \n",
    "\n",
    "- Each random walker will be modeled by a random variable $X_i$, assuming +1 or -1 values at every step. We will run N random walkers (rows) over n steps (columns)\n",
    "- We then take **cumulative sum  over n steps** thereby summing n random variables for N walkers. This will be done via a convenient ```np.cumsum()``` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rw_1d(n, N):\n",
    "    '''\n",
    "    n: Number of steps\n",
    "    N: Number of walkers\n",
    "    returns np.array with shape (n, N) \n",
    "    '''\n",
    "    \n",
    "    # Create random walks \n",
    "    r  = np.random.choice([-1,1], size=(n, N))\n",
    "    \n",
    "    #Sum over n steps\n",
    "    rw = r.cumsum(axis=0)\n",
    "\n",
    "    #Set initial position \n",
    "    rw[0,:]=0 \n",
    "    \n",
    "    return rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "rw = rw_1d(2000, 1000)\n",
    "\n",
    "print(rw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulate 1D random walk\n",
    "n_max = 1000\n",
    "N     = 1000 \n",
    "rw    = rw_1d(n_max, N)\n",
    "\n",
    "def rw_plotter(t=1):\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2)\n",
    "\n",
    "    ax[0].plot(rw)\n",
    "    ax[0].axvline(x=t, color='black', linestyle='-', lw=2)\n",
    "    ax[1].hist(rw[t, :], color='orange', density=True, label=f'time={t}')\n",
    "\n",
    "    ## Plot gaussian with width t**0.5\n",
    "    x = np.linspace(-100,100, 1000)\n",
    "    y = stats.norm.pdf(x, 0, np.sqrt(t))\n",
    "    ax[1].plot(x,y, color='black', lw=2, label=f'std={np.sqrt(t):.2f}')  \n",
    "\n",
    "    ax[0].set_ylabel('Position')\n",
    "    ax[0].set_title('RW trajectries');\n",
    "\n",
    "    ax[1].set_xlabel('Position')\n",
    "    ax[1].set_ylabel('Histogram')\n",
    "    ax[1].set_xlim([-100, 100])\n",
    "    ax[1].legend()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rw_2d(n, N):\n",
    "    '''2d random walk function:\n",
    "    n: Number of steps\n",
    "    N: Number of trajecotry\n",
    "    returns np.array with shape (T, N)\n",
    "    '''\n",
    "    verteces = np.array([(1,  0),\n",
    "                         (0,  1),\n",
    "                         (-1, 0),\n",
    "                         (0, -1)])\n",
    "    \n",
    "    rw       = verteces[choice([0,1,2,3], size=(T, N))]\n",
    "    \n",
    "    rw[0, :, :] = 0\n",
    "    \n",
    "    return rw.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = rw_2d(n=10000, N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate 2D random walk\n",
    "n, N = 10000, 1000\n",
    "traj = rw_2d(n, N)\n",
    "\n",
    "#Compute RSD \n",
    "dx = (traj[:, :, 0]- traj[0, :, 0]) \n",
    "dy = (traj[:, :, 1]- traj[0, :, 1]) \n",
    "\n",
    "R2     = np.mean(dx**2 + dy**2, axis = 1)   # notice how we averaging over N\n",
    "\n",
    "fig, ax  = plt.subplots(nrows=2, figsize=(10,10))\n",
    "\n",
    "t = np.arange(n) # time axis\n",
    "ax[1].loglog(t, np.sqrt(R2), lw=3, alpha=0.5);\n",
    "ax[1].loglog(t, np.sqrt(t), '--');\n",
    "\n",
    "ax[0].set_title('2D random walker',fontsize=15)\n",
    "ax[0].plot(traj[:3000, :5, 0], traj[:3000, :5, 1]);\n",
    "ax[0].set_xlabel('X')\n",
    "ax[1].set_xlabel('Y')\n",
    "\n",
    "ax[1].set_xlabel('Number of steps, n',fontsize=15)\n",
    "ax[1].set_ylabel(r'$MSD(n)$',fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean square displacement (MSD) of a random walker\n",
    "\n",
    "- After time n number of steps (or time t) how far has random walker moved from the origin?\n",
    "\n",
    "$$Z_n = \\sum^{n-1}_{i=0}X_n$$\n",
    "\n",
    "- We quantify this by computing **Mean Square Displacement (MSD)**. Note that the mean is computed over N number of simulated trajectories (ensemble average). Invoking central limit theorem, or simply realizing that off diagonal terms drop off we end up with expected 1/2 saling with steps.\n",
    "\n",
    "$$\n",
    "MSD(n)= \\Big\\langle \\big ( Z_n - Z_0 \\big)^2 \\Big \\rangle \\sim n^{1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean square displacement (MSD) and Diffusion\n",
    "\n",
    "- Einstein developed a theory of diffusion based on random walk ideas and obtained a key equation relating mean square displacement of particle in d dimensions to time $(t = n \\Delta t)$.\n",
    "\n",
    "- Consider recording n number of positions for a particle in solution starting from origin $r_0=0$\n",
    "\n",
    "$$R_n = \\sum^{n}_{i=0} r_i$$\n",
    "\n",
    "- Repeating experiment N number of times we can compute ensemble average\n",
    "\n",
    "$$\\langle R^2_n\\rangle = \\sum_i \\sum_j \\langle r_i r_j \\rangle = \\sum_i \\langle r^2_i \\rangle = \\sum_i d \\cdot \\langle \\delta x^2_i \\rangle =  d \\cdot n \\cdot  \\delta x^2$$\n",
    "\n",
    "- Where we decomposed displacement in d independent dimensions. Write number of steps in terms of time increments $n=t/\\delta t$ we get:\n",
    "\n",
    "$$\\langle R^2_n\\rangle =  d \\cdot \\frac{t}{\\delta t}  \\cdot \\delta x^2$$\n",
    "\n",
    "- Grouping constants together we defined the diffusion coefficient:\n",
    "\n",
    "$$\\langle R^2 (t) \\rangle = 2d  \\cdot  D \\cdot t$$\n",
    "\n",
    "\n",
    "$$D = \\frac{\\langle \\delta x^2 \\rangle}{2\\delta t}$$\n",
    "\n",
    "- We end up with a general expression for a mean square displacement as a function of time. Any motion which adheres to this scaling with time will be called **diffusive**. \n",
    "\n",
    "$$MSD(t) = 2d D \\cdot t^{1/2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, N = 2000, 1000\n",
    "rw = rw_1d(n, N)\n",
    "\n",
    "t = np.arange(n)\n",
    "\n",
    "R2 = (rw[:, :] - rw[0, :])**2 # Notice we subtract initial time\n",
    "\n",
    "msd =  np.mean(R2, axis=1)    # Notice we average over N\n",
    "\n",
    "plt.loglog(t, np.sqrt(msd), lw=3) \n",
    "\n",
    "plt.loglog(t, np.sqrt(t), '--')\n",
    "\n",
    "plt.title('Compute mean square deviation of 1D random walker',fontsize=15)\n",
    "plt.xlabel('Number of steps, n',fontsize=15)\n",
    "plt.ylabel(r'$MSD(n)$',fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brownian motion\n",
    "\n",
    "- The Brownian motion  describes the movement of a particle suspended in a fluid resulting from random collisions with the quick molecules in the fluid (diffusion). \n",
    "\n",
    "- A small particle undergoes many molecular collisions when going from one step to another or after $dt$ time. As a result, each displacement over time $dt$ can be viewed as a sum of random collisions, which can be approximated by a normal distribution via the Central Limit Theorem.\n",
    "\n",
    "- Thus, to simulate Brownian motion, we draw random displacements from normal distribution. \n",
    "\n",
    "$$x(t+dt)-x(t)=N(0,\\sqrt{2D dt})$$ \n",
    "\n",
    "We assume we have started at position $\\mu=0$, and our variance is given by $\\sigma^2=2Dt$, Where D is the diffusion coefficient, which is related to the parameters of the discrete random walk as shown in the lecture.  \n",
    "\n",
    "$$x(t+dt)=x(t)+\\sqrt{2D dt} \\cdot N(0,1)$$ \n",
    "\n",
    "In the last step, we re-wrote Brownian motion equation in a convenient way by shifting the normally distributed random variable by $\\mu$ and scaling it by $\\sigma$ \n",
    "\n",
    "$$N(\\mu, \\sigma^2) = \\mu + \\sigma N(0,1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brown(T, N, dt=1, D=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates 3D brownian path given:\n",
    "    time T \n",
    "    N=1 trajecotires\n",
    "    dt=1 timestep\n",
    "    D=1 diffusion coeff\n",
    "    returns np.array with shape (N, T, 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = int(T/dt) # how many points to sample\n",
    "    \n",
    "    dR = np.sqrt(2*D*dt) * np.random.randn(N, n, 3) # 3D position of brownian particle\n",
    "    \n",
    "    R = np.cumsum(dR, axis=1) # accumulated 3D position of brownian particle\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Appendix A: Stitrling's approximation of N!\n",
    ":class: tip, dropdown\n",
    "\n",
    "**Stirling's approximation**\n",
    "\n",
    "- This is the crude version of Stirling approximation that works out for $N\\gg 1$\n",
    "\n",
    "$$logN! \\approx \\sum log N_i = \\int log N dN$$\n",
    "\n",
    "$$\\boxed{logN! \\approx NlogN-N}$$\n",
    "\n",
    "$$\\boxed{N! \\approx N^N e^{-N}}$$\n",
    "\n",
    "A more accurate version is:\n",
    "\n",
    "$$\\boxed{N! \\approx N^N e^{-N} \\sqrt{2\\pi N}}$$\n",
    "\n",
    "**Applying Stirling approximation to the Binomial**\n",
    "\n",
    "$$N = N_1+N_2$$ \n",
    "\n",
    "$$\\frac{N!}{N_1! \\cdot N_2!} \\approx \\frac{N^N e^{-N}}{N_1^{N_1} e^{-N_1}\\cdot N_2^{N_2} e^{-N_2}} =\\frac{N^N } {N_1^{N_1} \\cdot N_2^{N_2} }$$\n",
    "\n",
    "$$log\\frac{N!}{N_1! \\cdot N_2!} \\approx NlogN -N_1 log N_1 - N_2log N_2 = \\\\ = N \\Big[ -\\frac{N_1}{N} lo\\frac {N_1}{N} - \\frac{N_2}{N} log \\frac{N_2}{N} \\Big] = N[-p_1 log p_1 -p_2log p_2]$$\n",
    "\n",
    "- The expression $S =  \\sum_i -p_i log p_i$ will be identified with Entropy in later sections. \n",
    "\n",
    "**Gamma function: generalizing the factorial**\n",
    "\n",
    "$$\\Gamma (n) = \\int^{\\infty}_0 x^{n-1} e^{-x}dx$$\n",
    "\n",
    "- The gamma function $\\Gamma(n)$ provides an extension of the factorial function to domain of noninteger and    complex numbers. \n",
    "- The gamma function is defined for all complex numbers except the non-positive integers. \n",
    "- For any positive integer gamma function reduces to factorials\n",
    "\n",
    "$$\\Gamma(n)=(n-1)!$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Appendix B. Gaussian approximation to the Binomial Distribution in the large $N$ limit.\n",
    ":class: tip, dropdown  \n",
    "\n",
    "Binomial distribution for large values of $N$ has a sharply peaked distribution around its maximum (most likely) value $\\bar{n}$. This motivates us to seek a continuous approximation by Taylor expanding probability distribution around its max value $\\Delta n = n-\\bar{n}$ and keeping up to quadratic terms.\n",
    "\n",
    "$$P_N(n) = \\frac{N!}{n! (N-n)!} p^n (1-p)^{(N-n)}$$\n",
    "\n",
    "Thus from the onset we are aiming for a Gaussian distribution. The task then is to find coefficients and to then justify that third term of Taylor expansion is negligible compared to the second!\n",
    "\n",
    "$$logP(n) = logP(\\bar{n}) + \\frac{1}{2}B_2\\Delta n^2 + O(\\delta n^3)$$\n",
    "\n",
    "$$log P(n) = log N! - log n! - log(N-n)! + nlog(p) + (N-n)log(1-p)$$\n",
    "\n",
    "We evaluate derivative of $logn!$ in the limit of $n\\gg1$ as: \n",
    "\n",
    "$$\\frac{d}{dn} log n! = \\frac{log(n+1)! - log n!}{n+1-n} \\approx log(n+1) \\approx log(n)$$\n",
    "\n",
    "- We could also arrive at the same result by using Stirling approximation $logN! \\approx  NlogN -n$\n",
    "- Taking first derivative of Taylor expansion to Binomial we find the peak of the distribution around which we are making expansion:\n",
    "\n",
    "$$\\frac{d}{dn}log P(n) \\Big |_{n=\\bar{n}} = - log n + log(n-n) + log(p)  -log(1-p)=0$$\n",
    "\n",
    "$$log \\Big( \\frac{N-n}{n} \\frac{p}{1-p}\\Big)=0\\,\\, \\rightarrow \\,\\, \\bar{n} = Np$$\n",
    "\n",
    "- We recall that $\\bar{n} = Np$ is also mean of the binomial! \n",
    "- Having found the peak of distribution and knowing first derivative we now proceed to compute the second derivative:\n",
    "\n",
    "$$B_2 = \\frac{d^2}{d n^2} logP(n) \\Big |_{n = \\bar{n}} = \\frac{d}{dn} log \\Big( \\frac{N-n}{n} \\frac{p}{1-p}\\Big) \\\\ = \\Big( - \\frac{1}{N-n}-\\frac{1}{n} \\Big) \\Big |_{n = \\bar{n}} = - \\frac{1}{Npq}$$\n",
    "\n",
    "- While first derivative gave us the mean of binomial we notice that second derivative produces the variance $\\sigma^2 = Npq$\n",
    "- Now all that remains to do is to plug  the coefficients into our approximated probability distribution and then normalize it. Why normalize? After all Binomial was already properly normalized.  But since we made approximation by leaving our some terms we have to re-normalize again!\n",
    "\n",
    "$$P(n) \\approx P(\\bar{n}) e^{-(n-\\bar{n})^2/ 2Npq}$$\n",
    "\n",
    "- Normalizing gaussian distribution is done via the following table integral\n",
    "\n",
    "$$\\int^{+\\infty}_{-\\infty} e^{-ax^2} = \\Big(\\frac{\\pi}{a} \\Big)^{1/2}$$\n",
    "\n",
    "$$\\int P(\\bar{n}) e^{-(n-\\bar{n})^2/ 2Npq} dn  = P(\\bar{n}) (2\\pi Npq)^{1/2}=1$$\n",
    "\n",
    "- At last we have the normalized approximation to Binomial which is a guassian distribution arond mean!\n",
    "\n",
    "$$P(n) \\approx \\frac{1}{(2\\pi Npq)^{1/2}} e^{-(n-\\bar{n})^2/ 2Npq} = \\frac{1}{(2\\pi \\sigma^2)^{1/2}}e^{(n-\\mu)^2/2\\sigma^2}$$\n",
    "\n",
    "**An Alternative derivation of Gaussian from Binomial making use of Stirling's approximation**\n",
    "\n",
    "Here we would like to start by choosing as a new independent variable the net right displacement of a random walker $m$  \n",
    "\n",
    "- $m = n - (N-n) = 2n-N$. \n",
    "- Hence the number of moves to the right and left are  $n = \\frac{N+m}{2}$ and  $(N - n)=\\frac{N-m}{2}$ respectively.\n",
    "- The left and right moves sum to the total move number $\\frac{N-m}{2} + \\frac{N-m}{2} = N$\n",
    "- Let us also assume $p = 1/2$ for the sake of simplicity.\n",
    "\n",
    "$$P_N(m) = \\frac{N!}{(\\frac{N+m}{2})! \\cdot (\\frac{N-m}{2})!} \\frac{1}{2^N}$$\n",
    "\n",
    "- Now we make use of Stirling approximation  valid for $N\\gg1$ $N! = N^N e^N$. and start group with common exponents containing N and m respectively. \n",
    "\n",
    "$$P_N(m) \\approx \\frac{N^N e^N}{ (N-m)^{(N-m)/2} (N+m)^{(N+m)/2}  \\cdot 2^{-N} e^N } \\cdot \\frac{1}{2^N}  \\\\ = \\frac{N^N}{ (N-m)^{(N-m)/2} (N+m)^{(N+m)/2}} \\\\ = \\frac{N^N}{ (N-m)^{N/2} (N+m)^{N/2}} \\cdot  \\Big( \\frac{N-m}{N+m}\\Big)^{m/2} \\\\ = \\frac{1}{[(1-m/N)(1+m/N)]^{N/2}} \\cdot  \\Big( \\frac{N-m}{N+m}\\Big)^{m/2} \\\\ = \\Big[(1-m^2/N^2) \\Big]^{-N/2} \\Big( \\frac{1-m/N}{1+m/N}\\Big)^{m/2}  \\approx (e^{-m^2/N^2})^{-N/2} (e^{-m/N})^{m/2} (e^{+m/N})^{-m/2} = e^{-m^2/2N}$$\n",
    "\n",
    "- Notice how factors involving $2^N$ and $e^N$ cancell in the first line.\n",
    "- We have casted all terms in terms of $m/N$. This allowed us to make use of $(1\\pm x)\\approx e^{\\pm x}$ approximation for small $x$. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Appendix C. Deriving Poisson distribution from Binomial \n",
    ":class: tip, dropdown \n",
    "\n",
    "**Consider the limit of large $N$ and small $p$ such that $Np=const$**\n",
    "\n",
    "- This is a situation of rare events like rains in forest or radioactive decay of uranium where each individual event has small chance of happening $p \\rightarrow 0$  yet there are large number of samples $N\\rightarrow \\infty$ such that one has a constant average rate of events $\\lambda = pN = const$\n",
    "- In this limit distirbution is no longer well described by the gaussian as the shape of distribution is heavily skewed due to tiny values of p.\n",
    "\n",
    "$$P_N(n) = \\frac{N!}{n! (N-n)!} p^n (1-p)^{(N-n)}$$\n",
    "\n",
    "- Writing factorial $N!/(N-n)!$ explicitely we realize that it is dominated $N^n$ and also $N-n \\approx N$\n",
    "\n",
    "$$P_N(n) = \\frac{N(N-1)...(N-1+1))}{n!} p^n (1-p)^{(N-n)} \\approx \\frac{N^n}{n!} p^n (1-p)^{N}$$\n",
    "\n",
    "- Next let us plug in $\\lambda = pN = const$ and recall the definition of exponential $lim_{x\\rightarrow \\infty }(1-1/x)^x = e^{-x}$\n",
    "\n",
    "$$P(n) = \\frac{N^n}{n!} \\Big( \\frac{\\lambda}{N} \\Big)^n \\Big( 1-\\frac{\\lambda}{N} \\Big)^{N} = \\frac{\\lambda^n}{n!} \\Big( 1-\\frac{\\lambda}{N} \\Big)^{N} \\approx \\frac{\\lambda^n}{n!} e^{-\\lambda}$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Confined diffusion.\n",
    "Simulate 2D random walk in a circular confinement. Re-write 2D random walk  code to simulate diffusion of a particle which is stuck inside a sphere. \n",
    "Study how root mean square deviation of position scales with time. \n",
    "- Carry out simulations for different confinement sizes. \n",
    "- Make plots of simulated trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return to the origin!\n",
    "\n",
    "- Simulate random walk in 1D and 2D for a different number of steps $N=10, 10^2,10^3, 10^4, 10^5$\n",
    "- Compute average number of returns to the origin $\\langle n_{orig} \\rangle$. That is number of times a random walker returns to the origin $0$ for 1D  or (0,0)$ for 2D . You may want to use some 1000 trajectories to obtain average. \n",
    "- Plot how $\\langle n_{orig} \\rangle$ depends on number of steps N for 1D and 2D walker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Breaking the CLT; Cauchy vs Normal random walk in 2D\n",
    "\n",
    "For this problem we are going to simulate two kinds of random walks in continuum space (not lattice): Levy flights and Normal distributd random walk. \n",
    "\n",
    "To simulate a 2D continuum space random walk we need to generate random step sizes $r_x$, $r_y$. \n",
    "Also you will need unifrom random namber to sample angles in 2D giving you a conitnuum random walk in 2D space: $x = r_x sin\\theta$ and $y=r_ycos\\theta$\n",
    "\n",
    "- Normally: $r\\sim N(0,1)$\n",
    "- Cauchy distribution (long tails, infinite variance) $r\\sim Cauchy(0,1)$\n",
    "- Unform angles $\\theta \\sim U(0,1)$\n",
    "\n",
    "Visualize random walk using matplotlib and study statistics of random walkers the way that is done for normal random walk/brownian motion examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional Problem) Continuous time random walk (CTRW)\n",
    "\n",
    "Simulate 1D random walk but instead of picking times at regular intervals pick them from  exponential distribution. <br>\n",
    "Hint: you may want to use random variables from scipy.stats.exp <br>\n",
    "\n",
    "[scipy.stats.expon](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html) <br>\n",
    "\n",
    "Study the root mean square deviation as a function of exponential decay parameter $\\lambda$ of exponential distribution $e^{-\\lambda x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "**The mighty little books**\n",
    "-  [\"Random Walks in Biology\",  H Berg (1993)](https://www.amazon.com/Random-Walks-Biology-Howard-Berg/dp/0691000646)\n",
    "-  [\"Physical models of Living systems\",  P Nelson (2015)](https://www.amazon.com/gp/product/1464140294/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&psc=1)\n",
    "\n",
    "**More in depth**\n",
    " - [\"Simple Brownian Diffusion: An Introduction to the Standard Theoretical Models\", D Gillespie](https://www.amazon.com/Simple-Brownian-Diffusion-Introduction-Theoretical/dp/0199664501/ref=sr_1_1?keywords=diffusion+brownian&qid=1579882520&sr=8-1)\n",
    " - [\"Stochastic Processes for Physicists\" K Jacobs](https://www.amazon.com/Stochastic-Processes-Physicists-Understanding-Systems/dp/0521765420/ref=sr_1_1?keywords=kurt+jacobs+stochastic&qid=1579882738&sr=8-1)\n",
    " \n",
    "**On the applied side**\n",
    "- [Brownian Motion: Elements of Colloid Dynamics A P Philipse (2018)](https://www.amazon.com/Brownian-Motion-Elements-Dynamics-Undergraduate/dp/3319980521/ref=sr_1_7?keywords=einstein+brownian&qid=1579882356&sr=8-7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "livereveal": {
   "theme": "sky"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
